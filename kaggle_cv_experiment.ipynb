{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9221e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple, Union, Any\n",
    "from dataclasses import dataclass\n",
    "from abc import ABC, abstractmethod\n",
    "from enum import Enum\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import TargetEncoder\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bcb205",
   "metadata": {},
   "source": [
    "## Configuration & Base Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e92c30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============== DatasetConfig ==============\n",
    "@dataclass\n",
    "class DatasetConfig:\n",
    "    training_col_names: List[str]\n",
    "    target_col_name: str\n",
    "    numerical_col_names: List[str]\n",
    "    categorical_col_names: List[str]\n",
    "    name: Optional[str] = None\n",
    "\n",
    "\n",
    "# ============== Base Preprocessor ==============\n",
    "class BasePreprocessor(ABC):\n",
    "    name = \"base-preprocessor\"\n",
    "\n",
    "    def __init__(self, config: DatasetConfig):\n",
    "        self._fitted = False\n",
    "        self.config = config\n",
    "\n",
    "    @abstractmethod\n",
    "    def fit(self, data: pd.DataFrame) -> \"BasePreprocessor\":\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def transform(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        pass\n",
    "\n",
    "    def fit_transform(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        return self.fit(data).transform(data)\n",
    "\n",
    "    @property\n",
    "    def fitted(self) -> bool:\n",
    "        return self._fitted\n",
    "\n",
    "\n",
    "# ============== Base Model ==============\n",
    "T = Union[np.ndarray, pd.DataFrame]\n",
    "\n",
    "class BaseKtoolsModel(ABC):\n",
    "    def __init__(self) -> None:\n",
    "        self._fitted = False\n",
    "        self.model = None\n",
    "\n",
    "    @abstractmethod\n",
    "    def fit(\n",
    "        self,\n",
    "        X: T,\n",
    "        y: T,\n",
    "        validation_set: Optional[Tuple[T, T]] = None,\n",
    "        weights: Optional[T] = None,\n",
    "        val_weights: Optional[T] = None,\n",
    "    ) -> \"BaseKtoolsModel\":\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict(self, X: T) -> np.ndarray:\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    def fitted(self) -> bool:\n",
    "        return self._fitted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4eddc82",
   "metadata": {},
   "source": [
    "## Preprocessing Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b829d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============== Categorical Frequency Encoder ==============\n",
    "class CategoricalFrequencyEncoder(BasePreprocessor):\n",
    "    freq_suffix = \"_frequency_encoding\"\n",
    "\n",
    "    def __init__(self, config: DatasetConfig, encode_missing_value: int = 0):\n",
    "        super().__init__(config)\n",
    "        self.train_freq_mappings: Dict[str, Dict[int, float]] = {}\n",
    "        self.encode_missing_value = encode_missing_value\n",
    "\n",
    "    def fit(self, data: pd.DataFrame) -> \"CategoricalFrequencyEncoder\":\n",
    "        for column in self.config.categorical_col_names:\n",
    "            new_col_name = column + self.freq_suffix\n",
    "            if new_col_name in self.config.training_col_names:\n",
    "                raise ValueError(\"Frequency encoded column already exists\")\n",
    "\n",
    "            freq_map = (\n",
    "                data[column]\n",
    "                .value_counts(normalize=True)\n",
    "                .to_dict()\n",
    "            )\n",
    "            self.train_freq_mappings[column] = freq_map\n",
    "        self._fitted = True\n",
    "        return self\n",
    "\n",
    "    def transform(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        copy = data.copy()\n",
    "        for column in self.config.categorical_col_names:\n",
    "            new_col_name = column + self.freq_suffix\n",
    "            freq_map = self.train_freq_mappings[column]\n",
    "            copy[new_col_name] = (\n",
    "                copy[column]\n",
    "                .astype(str)\n",
    "                .map(freq_map)\n",
    "                .fillna(self.encode_missing_value)\n",
    "                .astype(float)\n",
    "            )\n",
    "        return copy\n",
    "\n",
    "\n",
    "# ============== Categorical Target Encoder ==============\n",
    "class CategoricalTargetEncoder(BasePreprocessor):\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: DatasetConfig,\n",
    "        random_state: int = 42,\n",
    "        cv: int = 5,\n",
    "        smooth: int = 15,\n",
    "        shuffle: bool = True,\n",
    "    ):\n",
    "        super().__init__(config)\n",
    "        self.target_encoder = TargetEncoder(\n",
    "            random_state=random_state, cv=cv, smooth=smooth, shuffle=shuffle\n",
    "        )\n",
    "\n",
    "    def fit(self, data: pd.DataFrame) -> \"CategoricalTargetEncoder\":\n",
    "        self.target_encoder.fit(\n",
    "            data[self.config.categorical_col_names], data[self.config.target_col_name]\n",
    "        )\n",
    "        self._fitted = True\n",
    "        return self\n",
    "\n",
    "    def transform(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        copy = data.copy()\n",
    "        copy[self.config.categorical_col_names] = self.target_encoder.transform(\n",
    "            copy[self.config.categorical_col_names]\n",
    "        ).astype(\"float32\")\n",
    "        return copy\n",
    "\n",
    "    def fit_transform(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        copy = data.copy()\n",
    "        copy[self.config.categorical_col_names] = self.target_encoder.fit_transform(\n",
    "            copy[self.config.categorical_col_names], copy[self.config.target_col_name]\n",
    "        ).astype(\"float32\")\n",
    "        self._fitted = True\n",
    "        return copy\n",
    "\n",
    "\n",
    "# ============== Preprocessing Pipeline ==============\n",
    "class PreprocessingPipeline:\n",
    "    def __init__(self, preprocessors: List[BasePreprocessor]) -> None:\n",
    "        self.preprocessors = preprocessors\n",
    "\n",
    "    def train_pipe(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        for preprocessor in self.preprocessors:\n",
    "            data = preprocessor.fit_transform(data)\n",
    "        return data\n",
    "\n",
    "    def inference_pipe(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        for preprocessor in self.preprocessors:\n",
    "            data = preprocessor.transform(data)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5590d3b7",
   "metadata": {},
   "source": [
    "## Model Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868d47f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============== Helper Function ==============\n",
    "def infer_task(y: Union[np.ndarray, pd.Series]) -> str:\n",
    "    \"\"\"\n",
    "    Will infer binary, multiclass classification or regression based on the target values.\n",
    "    \"\"\"\n",
    "    if isinstance(y, pd.Series):\n",
    "        y = y.to_numpy()\n",
    "    y = y.flatten()\n",
    "\n",
    "    nuniques = np.unique(y).shape[0]\n",
    "    has_floats = np.any(y % 1 != 0)\n",
    "\n",
    "    if has_floats:\n",
    "        print(\"Target contains float values. Inferring regression task.\")\n",
    "        return \"regression\"\n",
    "    elif nuniques == 2:\n",
    "        print(\"Target contains two unique values. Inferring binary classification task.\")\n",
    "        return \"binary_classification\"\n",
    "    elif nuniques > 2:\n",
    "        print(\"Target contains more than two unique values. Inferring multiclass classification task.\")\n",
    "        return \"multiclass_classification\"\n",
    "\n",
    "    raise ValueError(\n",
    "        \"Unable to infer task type from target values. Is there only one target value?\"\n",
    "    )\n",
    "\n",
    "\n",
    "# ============== Default Objectives ==============\n",
    "class DefaultObjective(Enum):\n",
    "    regression = \"regression\"\n",
    "    binary_classification = \"binary\"\n",
    "    multiclass_classification = \"multiclass\"\n",
    "\n",
    "\n",
    "# ============== LightGBM Model ==============\n",
    "class LGBMModel(BaseKtoolsModel):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_boost_round: int = 100,\n",
    "        early_stopping_rounds: Union[int, None] = 20,\n",
    "        random_state: int = 129,\n",
    "        verbose: int = -1,\n",
    "        n_jobs: int = 1,\n",
    "        callbacks: List[Any] = [],\n",
    "        **lgb_param_grid,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self._num_boost_round = num_boost_round\n",
    "        self._verbose = verbose\n",
    "        self._n_jobs = n_jobs\n",
    "        self._callbacks = callbacks\n",
    "        self.early_stopping_rounds = early_stopping_rounds\n",
    "\n",
    "        self._lgb_param_grid = {\n",
    "            \"verbose\": verbose,\n",
    "            \"random_state\": random_state,\n",
    "            \"n_jobs\": n_jobs,\n",
    "            **lgb_param_grid,\n",
    "        }\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        X: T,\n",
    "        y: T,\n",
    "        validation_set: Optional[Tuple[T, T]] = None,\n",
    "        weights: Optional[T] = None,\n",
    "        val_weights: Optional[T] = None\n",
    "    ) -> \"LGBMModel\":\n",
    "        if \"objective\" not in self._lgb_param_grid:\n",
    "            task_id = infer_task(y)\n",
    "            self._lgb_param_grid[\"objective\"] = DefaultObjective[task_id].value\n",
    "            if task_id == \"multiclass_classification\":\n",
    "                self._lgb_param_grid[\"num_class\"] = np.unique(y).shape[0]\n",
    "\n",
    "        train_data = lgb.Dataset(X, label=y, weight=weights)\n",
    "        eval_sets = [train_data]\n",
    "        eval_names = [\"train\"]\n",
    "        if validation_set is not None:\n",
    "            X_val, y_val = validation_set\n",
    "            val_data = lgb.Dataset(X_val, label=y_val, reference=train_data, weight=val_weights)\n",
    "            eval_sets += [val_data]\n",
    "            eval_names += [\"valid\"]\n",
    "            self._lgb_param_grid[\"early_stopping_rounds\"] = self.early_stopping_rounds\n",
    "\n",
    "        train_params = {\n",
    "            \"params\": self._lgb_param_grid,\n",
    "            \"train_set\": train_data,\n",
    "            \"num_boost_round\": self._num_boost_round,\n",
    "            \"valid_sets\": eval_sets,\n",
    "            \"valid_names\": eval_names,\n",
    "            \"callbacks\": self._callbacks,\n",
    "        }\n",
    "\n",
    "        self.model = lgb.train(**train_params)\n",
    "        self._fitted = True\n",
    "        return self\n",
    "\n",
    "    def predict(self, X: T) -> np.ndarray:\n",
    "        y_pred = self.model.predict(X)\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "# ============== Model Pipeline ==============\n",
    "class ModelPipeline:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: BaseKtoolsModel,\n",
    "        config: DatasetConfig,\n",
    "        preprocessor: PreprocessingPipeline = None,\n",
    "    ) -> None:\n",
    "        self.model = model\n",
    "        self.config = config\n",
    "        self.preprocessor = preprocessor if preprocessor else PreprocessingPipeline([])\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        train_data: pd.DataFrame,\n",
    "        validation_data: Optional[pd.DataFrame] = None,\n",
    "        weights: Optional[Union[pd.Series, np.ndarray]] = None,\n",
    "    ) -> \"ModelPipeline\":\n",
    "        train_data = self.preprocessor.train_pipe(train_data)\n",
    "        X_train = train_data.drop(columns=[self.config.target_col_name])\n",
    "        y_train = train_data[self.config.target_col_name]\n",
    "\n",
    "        if validation_data is not None:\n",
    "            validation_data = self.preprocessor.inference_pipe(validation_data)\n",
    "            X_valid = validation_data.drop(columns=[self.config.target_col_name])\n",
    "            y_valid = validation_data[self.config.target_col_name]\n",
    "            validation_data = (X_valid, y_valid)\n",
    "\n",
    "        self.model.fit(\n",
    "            X=X_train, y=y_train, validation_set=validation_data, weights=weights\n",
    "        )\n",
    "        return self\n",
    "\n",
    "    def predict(self, data: pd.DataFrame) -> np.ndarray:\n",
    "        data = self.preprocessor.inference_pipe(data)\n",
    "        X_test = data[self.config.training_col_names]\n",
    "        return self.model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f992d0b3",
   "metadata": {},
   "source": [
    "## Data Loading & Preprocessing\n",
    "\n",
    "This section implements the data preparation logic from the test file:\n",
    "1. Load original, training, and test data\n",
    "2. Assign data source labels (0=validation part of train, 1=train part, 2=original)\n",
    "3. Create aggregated features from original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be37af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============== Configuration ==============\n",
    "# For Kaggle, update these paths to match the competition data location\n",
    "# DATA_PATH = Path(\"/kaggle/input/playground-series-s5e2/\")\n",
    "DATA_PATH = Path(\"./data/diabetes_prediction/\")  # Local path\n",
    "\n",
    "TARGET = \"diagnosed_diabetes\"\n",
    "SPLIT_ID = 678260  # Index to split training data into train/validation groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca313c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============== Load Data ==============\n",
    "original_data = pd.read_csv(DATA_PATH / \"original.csv\")\n",
    "training_data = pd.read_csv(DATA_PATH / \"train.csv\", index_col=0)\n",
    "test_data = pd.read_csv(DATA_PATH / \"test.csv\", index_col=0).assign(data=0)\n",
    "\n",
    "print(f\"Original data shape: {original_data.shape}\")\n",
    "print(f\"Training data shape: {training_data.shape}\")\n",
    "print(f\"Test data shape: {test_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56dcd8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============== Assign Data Source Labels ==============\n",
    "# data=0: validation portion of training data (after SPLIT_ID)\n",
    "# data=1: training portion of training data (before SPLIT_ID)\n",
    "# data=2: original external data\n",
    "\n",
    "original_data = original_data.assign(data=2)\n",
    "training_data = training_data.assign(data=0)\n",
    "training_data.iloc[:SPLIT_ID, training_data.columns.get_loc('data')] = 1\n",
    "\n",
    "print(f\"Training data source distribution:\")\n",
    "print(training_data['data'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180763bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============== Create Aggregated Features from Original Data ==============\n",
    "# Merge training, original and test data for feature engineering\n",
    "train_orig_test = pd.concat(\n",
    "    [training_data, original_data[training_data.columns], test_data], \n",
    "    axis=0, \n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "training_cols = training_data.columns.drop([\"data\", TARGET]).tolist()\n",
    "orig_target_mean = original_data[TARGET].mean()\n",
    "\n",
    "print(f\"Creating aggregated features from {len(training_cols)} columns...\")\n",
    "\n",
    "for c in training_cols:\n",
    "    for aggr in [\"mean\", \"count\"]:\n",
    "        col_name = f'{c}_org_{aggr}'\n",
    "        tmp = (\n",
    "            original_data.groupby(c)[TARGET]\n",
    "            .agg(aggr)\n",
    "            .rename(col_name)\n",
    "            .reset_index()\n",
    "        )\n",
    "        \n",
    "        train_orig_test = train_orig_test.merge(tmp, on=c, how='left')\n",
    "        fill_val = orig_target_mean if aggr == 'mean' else 0\n",
    "        train_orig_test[col_name] = train_orig_test[col_name].fillna(fill_val)\n",
    "\n",
    "print(f\"Combined data shape after feature engineering: {train_orig_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3792e6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============== Split Back into Train, Original, Test ==============\n",
    "len_train = training_data.shape[0]\n",
    "len_orig = original_data.shape[0]\n",
    "\n",
    "train_data = train_orig_test.iloc[:len_train, :].copy()\n",
    "orig_data = train_orig_test.iloc[len_train:len_train+len_orig, :].copy()\n",
    "test_data = train_orig_test.iloc[len_train+len_orig:, :].copy().drop(columns=[TARGET])\n",
    "\n",
    "print(f\"Final train data shape: {train_data.shape}\")\n",
    "print(f\"Final original data shape: {orig_data.shape}\")\n",
    "print(f\"Final test data shape: {test_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e7316f",
   "metadata": {},
   "source": [
    "## Dataset Configuration Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49165468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============== Define Feature Categories ==============\n",
    "all_features = train_data.columns.drop(TARGET).tolist()\n",
    "\n",
    "# Categorical features include object/bool types + specific columns\n",
    "categorical_features = (\n",
    "    train_data.drop(columns=TARGET)\n",
    "    .select_dtypes(include=['object', 'bool'])\n",
    "    .columns.tolist() \n",
    "    + ['family_history_diabetes', 'hypertension_history', 'cardiovascular_history']\n",
    ")\n",
    "\n",
    "# Remove duplicates\n",
    "categorical_features = list(set(categorical_features))\n",
    "\n",
    "numerical_features = [col for col in all_features if col not in categorical_features]\n",
    "\n",
    "print(f\"Total features: {len(all_features)}\")\n",
    "print(f\"Categorical features: {len(categorical_features)}\")\n",
    "print(f\"Numerical features: {len(numerical_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2628b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============== Create Dataset Config ==============\n",
    "config = DatasetConfig(\n",
    "    training_col_names=all_features,\n",
    "    categorical_col_names=categorical_features,\n",
    "    numerical_col_names=numerical_features,\n",
    "    target_col_name=TARGET,\n",
    ")\n",
    "\n",
    "# Convert categorical columns to category dtype\n",
    "train_data[categorical_features] = train_data[categorical_features].astype('category')\n",
    "orig_data[categorical_features] = orig_data[categorical_features].astype('category')\n",
    "test_data[categorical_features] = test_data[categorical_features].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d048671",
   "metadata": {},
   "source": [
    "## Cross-Validation Experiment\n",
    "\n",
    "Using Stratified K-Fold with stratification on both target and data source (from cell 8 of the original notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074711f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============== Stratification Categories ==============\n",
    "# Combined stratification on target + data source\n",
    "categories_of_interest = (\n",
    "    train_data[TARGET].astype(str) + \"_\" + train_data[\"data\"].astype(str)\n",
    ")\n",
    "\n",
    "print(\"Stratification category distribution:\")\n",
    "print(categories_of_interest.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7631fd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============== CV Training Loop ==============\n",
    "N_SPLITS = 5\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Initialize arrays for predictions\n",
    "train_oof_preds = np.empty(train_data.shape[0])\n",
    "test_oof_preds = np.zeros(test_data.shape[0])\n",
    "\n",
    "# Sample weights based on data source\n",
    "# data=2 (original): weight 8\n",
    "# data=0 (val portion): weight 16  \n",
    "# data=1 (train portion): weight 1\n",
    "WEIGHT_MAP = {2: 8, 0: 16, 1: 1}\n",
    "\n",
    "preprocessors = [\n",
    "    CategoricalFrequencyEncoder(config=config),\n",
    "    CategoricalTargetEncoder(config=config),\n",
    "]\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
    "mean_score = 0.0\n",
    "fold_scores = []\n",
    "\n",
    "for fold_idx, (train_idx, val_idx) in enumerate(kfold.split(train_data, categories_of_interest)):\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Fold {fold_idx + 1}/{N_SPLITS}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Split data\n",
    "    train_fold = train_data.iloc[train_idx].copy()\n",
    "    val_fold = train_data.iloc[val_idx].copy()\n",
    "    \n",
    "    # Subset validation fold for scoring (only data=0, the \"real\" validation portion)\n",
    "    subsetval_fold = val_fold[val_fold[\"data\"] == 0]\n",
    "    \n",
    "    # Add original data to training fold\n",
    "    train_fold = pd.concat([train_fold, orig_data], axis=0, ignore_index=True)\n",
    "    \n",
    "    print(f\"Train fold size: {train_fold.shape[0]} (including {orig_data.shape[0]} original samples)\")\n",
    "    print(f\"Validation fold size: {val_fold.shape[0]} (subset for scoring: {subsetval_fold.shape[0]})\")\n",
    "    \n",
    "    # Calculate sample weights\n",
    "    weights = train_fold[\"data\"].map(WEIGHT_MAP).values\n",
    "    \n",
    "    # Use ModelPipeline with PreprocessingPipeline\n",
    "    pipe = ModelPipeline(\n",
    "        model=LGBMModel(num_boost_round=1000, early_stopping_rounds=50, verbose=-1),\n",
    "        config=config,\n",
    "        preprocessor=PreprocessingPipeline(preprocessors=preprocessors),\n",
    "    )\n",
    "    \n",
    "    pipe.fit(train_fold, validation_data=subsetval_fold, weights=weights)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = pipe.predict(subsetval_fold)\n",
    "    test_pred = pipe.predict(test_data)\n",
    "    oof_pred = pipe.predict(val_fold)\n",
    "    \n",
    "    # Calculate fold score on SUBSET validation (data=0 only)\n",
    "    fold_score = roc_auc_score(subsetval_fold[TARGET], y_pred)\n",
    "    fold_scores.append(fold_score)\n",
    "    \n",
    "    # Store OOF predictions for full validation fold\n",
    "    train_oof_preds[val_idx] = oof_pred\n",
    "    test_oof_preds += test_pred / N_SPLITS\n",
    "    mean_score += fold_score / N_SPLITS\n",
    "    \n",
    "    print(f\"Fold {fold_idx + 1} ROC AUC Score: {fold_score:.6f}\")\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"CV Results\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Individual fold scores: {[f'{s:.6f}' for s in fold_scores]}\")\n",
    "print(f\"Mean ROC AUC Score: {mean_score:.6f}\")\n",
    "print(f\"Std ROC AUC Score: {np.std(fold_scores):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118bd042",
   "metadata": {},
   "source": [
    "## OOF Score & Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1975cd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============== Overall OOF Score ==============\n",
    "overall_oof_score = roc_auc_score(train_data[TARGET], train_oof_preds)\n",
    "print(f\"Overall OOF ROC AUC Score: {overall_oof_score:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81351ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============== Create Submission ==============\n",
    "# For Kaggle submission\n",
    "# sample_sub = pd.read_csv(DATA_PATH / \"sample_submission.csv\", index_col=0)\n",
    "# sample_sub[TARGET] = test_oof_preds\n",
    "# sample_sub.to_csv(\"submission.csv\")\n",
    "# print(\"Submission saved!\")\n",
    "\n",
    "# Preview predictions\n",
    "print(f\"Test predictions stats:\")\n",
    "print(f\"  Min: {test_oof_preds.min():.6f}\")\n",
    "print(f\"  Max: {test_oof_preds.max():.6f}\")\n",
    "print(f\"  Mean: {test_oof_preds.mean():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75b82bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============== Save OOF and Test Predictions ==============\n",
    "import uuid\n",
    "\n",
    "guid = uuid.uuid4()\n",
    "print(f\"Experiment GUID: {guid}\")\n",
    "\n",
    "# Uncomment to save predictions\n",
    "# save_path = DATA_PATH\n",
    "# pd.DataFrame({f\"{guid}\": train_oof_preds}).to_csv(save_path / \"oofs\" / f\"oof_preds_{guid}.csv\")\n",
    "# pd.DataFrame({f\"{guid}\": test_oof_preds}).to_csv(save_path / \"test_preds\" / f\"test_preds_{guid}.csv\")\n",
    "# print(f\"Predictions saved with GUID: {guid}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
