{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9221e9f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T01:38:20.254326Z",
     "iopub.status.busy": "2025-12-30T01:38:20.254037Z",
     "iopub.status.idle": "2025-12-30T01:38:27.415782Z",
     "shell.execute_reply": "2025-12-30T01:38:27.415215Z",
     "shell.execute_reply.started": "2025-12-30T01:38:20.254300Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple, Union, Any\n",
    "from dataclasses import dataclass\n",
    "from abc import ABC, abstractmethod\n",
    "from enum import Enum\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import TargetEncoder\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bcb205",
   "metadata": {},
   "source": [
    "## Configuration & Base Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e92c30b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T01:38:27.417533Z",
     "iopub.status.busy": "2025-12-30T01:38:27.417012Z",
     "iopub.status.idle": "2025-12-30T01:38:27.425355Z",
     "shell.execute_reply": "2025-12-30T01:38:27.424639Z",
     "shell.execute_reply.started": "2025-12-30T01:38:27.417509Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============== DatasetConfig ==============\n",
    "@dataclass\n",
    "class DatasetConfig:\n",
    "    training_col_names: List[str]\n",
    "    target_col_name: str\n",
    "    numerical_col_names: List[str]\n",
    "    categorical_col_names: List[str]\n",
    "    name: Optional[str] = None\n",
    "\n",
    "\n",
    "# ============== Base Preprocessor ==============\n",
    "class BasePreprocessor(ABC):\n",
    "    name = \"base-preprocessor\"\n",
    "\n",
    "    def __init__(self, config: DatasetConfig):\n",
    "        self._fitted = False\n",
    "        self.config = config\n",
    "\n",
    "    @abstractmethod\n",
    "    def fit(self, data: pd.DataFrame) -> \"BasePreprocessor\":\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def transform(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        pass\n",
    "\n",
    "    def fit_transform(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        return self.fit(data).transform(data)\n",
    "\n",
    "    @property\n",
    "    def fitted(self) -> bool:\n",
    "        return self._fitted\n",
    "\n",
    "\n",
    "# ============== Base Model ==============\n",
    "T = Union[np.ndarray, pd.DataFrame]\n",
    "\n",
    "class BaseKtoolsModel(ABC):\n",
    "    def __init__(self) -> None:\n",
    "        self._fitted = False\n",
    "        self.model = None\n",
    "\n",
    "    @abstractmethod\n",
    "    def fit(\n",
    "        self,\n",
    "        X: T,\n",
    "        y: T,\n",
    "        validation_set: Optional[Tuple[T, T]] = None,\n",
    "        weights: Optional[T] = None,\n",
    "        val_weights: Optional[T] = None,\n",
    "    ) -> \"BaseKtoolsModel\":\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict(self, X: T) -> np.ndarray:\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    def fitted(self) -> bool:\n",
    "        return self._fitted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4eddc82",
   "metadata": {},
   "source": [
    "## Preprocessing Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b829d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T01:38:27.426532Z",
     "iopub.status.busy": "2025-12-30T01:38:27.426249Z",
     "iopub.status.idle": "2025-12-30T01:38:27.452391Z",
     "shell.execute_reply": "2025-12-30T01:38:27.451716Z",
     "shell.execute_reply.started": "2025-12-30T01:38:27.426504Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============== Categorical Frequency Encoder ==============\n",
    "class CategoricalFrequencyEncoder(BasePreprocessor):\n",
    "    freq_suffix = \"_frequency_encoding\"\n",
    "\n",
    "    def __init__(self, config: DatasetConfig, encode_missing_value: int = 0):\n",
    "        super().__init__(config)\n",
    "        self.train_freq_mappings: Dict[str, Dict[int, float]] = {}\n",
    "        self.encode_missing_value = encode_missing_value\n",
    "\n",
    "    def fit(self, data: pd.DataFrame) -> \"CategoricalFrequencyEncoder\":\n",
    "        for column in self.config.categorical_col_names:\n",
    "            new_col_name = column + self.freq_suffix\n",
    "            if new_col_name in self.config.training_col_names:\n",
    "                raise ValueError(\"Frequency encoded column already exists\")\n",
    "\n",
    "            freq_map = (\n",
    "                data[column]\n",
    "                .value_counts(normalize=True)\n",
    "                .to_dict()\n",
    "            )\n",
    "            self.train_freq_mappings[column] = freq_map\n",
    "        self._fitted = True\n",
    "        return self\n",
    "\n",
    "    def transform(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        copy = data.copy()\n",
    "        for column in self.config.categorical_col_names:\n",
    "            new_col_name = column + self.freq_suffix\n",
    "            freq_map = self.train_freq_mappings[column]\n",
    "            copy[new_col_name] = (\n",
    "                copy[column]\n",
    "                .astype(str)\n",
    "                .map(freq_map)\n",
    "                .fillna(self.encode_missing_value)\n",
    "                .astype(float)\n",
    "            )\n",
    "        return copy\n",
    "\n",
    "\n",
    "# ============== Categorical Target Encoder ==============\n",
    "class CategoricalTargetEncoder(BasePreprocessor):\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: DatasetConfig,\n",
    "        random_state: int = 42,\n",
    "        cv: int = 5,\n",
    "        smooth: int = 15,\n",
    "        shuffle: bool = True,\n",
    "    ):\n",
    "        super().__init__(config)\n",
    "        self.target_encoder = TargetEncoder(\n",
    "            random_state=random_state, cv=cv, smooth=smooth, shuffle=shuffle\n",
    "        )\n",
    "\n",
    "    def fit(self, data: pd.DataFrame) -> \"CategoricalTargetEncoder\":\n",
    "        self.target_encoder.fit(\n",
    "            data[self.config.categorical_col_names], data[self.config.target_col_name]\n",
    "        )\n",
    "        self._fitted = True\n",
    "        return self\n",
    "\n",
    "    def transform(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        copy = data.copy()\n",
    "        copy[self.config.categorical_col_names] = self.target_encoder.transform(\n",
    "            copy[self.config.categorical_col_names]\n",
    "        ).astype(\"float32\")\n",
    "        return copy\n",
    "\n",
    "    def fit_transform(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        copy = data.copy()\n",
    "        copy[self.config.categorical_col_names] = self.target_encoder.fit_transform(\n",
    "            copy[self.config.categorical_col_names], copy[self.config.target_col_name]\n",
    "        ).astype(\"float32\")\n",
    "        self._fitted = True\n",
    "        return copy\n",
    "\n",
    "\n",
    "# ============== Preprocessing Pipeline ==============\n",
    "class PreprocessingPipeline:\n",
    "    def __init__(self, preprocessors: List[BasePreprocessor]) -> None:\n",
    "        self.preprocessors = preprocessors\n",
    "\n",
    "    def train_pipe(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        for preprocessor in self.preprocessors:\n",
    "            data = preprocessor.fit_transform(data)\n",
    "        return data\n",
    "\n",
    "    def inference_pipe(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        for preprocessor in self.preprocessors:\n",
    "            data = preprocessor.transform(data)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5590d3b7",
   "metadata": {},
   "source": [
    "## Model Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868d47f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T01:41:15.314322Z",
     "iopub.status.busy": "2025-12-30T01:41:15.314040Z",
     "iopub.status.idle": "2025-12-30T01:41:15.334733Z",
     "shell.execute_reply": "2025-12-30T01:41:15.334121Z",
     "shell.execute_reply.started": "2025-12-30T01:41:15.314293Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============== Helper Function ==============\n",
    "def infer_task(y: Union[np.ndarray, pd.Series]) -> str:\n",
    "    \"\"\"\n",
    "    Will infer binary, multiclass classification or regression based on the target values.\n",
    "    \"\"\"\n",
    "    if isinstance(y, pd.Series):\n",
    "        y = y.to_numpy()\n",
    "    y = y.flatten()\n",
    "\n",
    "    nuniques = np.unique(y).shape[0]\n",
    "    has_floats = np.any(y % 1 != 0)\n",
    "\n",
    "    if has_floats:\n",
    "        print(\"Target contains float values. Inferring regression task.\")\n",
    "        return \"regression\"\n",
    "    elif nuniques == 2:\n",
    "        print(\"Target contains two unique values. Inferring binary classification task.\")\n",
    "        return \"binary_classification\"\n",
    "    elif nuniques > 2:\n",
    "        print(\"Target contains more than two unique values. Inferring multiclass classification task.\")\n",
    "        return \"multiclass_classification\"\n",
    "\n",
    "    raise ValueError(\n",
    "        \"Unable to infer task type from target values. Is there only one target value?\"\n",
    "    )\n",
    "\n",
    "\n",
    "# ============== Default Objectives ==============\n",
    "class DefaultObjective(Enum):\n",
    "    regression = \"regression\"\n",
    "    binary_classification = \"binary\"\n",
    "    multiclass_classification = \"multiclass\"\n",
    "\n",
    "\n",
    "# ============== LightGBM Model ==============\n",
    "class LGBMModel(BaseKtoolsModel):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_boost_round: int = 100,\n",
    "        early_stopping_rounds: Union[int, None] = 20,\n",
    "        random_state: int = 129,\n",
    "        verbose: int = -1,\n",
    "        n_jobs: int = 1,\n",
    "        callbacks: List[Any] = [],\n",
    "        **lgb_param_grid,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self._num_boost_round = num_boost_round\n",
    "        self._verbose = verbose\n",
    "        self._n_jobs = n_jobs\n",
    "        self._callbacks = callbacks\n",
    "        self.early_stopping_rounds = early_stopping_rounds\n",
    "\n",
    "        self._lgb_param_grid = {\n",
    "            \"verbose\": verbose,\n",
    "            \"random_state\": random_state,\n",
    "            \"n_jobs\": n_jobs,\n",
    "            **lgb_param_grid,\n",
    "        }\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        X: T,\n",
    "        y: T,\n",
    "        validation_set: Optional[Tuple[T, T]] = None,\n",
    "        weights: Optional[T] = None,\n",
    "        val_weights: Optional[T] = None\n",
    "    ) -> \"LGBMModel\":\n",
    "        if \"objective\" not in self._lgb_param_grid:\n",
    "            task_id = infer_task(y)\n",
    "            self._lgb_param_grid[\"objective\"] = DefaultObjective[task_id].value\n",
    "            if task_id == \"multiclass_classification\":\n",
    "                self._lgb_param_grid[\"num_class\"] = np.unique(y).shape[0]\n",
    "\n",
    "        train_data = lgb.Dataset(X, label=y, weight=weights)\n",
    "        eval_sets = [train_data]\n",
    "        eval_names = [\"train\"]\n",
    "        if validation_set is not None:\n",
    "            X_val, y_val = validation_set\n",
    "            val_data = lgb.Dataset(X_val, label=y_val, reference=train_data, weight=val_weights)\n",
    "            eval_sets += [val_data]\n",
    "            eval_names += [\"valid\"]\n",
    "            self._lgb_param_grid[\"early_stopping_rounds\"] = self.early_stopping_rounds\n",
    "\n",
    "        train_params = {\n",
    "            \"params\": self._lgb_param_grid,\n",
    "            \"train_set\": train_data,\n",
    "            \"num_boost_round\": self._num_boost_round,\n",
    "            \"valid_sets\": eval_sets,\n",
    "            \"valid_names\": eval_names,\n",
    "            \"callbacks\": self._callbacks,\n",
    "        }\n",
    "\n",
    "        self.model = lgb.train(**train_params)\n",
    "        self._fitted = True\n",
    "        return self\n",
    "\n",
    "    def predict(self, X: T) -> np.ndarray:\n",
    "        y_pred = self.model.predict(X)\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "from enum import Enum\n",
    "from typing import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "T = Union[np.ndarray, pd.DataFrame]\n",
    "\n",
    "\n",
    "class DefaultObjective(Enum):\n",
    "    regression = \"reg:squarederror\"\n",
    "    binary_classification = \"binary:logistic\"\n",
    "    multiclass_classification = \"multi:softprob\"\n",
    "\n",
    "\n",
    "class XGBoostModel(BaseKtoolsModel):\n",
    "    def __init__(\n",
    "        self,\n",
    "        eval_verbosity: bool = False,\n",
    "        num_boost_round: int = 100,\n",
    "        early_stopping_rounds: Union[int, None] = 20,\n",
    "        random_state: int = 129,\n",
    "        verbosity: int = 0,\n",
    "        n_jobs: int = 1,\n",
    "        **xgb_param_grid,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self._eval_verbosity = eval_verbosity\n",
    "        self._num_boost_round = num_boost_round\n",
    "        self._verbosity = verbosity\n",
    "        self._n_jobs = n_jobs\n",
    "        self._early_stopping_rounds = early_stopping_rounds\n",
    "\n",
    "        self._xgb_param_grid = {\n",
    "            \"verbosity\": verbosity,\n",
    "            \"random_state\": random_state,\n",
    "            \"n_jobs\": n_jobs,\n",
    "            **xgb_param_grid,\n",
    "        }\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        X: T,\n",
    "        y: T,\n",
    "        validation_set: Optional[Tuple[T, T]] = None,\n",
    "        weights: Optional[T] = None,\n",
    "        val_weights: Optional[T] = None\n",
    "    ) -> \"XGBoostModel\":\n",
    "        train_params = {}\n",
    "        if \"objective\" not in self._xgb_param_grid:\n",
    "            task_id = infer_task(y)\n",
    "            self._xgb_param_grid[\"objective\"] = DefaultObjective[task_id].value\n",
    "            if task_id == \"multiclass_classification\":\n",
    "                self._xgb_param_grid[\"num_class\"] = np.unique(y).shape[0]\n",
    "\n",
    "        train_data = xgb.DMatrix(X, label=y, enable_categorical=True, weight=weights)\n",
    "        eval_data = [(train_data, \"train\")]\n",
    "        if validation_set is not None:\n",
    "            X_val, y_val = validation_set\n",
    "            valid_data = xgb.DMatrix(X_val, label=y_val, enable_categorical=True, weight=val_weights)\n",
    "            eval_data += [(valid_data, \"eval\")]\n",
    "            train_params[\"early_stopping_rounds\"] = self._early_stopping_rounds\n",
    "\n",
    "        train_params = {\n",
    "            \"params\": self._xgb_param_grid,\n",
    "            \"dtrain\": train_data,\n",
    "            \"evals\": eval_data,\n",
    "            \"num_boost_round\": self._num_boost_round,\n",
    "            \"verbose_eval\": self._eval_verbosity,\n",
    "            **train_params,\n",
    "        }\n",
    "\n",
    "        self.model = xgb.train(**train_params)\n",
    "        self._fitted = True\n",
    "        return self\n",
    "\n",
    "    def predict(self, X: T) -> np.ndarray:\n",
    "        test_data = xgb.DMatrix(X, enable_categorical=True)\n",
    "        y_pred = self.model.predict(test_data)\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "\n",
    "# ============== Model Pipeline ==============\n",
    "class ModelPipeline:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: BaseKtoolsModel,\n",
    "        config: DatasetConfig,\n",
    "        preprocessor: PreprocessingPipeline = None,\n",
    "    ) -> None:\n",
    "        self.model = model\n",
    "        self.config = config\n",
    "        self.preprocessor = preprocessor if preprocessor else PreprocessingPipeline([])\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        train_data: pd.DataFrame,\n",
    "        validation_data: Optional[pd.DataFrame] = None,\n",
    "        weights: Optional[Union[pd.Series, np.ndarray]] = None,\n",
    "    ) -> \"ModelPipeline\":\n",
    "        train_data = self.preprocessor.train_pipe(train_data)\n",
    "        X_train = train_data.drop(columns=[self.config.target_col_name])\n",
    "        y_train = train_data[self.config.target_col_name]\n",
    "\n",
    "        if validation_data is not None:\n",
    "            validation_data = self.preprocessor.inference_pipe(validation_data)\n",
    "            X_valid = validation_data.drop(columns=[self.config.target_col_name])\n",
    "            y_valid = validation_data[self.config.target_col_name]\n",
    "            validation_data = (X_valid, y_valid)\n",
    "\n",
    "        self.model.fit(\n",
    "            X=X_train, y=y_train, validation_set=validation_data, weights=weights\n",
    "        )\n",
    "        return self\n",
    "\n",
    "    def predict(self, data: pd.DataFrame) -> np.ndarray:\n",
    "        data = self.preprocessor.inference_pipe(data)\n",
    "        X_test = data #[self.config.training_col_names]\n",
    "        return self.model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e6aaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============== CatBoost Model ==============\n",
    "import catboost as cat\n",
    "from catboost import Pool\n",
    "\n",
    "class CatBoostModel(BaseKtoolsModel):\n",
    "    \"\"\"CatBoost model wrapper\"\"\"\n",
    "    class DefaultObjective(Enum):\n",
    "        regression = \"RMSE\"\n",
    "        binary_classification = \"Logloss\"\n",
    "        multiclass_classification = \"MultiClass\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_boost_round: int = 100,\n",
    "        early_stopping_rounds: Optional[int] = 20,\n",
    "        random_state: int = 129,\n",
    "        verbose: bool = False,\n",
    "        allow_writing_files: bool = False,\n",
    "        **catboost_params,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.model: Union[cat.CatBoost, None] = None\n",
    "        self._task: str = \"\"\n",
    "        self._num_boost_round = num_boost_round\n",
    "        self._verbose = verbose\n",
    "        self._allow_writing_files = allow_writing_files\n",
    "        self._early_stopping_rounds = early_stopping_rounds\n",
    "\n",
    "        self._catboost_params = {\n",
    "            \"random_seed\": random_state,\n",
    "            \"verbose\": verbose,\n",
    "            \"allow_writing_files\": allow_writing_files,\n",
    "            **catboost_params,\n",
    "        }\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        X: T,\n",
    "        y: T,\n",
    "        validation_set: Optional[Tuple[T, T]] = None,\n",
    "        weights: Optional[T] = None,\n",
    "        val_weights: Optional[T] = None,\n",
    "    ) -> \"CatBoostModel\":\n",
    "        task_id = infer_task(y)\n",
    "        self._task = task_id\n",
    "        if \"loss_function\" not in self._catboost_params:\n",
    "            self._catboost_params[\"loss_function\"] = self.DefaultObjective[task_id].value\n",
    "\n",
    "        self.cat_col_names = (\n",
    "            [col for col in X.columns if X[col].dtype == \"category\"]\n",
    "            if isinstance(X, pd.DataFrame)\n",
    "            else []\n",
    "        )\n",
    "        train_params: Dict[Any, Any] = {\"eval_set\": None}\n",
    "        train_pool = Pool(\n",
    "            data=X, label=y, cat_features=self.cat_col_names, weight=weights\n",
    "        )\n",
    "        if validation_set is not None:\n",
    "            X_val, y_val = validation_set\n",
    "            train_params[\"eval_set\"] = Pool(\n",
    "                data=X_val, label=y_val, cat_features=self.cat_col_names, weight=val_weights\n",
    "            )\n",
    "            train_params[\"early_stopping_rounds\"] = self._early_stopping_rounds\n",
    "\n",
    "        train_params = {\n",
    "            \"params\": self._catboost_params,\n",
    "            \"dtrain\": train_pool,\n",
    "            \"num_boost_round\": self._num_boost_round,\n",
    "            **train_params,\n",
    "        }\n",
    "        self.model = cat.train(**train_params)\n",
    "        self._fitted = True\n",
    "        return self\n",
    "\n",
    "    def predict(self, X: T) -> np.ndarray:\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model is not fitted yet. Please call 'fit' first.\")\n",
    "        test_pool = Pool(data=X, cat_features=self.cat_col_names)\n",
    "        if self._task == \"binary_classification\":\n",
    "            y_pred = self.model.predict(test_pool, prediction_type=\"Probability\")[:, 1]\n",
    "        elif self._task == \"multiclass_classification\":\n",
    "            y_pred = self.model.predict(test_pool, prediction_type=\"Probability\")\n",
    "        else:\n",
    "            y_pred = self.model.predict(test_pool)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086d9a06",
   "metadata": {},
   "source": [
    "## CV Tunable Function & Optuna Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3aee52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from optuna.trial import Trial\n",
    "from copy import deepcopy\n",
    "import yaml\n",
    "\n",
    "NestedDict = dict[str, \"NestedDict | Any\"]\n",
    "TrialSampler = Callable[[Trial], Any]\n",
    "\n",
    "\n",
    "def load_optuna_grid(\n",
    "    path: str,\n",
    "    model_type: str,\n",
    "    extra_samplers: Dict[str, TrialSampler] | None = None,\n",
    ") -> Callable[[Trial], Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Load an Optuna parameter grid from a YAML file.\n",
    "    \"\"\"\n",
    "    with open(path, \"r\") as f:\n",
    "        param_grid: NestedDict = yaml.safe_load(f)\n",
    "    param_grid = param_grid.get(model_type, {})\n",
    "    if len(param_grid) == 0:\n",
    "        raise ValueError(f\"No parameter grid found for model type: {model_type}\")\n",
    "\n",
    "    def param_grid_getter(trial: Trial) -> Dict[str, Any]:\n",
    "        unpacked = {}\n",
    "        for param_name, param_info in param_grid.items():\n",
    "            dtype = param_info.get(\"type\")\n",
    "            if dtype == \"int\":\n",
    "                unpacked[param_name] = trial.suggest_int(\n",
    "                    param_name, param_info[\"low\"], param_info[\"high\"],\n",
    "                )\n",
    "            elif dtype == \"float\":\n",
    "                unpacked[param_name] = trial.suggest_float(\n",
    "                    param_name, param_info[\"low\"], param_info[\"high\"],\n",
    "                    log=param_info.get(\"log\", False),\n",
    "                )\n",
    "            elif dtype == \"categorical\":\n",
    "                unpacked[param_name] = trial.suggest_categorical(\n",
    "                    param_name, param_info[\"choices\"],\n",
    "                )\n",
    "            elif dtype == \"fixed\":\n",
    "                unpacked[param_name] = param_info[\"value\"]\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported parameter type: {dtype}\")\n",
    "\n",
    "        if extra_samplers:\n",
    "            for param_name, sampler in extra_samplers.items():\n",
    "                unpacked[param_name] = sampler(trial)\n",
    "\n",
    "        return unpacked\n",
    "\n",
    "    return param_grid_getter\n",
    "\n",
    "\n",
    "def cv_tunable_func(\n",
    "    train_data: pd.DataFrame,\n",
    "    orig_data: pd.DataFrame,\n",
    "    test_data: pd.DataFrame,\n",
    "    categories_of_interest: pd.Series,\n",
    "    config: DatasetConfig,\n",
    "    preprocessors: List[BasePreprocessor],\n",
    "    model_class: type,\n",
    "    weight_map: Dict[int, float],\n",
    "    n_splits: int = 5,\n",
    "    random_state: int = 42,\n",
    "    **model_params,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Cross-validation function to be tuned by Optuna.\n",
    "    \n",
    "    Follows the cv-experimentation pattern:\n",
    "    - Stratified K-Fold on (target + data source)\n",
    "    - Validation fold filtered to data == 0 only\n",
    "    - Sample weights based on data source\n",
    "    - Original data added to each training fold\n",
    "    \"\"\"\n",
    "    kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    mean_score: float = 0.0\n",
    "    \n",
    "    for train_idx, val_idx in kfold.split(train_data, categories_of_interest):\n",
    "        train_fold = train_data.iloc[train_idx].copy()\n",
    "        val_fold = train_data.iloc[val_idx].copy()\n",
    "        \n",
    "        # Filter validation to data == 0 only\n",
    "        subsetval_fold = val_fold[val_fold[\"data\"] == 0]\n",
    "        \n",
    "        # Add original data to training fold\n",
    "        train_fold = pd.concat([train_fold, orig_data], axis=0, ignore_index=True)\n",
    "        \n",
    "        # Sample weights from weight_map\n",
    "        weights = train_fold[\"data\"].map(weight_map).values\n",
    "        \n",
    "        # Fresh preprocessor pipeline for each fold\n",
    "        preprocessor_pipeline = PreprocessingPipeline(\n",
    "            preprocessors=[deepcopy(p) for p in preprocessors]\n",
    "        )\n",
    "        \n",
    "        pipe = ModelPipeline(\n",
    "            model=model_class(**model_params),\n",
    "            config=config,\n",
    "            preprocessor=preprocessor_pipeline,\n",
    "        )\n",
    "        \n",
    "        pipe.fit(train_fold, validation_data=subsetval_fold, weights=weights)\n",
    "        y_pred = pipe.predict(subsetval_fold.drop(columns=config.target_col_name))\n",
    "        \n",
    "        score = roc_auc_score(subsetval_fold[config.target_col_name], y_pred)\n",
    "        mean_score += score / n_splits\n",
    "    \n",
    "    return mean_score\n",
    "\n",
    "\n",
    "class OptunaHyperparameterOptimizer:\n",
    "    \"\"\"\n",
    "    Hyperparameter optimizer using Optuna's TPE sampler.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_type: str,\n",
    "        grid_yaml_path: str,\n",
    "        extra_samplers: Dict[str, TrialSampler] | None = None,\n",
    "        timeout: int = 3600,\n",
    "        direction: str = \"maximize\",\n",
    "        n_trials: int = 100,\n",
    "        study_name: str = \"ml_experiment\",\n",
    "        explore_fraction: float = 0.1,\n",
    "        save_study: bool = False,\n",
    "        load_if_exists: bool = True,\n",
    "        catch_exceptions: Tuple[type, ...] = (),\n",
    "        verbose: bool = False,\n",
    "        random_state: int = 42,\n",
    "    ) -> None:\n",
    "        self._param_space_builder = load_optuna_grid(grid_yaml_path, model_type, extra_samplers=extra_samplers)\n",
    "        self._timeout = timeout\n",
    "        self._direction = direction\n",
    "        self._n_trials = n_trials\n",
    "        self._study_name = study_name\n",
    "        self._explore_fraction = explore_fraction\n",
    "        self._save_study = save_study\n",
    "        self._load_if_exists = load_if_exists\n",
    "        self._catch_exceptions = catch_exceptions\n",
    "        self._verbose = verbose\n",
    "        self._random_state = random_state\n",
    "        self.study: optuna.Study | None = None\n",
    "\n",
    "    def optimize(\n",
    "        self,\n",
    "        *args: Any,\n",
    "        tunable_func: Callable[..., float],\n",
    "    ) -> dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Run hyperparameter optimization.\n",
    "        \"\"\"\n",
    "        if self._verbose:\n",
    "            print(\"#\" * 60)\n",
    "            print(\"Starting Optuna Optimizer\")\n",
    "            print(\"#\" * 60)\n",
    "\n",
    "        sampler = TPESampler(\n",
    "            n_startup_trials=int(self._n_trials * self._explore_fraction),\n",
    "            seed=self._random_state,\n",
    "        )\n",
    "\n",
    "        storage_name = f\"sqlite:///{self._study_name}.db\" if self._save_study else None\n",
    "\n",
    "        self.study = optuna.create_study(\n",
    "            sampler=sampler,\n",
    "            study_name=self._study_name,\n",
    "            direction=self._direction,\n",
    "            storage=storage_name,\n",
    "            load_if_exists=self._load_if_exists,\n",
    "        )\n",
    "\n",
    "        def objective(trial: optuna.Trial) -> float:\n",
    "            parameters = self._param_space_builder(trial)\n",
    "            return tunable_func(*args, **parameters)\n",
    "\n",
    "        self.study.optimize(\n",
    "            objective,\n",
    "            n_trials=self._n_trials,\n",
    "            timeout=self._timeout,\n",
    "            catch=self._catch_exceptions,\n",
    "        )\n",
    "\n",
    "        return self.study.best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f992d0b3",
   "metadata": {},
   "source": [
    "## Data Loading & Preprocessing\n",
    "\n",
    "This section implements the data preparation logic from the test file:\n",
    "1. Load original, training, and test data\n",
    "2. Assign data source labels (0=validation part of train, 1=train part, 2=original)\n",
    "3. Create aggregated features from original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be37af8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T01:38:56.775772Z",
     "iopub.status.busy": "2025-12-30T01:38:56.775033Z",
     "iopub.status.idle": "2025-12-30T01:38:56.779213Z",
     "shell.execute_reply": "2025-12-30T01:38:56.778513Z",
     "shell.execute_reply.started": "2025-12-30T01:38:56.775741Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============== Configuration ==============\n",
    "# For Kaggle, update these paths to match the competition data location\n",
    "DATA_PATH = Path(\"/kaggle/input/playground-series-s5e12\")\n",
    "# DATA_PATH = Path(\"./data/diabetes_prediction/\")  # Local path\n",
    "\n",
    "TARGET = \"diagnosed_diabetes\"\n",
    "SPLIT_ID = 678260  # Index to split training data into train/validation groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca313c39",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T01:38:57.387602Z",
     "iopub.status.busy": "2025-12-30T01:38:57.386881Z",
     "iopub.status.idle": "2025-12-30T01:39:00.048962Z",
     "shell.execute_reply": "2025-12-30T01:39:00.048237Z",
     "shell.execute_reply.started": "2025-12-30T01:38:57.387575Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============== Load Data ==============\n",
    "original_data = pd.read_csv(\"/kaggle/input/diabetes-health-indicators-dataset/diabetes_dataset.csv\")\n",
    "training_data = pd.read_csv(DATA_PATH / \"train.csv\", index_col=0)\n",
    "test_data = pd.read_csv(DATA_PATH / \"test.csv\", index_col=0).assign(data=0)\n",
    "\n",
    "print(f\"Original data shape: {original_data.shape}\")\n",
    "print(f\"Training data shape: {training_data.shape}\")\n",
    "print(f\"Test data shape: {test_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56dcd8e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T01:39:00.818222Z",
     "iopub.status.busy": "2025-12-30T01:39:00.817713Z",
     "iopub.status.idle": "2025-12-30T01:39:00.923127Z",
     "shell.execute_reply": "2025-12-30T01:39:00.922430Z",
     "shell.execute_reply.started": "2025-12-30T01:39:00.818196Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============== Assign Data Source Labels ==============\n",
    "# data=0: validation portion of training data (after SPLIT_ID)\n",
    "# data=1: training portion of training data (before SPLIT_ID)\n",
    "# data=2: original external data\n",
    "\n",
    "original_data = original_data.assign(data=2)\n",
    "training_data = training_data.assign(data=0)\n",
    "training_data.iloc[:SPLIT_ID, training_data.columns.get_loc('data')] = 1\n",
    "\n",
    "print(f\"Training data source distribution:\")\n",
    "print(training_data['data'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180763bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T01:39:02.878405Z",
     "iopub.status.busy": "2025-12-30T01:39:02.877918Z",
     "iopub.status.idle": "2025-12-30T01:39:22.649291Z",
     "shell.execute_reply": "2025-12-30T01:39:22.648573Z",
     "shell.execute_reply.started": "2025-12-30T01:39:02.878380Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============== Create Aggregated Features from Original Data ==============\n",
    "# Merge training, original and test data for feature engineering\n",
    "train_orig_test = pd.concat(\n",
    "    [training_data, original_data[training_data.columns], test_data], \n",
    "    axis=0, \n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "training_cols = training_data.columns.drop([\"data\", TARGET]).tolist()\n",
    "orig_target_mean = original_data[TARGET].mean()\n",
    "\n",
    "print(f\"Creating aggregated features from {len(training_cols)} columns...\")\n",
    "\n",
    "for c in training_cols:\n",
    "    for aggr in [\"mean\", \"count\"]:\n",
    "        col_name = f'{c}_org_{aggr}'\n",
    "        tmp = (\n",
    "            original_data.groupby(c)[TARGET]\n",
    "            .agg(aggr)\n",
    "            .rename(col_name)\n",
    "            .reset_index()\n",
    "        )\n",
    "        \n",
    "        train_orig_test = train_orig_test.merge(tmp, on=c, how='left')\n",
    "        fill_val = orig_target_mean if aggr == 'mean' else 0\n",
    "        train_orig_test[col_name] = train_orig_test[col_name].fillna(fill_val)\n",
    "\n",
    "print(f\"Combined data shape after feature engineering: {train_orig_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3792e6e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T01:39:44.202977Z",
     "iopub.status.busy": "2025-12-30T01:39:44.202702Z",
     "iopub.status.idle": "2025-12-30T01:39:44.694056Z",
     "shell.execute_reply": "2025-12-30T01:39:44.693319Z",
     "shell.execute_reply.started": "2025-12-30T01:39:44.202954Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============== Split Back into Train, Original, Test ==============\n",
    "len_train = training_data.shape[0]\n",
    "len_orig = original_data.shape[0]\n",
    "\n",
    "train_data = train_orig_test.iloc[:len_train, :].copy()\n",
    "orig_data = train_orig_test.iloc[len_train:len_train+len_orig, :].copy()\n",
    "test_data = train_orig_test.iloc[len_train+len_orig:, :].copy().drop(columns=[TARGET])\n",
    "\n",
    "print(f\"Final train data shape: {train_data.shape}\")\n",
    "print(f\"Final original data shape: {orig_data.shape}\")\n",
    "print(f\"Final test data shape: {test_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e7316f",
   "metadata": {},
   "source": [
    "## Dataset Configuration Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49165468",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T01:39:48.402812Z",
     "iopub.status.busy": "2025-12-30T01:39:48.402236Z",
     "iopub.status.idle": "2025-12-30T01:39:48.596254Z",
     "shell.execute_reply": "2025-12-30T01:39:48.595520Z",
     "shell.execute_reply.started": "2025-12-30T01:39:48.402787Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============== Define Feature Categories ==============\n",
    "all_features = train_data.columns.drop(TARGET).tolist()\n",
    "\n",
    "# Categorical features include object/bool types + specific columns\n",
    "categorical_features = (\n",
    "    train_data.drop(columns=TARGET)\n",
    "    .select_dtypes(include=['object', 'bool'])\n",
    "    .columns.tolist() \n",
    "    + ['family_history_diabetes', 'hypertension_history', 'cardiovascular_history']\n",
    ")\n",
    "\n",
    "# Remove duplicates\n",
    "categorical_features = list(set(categorical_features))\n",
    "\n",
    "numerical_features = [col for col in all_features if col not in categorical_features]\n",
    "\n",
    "print(f\"Total features: {len(all_features)}\")\n",
    "print(f\"Categorical features: {len(categorical_features)}\")\n",
    "print(f\"Numerical features: {len(numerical_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2628b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T01:39:50.938700Z",
     "iopub.status.busy": "2025-12-30T01:39:50.938151Z",
     "iopub.status.idle": "2025-12-30T01:39:51.411332Z",
     "shell.execute_reply": "2025-12-30T01:39:51.410576Z",
     "shell.execute_reply.started": "2025-12-30T01:39:50.938673Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============== Create Dataset Config ==============\n",
    "config = DatasetConfig(\n",
    "    training_col_names=all_features,\n",
    "    categorical_col_names=categorical_features,\n",
    "    numerical_col_names=numerical_features,\n",
    "    target_col_name=TARGET,\n",
    ")\n",
    "\n",
    "# Convert categorical columns to category dtype\n",
    "train_data[categorical_features] = train_data[categorical_features].astype('category')\n",
    "orig_data[categorical_features] = orig_data[categorical_features].astype('category')\n",
    "test_data[categorical_features] = test_data[categorical_features].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78f8135",
   "metadata": {},
   "source": [
    "## Optuna Hyperparameter Tuning\n",
    "\n",
    "Use the `OptunaHyperparameterOptimizer` with `cv_tunable_func` to tune XGBoost, LightGBM, or CatBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15776c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============== Setup for Optuna Tuning ==============\n",
    "# Stratification categories\n",
    "categories_of_interest = (\n",
    "    train_data[TARGET].astype(str) + \"_\" + train_data[\"data\"].astype(str)\n",
    ")\n",
    "\n",
    "# Sample weights based on data source\n",
    "WEIGHT_MAP = {2: 8, 0: 16, 1: 1}\n",
    "\n",
    "# Choose model to tune: XGBoostModel, LGBMModel, or CatBoostModel\n",
    "MODEL_CLASS = XGBoostModel\n",
    "GRID_YAML_PATH = \"ktools/hyperopt/grids/xgb.yml\"  # Update path as needed\n",
    "MODEL_TYPE = \"base\"  # Key in YAML file\n",
    "\n",
    "# Extra tunable parameters (e.g., sample weights)\n",
    "extra_samplers = {\n",
    "    \"weight_data0\": lambda t: t.suggest_float(\"weight_data0\", 1.0, 20.0),\n",
    "    \"weight_data2\": lambda t: t.suggest_float(\"weight_data2\", 1.0, 15.0),\n",
    "}\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = OptunaHyperparameterOptimizer(\n",
    "    model_type=MODEL_TYPE,\n",
    "    grid_yaml_path=GRID_YAML_PATH,\n",
    "    extra_samplers=extra_samplers,\n",
    "    timeout=7200,  # 2 hours\n",
    "    direction=\"maximize\",\n",
    "    n_trials=100,\n",
    "    study_name=\"xgb_cv_tuning\",\n",
    "    explore_fraction=0.1,\n",
    "    save_study=True,\n",
    "    verbose=True,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "print(f\"Optimizer configured for {MODEL_CLASS.__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52929b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============== Run Optuna Optimization ==============\n",
    "# Uncomment to run (can take a long time!)\n",
    "\n",
    "# def tunable_wrapper(\n",
    "#     train_data, orig_data, test_data, categories_of_interest, config,\n",
    "#     preprocessors, model_class, weight_map, n_splits, random_state,\n",
    "#     weight_data0=16, weight_data2=8, **model_params\n",
    "# ):\n",
    "#     \"\"\"Wrapper to inject tunable weights into weight_map.\"\"\"\n",
    "#     dynamic_weight_map = {0: weight_data0, 1: 1, 2: weight_data2}\n",
    "#     return cv_tunable_func(\n",
    "#         train_data=train_data,\n",
    "#         orig_data=orig_data,\n",
    "#         test_data=test_data,\n",
    "#         categories_of_interest=categories_of_interest,\n",
    "#         config=config,\n",
    "#         preprocessors=preprocessors,\n",
    "#         model_class=model_class,\n",
    "#         weight_map=dynamic_weight_map,\n",
    "#         n_splits=n_splits,\n",
    "#         random_state=random_state,\n",
    "#         **model_params,\n",
    "#     )\n",
    "\n",
    "# preprocessors = [\n",
    "#     CategoricalFrequencyEncoder(config=config),\n",
    "#     CategoricalTargetEncoder(config=config),\n",
    "# ]\n",
    "\n",
    "# best_params = optimizer.optimize(\n",
    "#     train_data,\n",
    "#     orig_data,\n",
    "#     test_data,\n",
    "#     categories_of_interest,\n",
    "#     config,\n",
    "#     preprocessors,\n",
    "#     MODEL_CLASS,\n",
    "#     WEIGHT_MAP,\n",
    "#     5,  # n_splits\n",
    "#     42,  # random_state\n",
    "#     tunable_func=tunable_wrapper,\n",
    "# )\n",
    "\n",
    "# print(f\"\\nBest Score: {optimizer.study.best_value:.6f}\")\n",
    "# print(f\"Best Params: {best_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d048671",
   "metadata": {},
   "source": [
    "## Cross-Validation Experiment\n",
    "\n",
    "Using Stratified K-Fold with stratification on both target and data source (from cell 8 of the original notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074711f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T01:39:53.485523Z",
     "iopub.status.busy": "2025-12-30T01:39:53.484808Z",
     "iopub.status.idle": "2025-12-30T01:39:54.046531Z",
     "shell.execute_reply": "2025-12-30T01:39:54.045731Z",
     "shell.execute_reply.started": "2025-12-30T01:39:53.485487Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============== Stratification Categories ==============\n",
    "# Combined stratification on target + data source\n",
    "categories_of_interest = (\n",
    "    train_data[TARGET].astype(str) + \"_\" + train_data[\"data\"].astype(str)\n",
    ")\n",
    "\n",
    "print(\"Stratification category distribution:\")\n",
    "print(categories_of_interest.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74597fec-331b-42d4-88da-63f9a0b19a99",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T01:39:54.977642Z",
     "iopub.status.busy": "2025-12-30T01:39:54.977109Z",
     "iopub.status.idle": "2025-12-30T01:39:54.981915Z",
     "shell.execute_reply": "2025-12-30T01:39:54.981261Z",
     "shell.execute_reply.started": "2025-12-30T01:39:54.977615Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "DEVICE = \"gpu\"\n",
    "SEED = 42\n",
    "\n",
    "XGB_PARAMS = {\n",
    "    'tree_method': 'hist',\n",
    "    'objective': 'binary:logistic',\n",
    "    'random_state': SEED,\n",
    "    'eval_metric': 'auc',\n",
    "    'booster': 'gbtree',\n",
    "    'n_jobs': -1,\n",
    "    'learning_rate': 0.01,\n",
    "    \"device\": DEVICE,\n",
    "    'lambda': 1.134330929497114,\n",
    "    'alpha': 6.780537184218281,\n",
    "    'colsample_bytree': 0.1007615968427798,\n",
    "    'subsample': 0.7215917619002097,\n",
    "    'max_depth': 4,\n",
    "    'min_child_weight': 5,\n",
    "    'num_boost_round' : 100_000,\n",
    "    'early_stopping_rounds':200,\n",
    "    # Note: enable_categorical is passed to DMatrix, not params in native API usually,\n",
    "    # but kept here for consistency with modern XGBoost versions.\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7631fd98",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T01:44:57.472178Z",
     "iopub.status.busy": "2025-12-30T01:44:57.471666Z",
     "iopub.status.idle": "2025-12-30T01:49:46.857126Z",
     "shell.execute_reply": "2025-12-30T01:49:46.856483Z",
     "shell.execute_reply.started": "2025-12-30T01:44:57.472152Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============== CV Training Loop ==============\n",
    "N_SPLITS = 5\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Initialize arrays for predictions\n",
    "train_oof_preds = np.empty(train_data.shape[0])\n",
    "test_oof_preds = np.zeros(test_data.shape[0])\n",
    "\n",
    "# Sample weights based on data source\n",
    "# data=2 (original): weight 8\n",
    "# data=0 (val portion): weight 16  \n",
    "# data=1 (train portion): weight 1\n",
    "WEIGHT_MAP = {2: 8, 0: 16, 1: 1}\n",
    "\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
    "mean_score = 0.0\n",
    "fold_scores = []\n",
    "\n",
    "for fold_idx, (train_idx, val_idx) in enumerate(kfold.split(train_data, categories_of_interest)):\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Fold {fold_idx + 1}/{N_SPLITS}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Split data\n",
    "    train_fold = train_data.iloc[train_idx].copy()\n",
    "    val_fold = train_data.iloc[val_idx].copy()\n",
    "    \n",
    "    # Subset validation fold for scoring (only data=0, the \"real\" validation portion)\n",
    "    subsetval_fold = val_fold[val_fold[\"data\"] == 0]\n",
    "    \n",
    "    # Add original data to training fold\n",
    "    train_fold = pd.concat([train_fold, orig_data], axis=0, ignore_index=True)\n",
    "    \n",
    "    print(f\"Train fold size: {train_fold.shape[0]} (including {orig_data.shape[0]} original samples)\")\n",
    "    print(f\"Validation fold size: {val_fold.shape[0]} (subset for scoring: {subsetval_fold.shape[0]})\")\n",
    "    \n",
    "    # Calculate sample weights\n",
    "    weights = train_fold[\"data\"].map(WEIGHT_MAP).values\n",
    "    \n",
    "    # Use ModelPipeline with PreprocessingPipeline\n",
    "\n",
    "    preprocessors = [\n",
    "        CategoricalFrequencyEncoder(config=config),\n",
    "        CategoricalTargetEncoder(config=config),\n",
    "    ]\n",
    "    \n",
    "    pipe = ModelPipeline(\n",
    "        model=XGBoostModel(**XGB_PARAMS),\n",
    "        config=config,\n",
    "        preprocessor=PreprocessingPipeline(preprocessors=preprocessors),\n",
    "    )\n",
    "    \n",
    "    pipe.fit(train_fold, validation_data=subsetval_fold, weights=weights)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = pipe.predict(subsetval_fold.drop(columns=TARGET))\n",
    "    test_pred = pipe.predict(test_data)\n",
    "    oof_pred = pipe.predict(val_fold.drop(columns=TARGET))\n",
    "    \n",
    "    # Calculate fold score on SUBSET validation (data=0 only)\n",
    "    fold_score = roc_auc_score(subsetval_fold[TARGET], y_pred)\n",
    "    fold_scores.append(fold_score)\n",
    "    \n",
    "    # Store OOF predictions for full validation fold\n",
    "    train_oof_preds[val_idx] = oof_pred\n",
    "    test_oof_preds += test_pred / N_SPLITS\n",
    "    mean_score += fold_score / N_SPLITS\n",
    "    \n",
    "    print(f\"Fold {fold_idx + 1} ROC AUC Score: {fold_score:.6f}\")\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"CV Results\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Individual fold scores: {[f'{s:.6f}' for s in fold_scores]}\")\n",
    "print(f\"Mean ROC AUC Score: {mean_score:.6f}\")\n",
    "print(f\"Std ROC AUC Score: {np.std(fold_scores):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118bd042",
   "metadata": {},
   "source": [
    "## OOF Score & Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1975cd90",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T01:49:58.304431Z",
     "iopub.status.busy": "2025-12-30T01:49:58.303943Z",
     "iopub.status.idle": "2025-12-30T01:49:58.472833Z",
     "shell.execute_reply": "2025-12-30T01:49:58.472229Z",
     "shell.execute_reply.started": "2025-12-30T01:49:58.304405Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============== Overall OOF Score ==============\n",
    "overall_oof_score = roc_auc_score(train_data[TARGET], train_oof_preds)\n",
    "print(f\"Overall OOF ROC AUC Score: {overall_oof_score:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8590d36b-b986-47e8-80c6-6dfb1ba1496b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T01:51:06.217121Z",
     "iopub.status.busy": "2025-12-30T01:51:06.216536Z",
     "iopub.status.idle": "2025-12-30T01:51:06.746181Z",
     "shell.execute_reply": "2025-12-30T01:51:06.745402Z",
     "shell.execute_reply.started": "2025-12-30T01:51:06.217092Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({'id': test_data.index, TARGET: test_oof_preds})\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81351ba8",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-12-30T01:38:27.731758Z",
     "iopub.status.idle": "2025-12-30T01:38:27.732084Z",
     "shell.execute_reply": "2025-12-30T01:38:27.731938Z",
     "shell.execute_reply.started": "2025-12-30T01:38:27.731919Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============== Create Submission ==============\n",
    "# For Kaggle submission\n",
    "# sample_sub = pd.read_csv(DATA_PATH / \"sample_submission.csv\", index_col=0)\n",
    "# sample_sub[TARGET] = test_oof_preds\n",
    "# sample_sub.to_csv(\"submission.csv\")\n",
    "# print(\"Submission saved!\")\n",
    "\n",
    "# Preview predictions\n",
    "print(f\"Test predictions stats:\")\n",
    "print(f\"  Min: {test_oof_preds.min():.6f}\")\n",
    "print(f\"  Max: {test_oof_preds.max():.6f}\")\n",
    "print(f\"  Mean: {test_oof_preds.mean():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75b82bb",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-12-30T01:38:27.733020Z",
     "iopub.status.idle": "2025-12-30T01:38:27.733233Z",
     "shell.execute_reply": "2025-12-30T01:38:27.733144Z",
     "shell.execute_reply.started": "2025-12-30T01:38:27.733132Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============== Save OOF and Test Predictions ==============\n",
    "import uuid\n",
    "\n",
    "guid = uuid.uuid4()\n",
    "print(f\"Experiment GUID: {guid}\")\n",
    "\n",
    "# Uncomment to save predictions\n",
    "# save_path = DATA_PATH\n",
    "# pd.DataFrame({f\"{guid}\": train_oof_preds}).to_csv(save_path / \"oofs\" / f\"oof_preds_{guid}.csv\")\n",
    "# pd.DataFrame({f\"{guid}\": test_oof_preds}).to_csv(save_path / \"test_preds\" / f\"test_preds_{guid}.csv\")\n",
    "# print(f\"Predictions saved with GUID: {guid}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 14272474,
     "sourceId": 91723,
     "sourceType": "competition"
    },
    {
     "datasetId": 8316713,
     "sourceId": 13128284,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9118361,
     "sourceId": 14295460,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31234,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "kaggle-tools",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
