{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "import os\n",
    "import random\n",
    "from typing import Any, Dict, List, Union\n",
    "import torch\n",
    "from copy import deepcopy\n",
    "from dataclasses import dataclass\n",
    "from typing import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(x):\n",
    "    return x\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataSciencePipelineSettings:\n",
    "    train_csv_path : str\n",
    "    test_csv_path : str\n",
    "    target_col_name : str\n",
    "    original_csv_path : str = None\n",
    "    original_csv_processing : callable = func\n",
    "    sample_submission_path : str = None\n",
    "    training_col_names : List[str] = None\n",
    "    categorical_col_names : List[str] = None\n",
    "    training_data_percentage : float = 0.8\n",
    "    category_occurrence_threshold : int = 300\n",
    "    logged : bool = False\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.train_df, self.test_df = self._load_csv_paths()\n",
    "        self.training_col_names, self.categorical_col_names = self._get_column_info()\n",
    "        self.combined_df = self._combine_datasets()\n",
    "\n",
    "    def _load_csv_paths(self):\n",
    "        train_df = self._smart_drop_index(pd.read_csv(self.train_csv_path))\n",
    "        test_df = self._smart_drop_index(pd.read_csv(self.test_csv_path))\n",
    "        if self.original_csv_path is not None:\n",
    "            train_df = train_df.assign(source=0)\n",
    "            test_df = test_df.assign(source=0)\n",
    "            original_df = self._smart_drop_index(pd.read_csv(self.original_csv_path)).assign(source=1)\n",
    "            original_df = self.original_csv_processing(original_df)\n",
    "\n",
    "            pd.testing.assert_index_equal(train_df.columns.sort_values(), original_df.columns.sort_values(), check_exact=True)\n",
    "            pd.testing.assert_series_equal(train_df.dtypes.sort_index(), original_df.dtypes.sort_index(), check_exact=True)\n",
    "            train_df = pd.concat([train_df, original_df], axis=0).reset_index(drop=True)\n",
    "\n",
    "        return train_df, test_df\n",
    "    \n",
    "    def _get_column_info(self):\n",
    "        cat_col_names = [col_name for col_name in self.train_df.columns if self.train_df[col_name].dtype == 'object']\n",
    "        training_features = list(self.train_df.drop(columns=self.target_col_name).columns)\n",
    "        return training_features, cat_col_names\n",
    "    \n",
    "    def _combine_datasets(self):\n",
    "        combined_df = pd.concat([self.train_df, self.test_df], keys=['train', 'test'])\n",
    "        return combined_df\n",
    "    \n",
    "    def update(self):\n",
    "        self.train_df = self.combined_df.loc['train'].copy()\n",
    "        self.test_df = self.combined_df.loc['test'].copy()\n",
    "        return self.train_df, self.test_df        \n",
    "\n",
    "    @staticmethod\n",
    "    def _smart_drop_index(df):\n",
    "        try:\n",
    "            differences = df.iloc[:, 0].diff().dropna()\n",
    "            if differences.nunique() == 1:\n",
    "                df = df.drop(columns=df.columns[0])\n",
    "        except:\n",
    "            pass\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FillNullValues():\n",
    "    @staticmethod\n",
    "    def transform(original_settings : DataSciencePipelineSettings, numeric_fill=-1, category_fill='missing'):\n",
    "        settings = deepcopy(original_settings)\n",
    "        for col_name in settings.training_col_names:\n",
    "            if pd.api.types.is_numeric_dtype(settings.combined_df[col_name]):\n",
    "                settings.combined_df[col_name] = settings.combined_df[col_name].fillna(numeric_fill)\n",
    "            else:\n",
    "                settings.combined_df[col_name] = settings.combined_df[col_name].fillna(category_fill)\n",
    "        return settings\n",
    "    \n",
    "\n",
    "class ConvertObjectToCategorical():\n",
    "    @staticmethod\n",
    "    def transform(original_settings : DataSciencePipelineSettings):\n",
    "        settings = deepcopy(original_settings)\n",
    "        cat_cols = settings.categorical_col_names\n",
    "        settings.combined_df[cat_cols] = settings.combined_df[cat_cols].astype('category')\n",
    "        return settings\n",
    "    \n",
    "class LogTransformTarget():\n",
    "    @staticmethod\n",
    "    def transform(original_settings : DataSciencePipelineSettings):\n",
    "        settings = deepcopy(original_settings)\n",
    "        settings.combined_df[settings.target_col_name] = np.log1p(settings.combined_df[settings.target_col_name])\n",
    "        return settings\n",
    "\n",
    "class CreateYuweiFeatures():\n",
    "\n",
    "    @staticmethod\n",
    "    def transform(original_settings : DataSciencePipelineSettings):\n",
    "        settings = deepcopy(original_settings)\n",
    "\n",
    "        df = settings.combined_df\n",
    "\n",
    "\n",
    "        cat_cols = [col for col in df.columns if df[col].dtype == 'object']\n",
    "        \n",
    "        df['Policy Start Date'] = pd.to_datetime(df['Policy Start Date'])\n",
    "        df['Days Passed'] = (df['Policy Start Date'].max() - df['Policy Start Date']).dt.days\n",
    "        df['numYear'] = df['Policy Start Date'].dt.year\n",
    "        num_day = df['Policy Start Date'].dt.day\n",
    "        df['numMonth'] = df['Policy Start Date'].dt.month\n",
    "        \n",
    "        df['catMonth_name'] = df['Policy Start Date'].dt.month_name()\n",
    "        df['catDay_of_week'] = df['Policy Start Date'].dt.day_name()\n",
    "        \n",
    "        df['numWeek'] = df['Policy Start Date'].dt.isocalendar().week\n",
    "        \n",
    "        df['numYear_sin'] = np.sin(2 * np.pi * df['numYear'])\n",
    "        df['numMonth_sin'] = np.sin(2 * np.pi * df['numMonth'] / 12) \n",
    "        df['numMonth_cos'] = np.cos(2 * np.pi * df['numMonth'] / 12)\n",
    "        df['numGroup']=(df['numYear']-2020)*48+df['numMonth']*4+num_day//7\n",
    "        df['contract length'] = pd.cut(df[\"Insurance Duration\"].fillna(99),  \n",
    "        bins=[-float('inf'), 1, 3, float('inf')], labels=[0, 1, 2]).astype(int)\n",
    "        \n",
    "        print(\"done initial time features\")\n",
    "\n",
    "        cat_cols += ['catMonth_name', 'catDay_of_week']\n",
    "        temp = [x for x in cat_cols if x not in ['Location', \n",
    "                                                 'Education Level', \n",
    "                                                 'Policy Type', \n",
    "                                                 'Smoking Status',\n",
    "                                                 'Marital Status',\n",
    "                                                 'Exercise Frequency',\n",
    "                                                 'Gender', \n",
    "                                                 'Occupation', \n",
    "                                                 'catMonth_name', \n",
    "                                                 'Property Type', \n",
    "                                                 'catMonth_name']]\n",
    "        for col in temp:\n",
    "            print(f\"frequency encoding col: {col}\")\n",
    "            freq_encoding = df[col].value_counts().to_dict()\n",
    "\n",
    "            new_col_name = f\"{col}_freq\"\n",
    "            df[new_col_name] = df[col].map(freq_encoding)\n",
    "            df[new_col_name] = df[col].map(freq_encoding)\n",
    "\n",
    "            cat_cols += [new_col_name]\n",
    "\n",
    "\n",
    "        num_cols = [col for col in df.columns if col not in cat_cols]\n",
    "        df[cat_cols] = df[cat_cols].fillna('None').astype(str).astype('category')\n",
    "        df[num_cols] = df[num_cols].fillna(0).astype(float)\n",
    "\n",
    "\n",
    "        df['cat_annual_income'] = df['Annual Income'].astype(str).astype('category')\n",
    "        df['cat_health_score'] = df['Health Score'].astype(str).astype('category')\n",
    "        df['cat_credit_score'] = df['Credit Score'].astype(str).astype('category')\n",
    "\n",
    "        df['catHealth vs Claims'] = df['Health Score'] / (df['Previous Claims'] + 2)\n",
    "        # df['catClaims v Duration'] = df['Previous Claims'] / df['Insurance Duration']\n",
    "        # df['Cat Credit Score'] = df['Credit Score'].copy()\n",
    "        df['catInt Credit Score'] = df['Credit Score'].apply(lambda x: int(x) if pd.notna(x) else x)\n",
    "        df['HealthScore'] = df['Health Score'].apply(lambda x: int(x) if pd.notna(x) else x)\n",
    "\n",
    "        # df['HealthScore'] = df['Health Score'].apply(lambda x: int(x) if pd.notna(x) else x)\n",
    "        \n",
    "        settings.categorical_col_names += [\n",
    "                                            'catHealth vs Claims',\n",
    "                                            'catInt Credit Score'\n",
    "                                        ]\n",
    "        # df = df.drop(columns='Policy Start Date')\n",
    "        # settings.categorical_col_names.remove('Policy Start Date')\n",
    "        \n",
    "        df[settings.categorical_col_names] = df[settings.categorical_col_names].astype(str)\n",
    "        settings.combined_df = df\n",
    "        return settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for root, dirs, files in os.walk(\"/kaggle/input/\"):\n",
    "#     for file in files:\n",
    "#         file_path = os.path.join(root, file)\n",
    "#         print(file_path)\n",
    "#         if \"train.csv\" in file_path:\n",
    "#             train_csv_path = file_path\n",
    "#         elif \"test.csv\" in file_path:\n",
    "#             test_csv_path = file_path\n",
    "# target_col_name = pd.read_csv(train_csv_path).columns[-1]\n",
    "\n",
    "train_csv_path = \"/Users/yuwei-1/Documents/projects/Kaggle-tools/data/insurance/train.csv\"\n",
    "original_csv_path = \"/Users/yuwei-1/Documents/projects/Kaggle-tools/data/insurance/original.csv\"\n",
    "test_csv_path = \"/Users/yuwei-1/Documents/projects/Kaggle-tools/data/insurance/test.csv\"\n",
    "sample_sub_csv_path = \"/Users/yuwei-1/Documents/projects/Kaggle-tools/data/insurance/sample_submission.csv\"\n",
    "target_col_name = \"Premium Amount\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from typing import Any, Dict, List, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, root_mean_squared_error\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "\n",
    "\n",
    "class CrossValidationExecutor:\n",
    "\n",
    "    def __init__(self,\n",
    "                 sklearn_model_instance,\n",
    "                 evaluation_metric : callable,\n",
    "                 kfold_object,\n",
    "                 use_test_as_valid=True,\n",
    "                 num_classes=None,\n",
    "                 verbose=1) -> None:\n",
    "        \n",
    "        self.model = sklearn_model_instance\n",
    "        self._evaluation_metric = evaluation_metric\n",
    "        self._kf = kfold_object\n",
    "        self._num_splits = kfold_object.get_n_splits()\n",
    "        self._use_test_as_valid = use_test_as_valid\n",
    "        self._num_classes = num_classes\n",
    "        self._verbose = verbose\n",
    "\n",
    "    def run(self, X, y, additional_data=None, local_transform_list=[lambda x : x], output_transform_list=[lambda x : x]) -> Tuple[Tuple[float], np.ndarray, List[Any]]:\n",
    "        if additional_data is not None:\n",
    "            X_add, y_add = additional_data\n",
    "            pd.testing.assert_index_equal(X.columns, X_add.columns, check_exact=True)\n",
    "            pd.testing.assert_series_equal(X.dtypes, X_add.dtypes, check_exact=True)\n",
    "            pd.testing.assert_index_equal(y.columns, y_add.columns, check_exact=True)\n",
    "            pd.testing.assert_series_equal(y.dtypes, y_add.dtypes, check_exact=True)\n",
    "\n",
    "        cv_results = []\n",
    "        model_list = []\n",
    "        oof_predictions = np.zeros(y.shape[0]) if self._num_classes is None else np.zeros((y.shape[0], self._num_classes))\n",
    "        metric_predictions = np.zeros(y.shape[0]) if self._num_classes is None else np.zeros((y.shape[0], self._num_classes))\n",
    "\n",
    "        for i, (train_index, val_index) in enumerate(self._kf.split(X, y)):\n",
    "            \n",
    "            X_train, X_test = X.iloc[train_index], X.iloc[val_index]\n",
    "            y_train, y_test = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "            if additional_data is not None:\n",
    "                X_train = pd.concat([X_train, X_add], axis=0)\n",
    "                y_train = pd.concat([y_train, y_add], axis=0)\n",
    "\n",
    "            X_train, y_train = reduce(lambda acc, func: func(acc), local_transform_list, (X_train, y_train))\n",
    "            validation_set = None\n",
    "            if self._use_test_as_valid:\n",
    "                validation_set = [X_test, y_test]\n",
    "\n",
    "            model = deepcopy(self.model).fit(X_train, y_train, validation_set=validation_set)\n",
    "            model_list += [model]\n",
    "            y_pred = model.predict(X_test)\n",
    "            y_pred_processed = reduce(lambda acc, func: func(acc), output_transform_list, y_pred)\n",
    "            \n",
    "            cv_results += [self._evaluation_metric(y_test, y_pred_processed)]\n",
    "            oof_predictions[val_index] = y_pred\n",
    "            metric_predictions[val_index] = y_pred_processed\n",
    "\n",
    "            if self._verbose > 1:\n",
    "                print(f\"The CV results of the current fold is {cv_results[-1]}\")\n",
    "\n",
    "        oof_score = self._evaluation_metric(y, metric_predictions)\n",
    "        mean_cv_score = np.mean(cv_results)\n",
    "        score_tuple = (oof_score, mean_cv_score)\n",
    "\n",
    "        if self._verbose > 0:\n",
    "            print(\"#\"*100)\n",
    "            print(\"OOF prediction score : \", oof_score)\n",
    "            print(f\"Mean {self._num_splits}-cv results : {mean_cv_score} +- {np.std(cv_results)}\")\n",
    "            print(\"#\"*100)\n",
    "\n",
    "        return score_tuple, oof_predictions, model_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import sys\n",
    "from lightgbm import early_stopping, log_evaluation\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "class LGBMModel():\n",
    "\n",
    "    def __init__(self,\n",
    "                 num_boost_round=100,\n",
    "                 early_stopping_rounds=20,\n",
    "                 random_state=129,\n",
    "                 verbose=-1,\n",
    "                 n_jobs=1,\n",
    "                 **lgb_param_grid,) -> None:\n",
    "        super().__init__()\n",
    "        self._num_boost_round = num_boost_round\n",
    "        self._lgb_param_grid = {\"verbose\" : verbose, \n",
    "                                \"early_stopping_rounds\" : early_stopping_rounds,\n",
    "                                \"random_state\" : random_state,\n",
    "                                \"n_jobs\" : n_jobs,\n",
    "                                **lgb_param_grid}\n",
    "        self._callbacks = [\n",
    "                            # log_evaluation(period=log_period), \n",
    "                            # early_stopping(stopping_rounds=stopping_rounds)\n",
    "                           ]\n",
    "        self._random_state = random_state\n",
    "        \n",
    "    def fit(self, X, y, validation_set = None, val_size=0.05):\n",
    "        if validation_set is None:\n",
    "            X_train, X_valid, y_train, y_valid = train_test_split(X, \n",
    "                                                                  y, \n",
    "                                                                  test_size=val_size, \n",
    "                                                                  random_state=self._random_state)\n",
    "        else:\n",
    "            X_train, y_train = X, y\n",
    "            X_valid, y_valid = validation_set\n",
    "\n",
    "        train_data = lgb.Dataset(X_train, label=y_train)\n",
    "        val_data = lgb.Dataset(X_valid, label=y_valid, reference=train_data)\n",
    "        self.model = lgb.train(self._lgb_param_grid,\n",
    "                                train_data,\n",
    "                                num_boost_round=self._num_boost_round,\n",
    "                                valid_sets=[train_data, val_data],\n",
    "                                valid_names=['train', 'valid'],\n",
    "                                callbacks=self._callbacks,\n",
    "                                )\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = self.model.predict(X)\n",
    "        return y_pred\n",
    "    \n",
    "\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import catboost as cat\n",
    "from sklearn.model_selection import train_test_split\n",
    "from catboost import CatBoostRegressor, Pool, train\n",
    "\n",
    "\n",
    "class CatBoostModel():\n",
    "\n",
    "    def __init__(self,\n",
    "                 num_boost_round=100,\n",
    "                 early_stopping_rounds=20,\n",
    "                 random_state=129,\n",
    "                 predict_type='prob',\n",
    "                 verbose=False,\n",
    "                 **catboost_params) -> None:\n",
    "        super().__init__()\n",
    "        self._num_boost_round = num_boost_round\n",
    "        self._stopping_rounds = early_stopping_rounds\n",
    "        self._catboost_params = {\"random_seed\" : random_state,\n",
    "                                 \"verbose\" : verbose,\n",
    "                                 **catboost_params}\n",
    "        self._random_state = random_state\n",
    "        self._predict_type = predict_type\n",
    "\n",
    "    def fit(self, X, y, validation_set = None, val_size=0.05):\n",
    "        self.cat_col_names = [col_name for col_name in X.columns if X[col_name].dtype == 'category']\n",
    "\n",
    "        if validation_set is None:\n",
    "            X_train, X_valid, y_train, y_valid = train_test_split(X, \n",
    "                                                                  y, \n",
    "                                                                  test_size=val_size, \n",
    "                                                                  random_state=self._random_state)\n",
    "        else:\n",
    "            X_train, y_train = X, y\n",
    "            X_valid, y_valid = validation_set\n",
    "            \n",
    "        train_pool = Pool(data=X_train, label=y_train, cat_features=self.cat_col_names)\n",
    "        val_pool = Pool(data=X_valid, label=y_valid, cat_features=self.cat_col_names)\n",
    "        self.model = cat.train(\n",
    "                params=self._catboost_params,           \n",
    "                dtrain=train_pool,   \n",
    "                eval_set=val_pool,\n",
    "                num_boost_round=self._num_boost_round,   \n",
    "                early_stopping_rounds=self._stopping_rounds  \n",
    "                )\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        test_pool = Pool(data=X, cat_features=self.cat_col_names)\n",
    "        if self._predict_type == \"prob\":\n",
    "            y_pred = self.model.predict(test_pool, prediction_type='Probability')[:, 1]\n",
    "        elif self._predict_type == \"class\":\n",
    "            y_pred = self.model.predict(test_pool, prediction_type='Class')\n",
    "        else:\n",
    "            y_pred = self.model.predict(test_pool)\n",
    "        return y_pred\n",
    "    \n",
    "from typing import List\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "class XGBoostModel():\n",
    "\n",
    "    def __init__(self,\n",
    "                 eval_verbosity=False,\n",
    "                 num_boost_round=100,\n",
    "                 early_stopping_rounds=20,\n",
    "                 random_state=129,\n",
    "                 verbosity=0,\n",
    "                 n_jobs=1,\n",
    "                 **xgb_param_grid) -> None:\n",
    "        super().__init__()\n",
    "        self._eval_verbosity = eval_verbosity\n",
    "        self._num_boost_round = num_boost_round\n",
    "        self._early_stopping_rounds = early_stopping_rounds\n",
    "        self._xgb_param_grid = {\"verbosity\" : verbosity,\n",
    "                                \"random_state\" : random_state,\n",
    "                                \"n_jobs\" : n_jobs,\n",
    "                                **xgb_param_grid}\n",
    "        self._random_state = random_state\n",
    "    \n",
    "    def fit(self, X, y, validation_set = None, val_size=0.05):\n",
    "        if validation_set is None:\n",
    "            X_train, X_valid, y_train, y_valid = train_test_split(X, \n",
    "                                                                  y, \n",
    "                                                                  test_size=val_size, \n",
    "                                                                  random_state=self._random_state)\n",
    "        else:\n",
    "            X_train, y_train = X, y\n",
    "            X_valid, y_valid = validation_set\n",
    "        train_data = xgb.DMatrix(X_train, label=y_train, enable_categorical=True)\n",
    "        valid_data = xgb.DMatrix(X_valid, label=y_valid, enable_categorical=True)\n",
    "        eval_data = [(train_data, 'train'), (valid_data, 'eval')]\n",
    "    \n",
    "        self.model = xgb.train(\n",
    "            self._xgb_param_grid, \n",
    "            train_data, \n",
    "            evals=eval_data,                       \n",
    "            early_stopping_rounds=self._early_stopping_rounds,   \n",
    "            num_boost_round=self._num_boost_round,        \n",
    "            verbose_eval=self._eval_verbosity                 \n",
    "        )\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        test_data = xgb.DMatrix(X, enable_categorical=True)\n",
    "        y_pred = self.model.predict(test_data)\n",
    "        return y_pred\n",
    "    \n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, TargetEncoder\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier, HistGradientBoostingRegressor\n",
    "\n",
    "\n",
    "class HGBModel():\n",
    "\n",
    "    def __init__(self,\n",
    "                 smooth=\"auto\", \n",
    "                 target_type=\"continuous\",\n",
    "                 num_boost_round=100,\n",
    "                 early_stopping=True,\n",
    "                 validation_fraction=0.05,\n",
    "                 early_stopping_rounds=20,\n",
    "                 verbose=0,\n",
    "                 random_state=129,\n",
    "                 **hgb_params) -> None:\n",
    "        hgb_params = {\"max_iter\" : num_boost_round,\n",
    "                      \"early_stopping\" : early_stopping,\n",
    "                      \"validation_fraction\" : validation_fraction,\n",
    "                      \"n_iter_no_change\" : early_stopping_rounds,\n",
    "                      \"verbose\" : verbose,\n",
    "                      \"random_state\" : random_state,\n",
    "                      \"categorical_features\" : \"from_dtype\",\n",
    "                      **hgb_params}\n",
    "        \n",
    "        self._target_enc = TargetEncoder(target_type=target_type, \n",
    "                                         smooth=smooth, \n",
    "                                         random_state=random_state)\n",
    "        self._target_type = target_type\n",
    "        if target_type == \"continuous\":\n",
    "            self.model = HistGradientBoostingRegressor(**hgb_params)\n",
    "        else:\n",
    "            self.model = HistGradientBoostingClassifier(**hgb_params)\n",
    "        \n",
    "    \n",
    "    def fit(self, X, y, validation_set=None, **kwargs):\n",
    "        categorical_features = [col_name for col_name in X.columns if X[col_name].dtype == 'category']\n",
    "        target_enc_values = self._target_enc.fit_transform(X[categorical_features], y)\n",
    "        X = X.drop(columns=categorical_features)\n",
    "        X[categorical_features] = target_enc_values\n",
    "        self.model.fit(X, y)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        categorical_features = [col_name for col_name in X.columns if X[col_name].dtype == 'category']\n",
    "        target_enc_values = self._target_enc.transform(X[categorical_features])\n",
    "        X = X.drop(columns=categorical_features)\n",
    "        X[categorical_features] = target_enc_values\n",
    "        if self._target_type == \"continuous\":\n",
    "            y_pred = self.model.predict(X)\n",
    "        else:\n",
    "            y_pred = self.model.predict_proba(X)[:, 1]\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from functools import reduce\n",
    "from typing import Any, Dict, List\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class SupportedModelTypes(Enum):\n",
    "    LGBM = LGBMModel\n",
    "    CAT = CatBoostModel\n",
    "    XGB = XGBoostModel\n",
    "    HGB = HGBModel\n",
    "    # PYTORCH = PytorchFFNModel\n",
    "    # KERAS_EMB = KerasEmbeddingModel\n",
    "    # KERAS_FM = KerasFM\n",
    "    # TABNET = TabNetModel \n",
    "\n",
    "class SupportedClassificationParams(Enum):\n",
    "    LGBM = {'objective' : 'binary', 'metric' : 'binary_logloss'}\n",
    "    CAT = {'loss_function':'Logloss', 'eval_metric' : \"AUC\"}\n",
    "    XGB = {'objective' : 'binary:logistic', 'eval_metric' : 'logloss'}\n",
    "    HGB = {'target_type' : 'binary'}\n",
    "    # PYTORCH = {}\n",
    "    # KERAS_EMB = {}\n",
    "    # KERAS_FM = {}\n",
    "    # TABNET =  {}\n",
    "\n",
    "class SupportedRegressionParams(Enum):\n",
    "    LGBM = {'objective': 'regression', 'metric': 'rmse'}\n",
    "    CAT = {\"loss_function\" : \"RMSE\", 'eval_metric': 'RMSE'}\n",
    "    XGB = {\"objective\": \"reg:squarederror\", \"eval_metric\": \"rmse\"}\n",
    "    HGB = {\"target_type\" : \"continuous\"}\n",
    "    # PYTORCH = {\"loss\": nn.MSELoss(), \"metric_callable\": mean_squared_error}\n",
    "    # KERAS_EMB = {}\n",
    "    # KERAS_FM = {}\n",
    "    # TABNET =  {}\n",
    "    \n",
    "\n",
    "class KToolsTrainer:\n",
    "\n",
    "    def __init__(self,\n",
    "                 model_type : str,\n",
    "                 task : str,\n",
    "                 model_parameters : Dict[str, Any],\n",
    "                 kfold_object,\n",
    "                 train_csv_path : str,\n",
    "                 test_csv_path : str,\n",
    "                 sample_csv_path : str,\n",
    "                 target_col_name : str,\n",
    "                 model_name : str = None,\n",
    "                 output_file_path : str = None,\n",
    "                 data_transforms : List[Any] = [FillNullValues.transform,\n",
    "                                                ConvertObjectToCategorical.transform],\n",
    "                 eval_metric : callable = None,\n",
    "                 verbose : bool = False\n",
    "                 ) -> None:\n",
    "        self._model_type = model_type.upper()\n",
    "        self.model_name = model_name\n",
    "        self._task = task.upper()\n",
    "        self._model_parameters = model_parameters\n",
    "        self._kfold_object = kfold_object\n",
    "        self._eval_metric = eval_metric\n",
    "        self._verbose = verbose\n",
    "        self._data_transforms = data_transforms\n",
    "        self._train_csv_path = train_csv_path\n",
    "        self._test_csv_path = test_csv_path\n",
    "        self._sample_csv_path = sample_csv_path\n",
    "        self._target_col_name = target_col_name\n",
    "        self._output_file_path = output_file_path\n",
    "        self.model = self._setup_model()\n",
    "        self.train_df, self.test_df = self._setup_dataset()\n",
    "\n",
    "    def _setup_model(self):\n",
    "        model_class_obj = SupportedModelTypes[self._model_type].value\n",
    "        if self._task == \"BINARY\":\n",
    "            task_params = SupportedClassificationParams[self._model_type].value\n",
    "        elif self._task == \"REGRESSION\":\n",
    "            task_params = SupportedRegressionParams[self._model_type].value\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        \n",
    "        self._model_parameters.update(task_params)\n",
    "        return model_class_obj(**self._model_parameters)\n",
    "    \n",
    "    def _setup_dataset(self):\n",
    "        settings = DataSciencePipelineSettings(self._train_csv_path,\n",
    "                                               self._test_csv_path,\n",
    "                                               self._target_col_name,\n",
    "                                               )\n",
    "\n",
    "        settings = reduce(lambda acc, func: func(acc), self._data_transforms, settings)\n",
    "        train_df, test_df = settings.update()\n",
    "        test_df.drop(columns=[self._target_col_name], inplace=True)\n",
    "        return train_df, test_df\n",
    "\n",
    "    def fit_predict(self):\n",
    "        \n",
    "        X, y = self.train_df.drop(columns=self._target_col_name), self.train_df[[self._target_col_name]]\n",
    "        score_tuple, oof_predictions, model_list = CrossValidationExecutor(self.model,\n",
    "                                                                           self._eval_metric,\n",
    "                                                                           self._kfold_object,\n",
    "                                                                           verbose=2\n",
    "                                                                           ).run(X, y)\n",
    "        \n",
    "        num_splits = self._kfold_object.get_n_splits()\n",
    "        test_predictions = np.zeros(self.test_df.shape[0])\n",
    "        for model in model_list:\n",
    "            test_predictions += model.predict(self.test_df)/num_splits\n",
    "\n",
    "        self.model_name = str(self.model) if self.model_name is None else self.model_name\n",
    "        if self._output_file_path is not None:\n",
    "            pd.Series(oof_predictions).to_csv(self._output_file_path + self.model_name + \"_oofs.csv\")\n",
    "            pd.Series(test_predictions).to_csv(self._output_file_path + self.model_name + \"_test.csv\")\n",
    "\n",
    "            sample_sub = pd.read_csv(self._sample_csv_path)\n",
    "            sample_sub.iloc[:, 1] =  test_predictions\n",
    "            sample_sub.to_csv(f\"{self.model_name}_submission.csv\", index=False)\n",
    "            sample_sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_gpu_params = {\"loss_function\" : \"RMSE\", 'eval_metric': 'RMSE', 'task_type' : \"GPU\", 'devices' : '0,1', \"bootstrap_type\" : \"Bayesian\", \"num_boost_round\" : 10000, \"early_stopping_rounds\" : 20, 'max_bin': 119, 'learning_rate': 0.03544042692533354, 'depth': 11, 'bagging_temperature': 0.274090640514847, 'min_data_in_leaf': 322.15353586384697, 'l2_leaf_reg': 2.683155590343877e-05, 'leaf_estimation_iterations': 5, 'leaf_estimation_method': 'Newton'}\n",
    "cat_lg_params = {\"loss_function\" : \"RMSE\", 'eval_metric': 'RMSE', \"grow_policy\" : \"Lossguide\", \"num_boost_round\" : 1500, \"early_stopping_rounds\" : 10, 'max_bin': 429, 'learning_rate': 0.0444690180044118, 'depth': 16, 'bagging_temperature': 46.938242378130056, 'subsample': 0.9822763758421824, 'colsample_bylevel': 0.9965797729601968, 'min_data_in_leaf': 200.85284748997194, 'l2_leaf_reg': 0.028938997264293807, 'leaf_estimation_iterations': 1, 'random_strength': 3.568433370233164, 'leaf_estimation_method': 'Gradient'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(5, shuffle=True, random_state=42)\n",
    "\n",
    "transforms = [\n",
    "            FillNullValues.transform,\n",
    "            CreateYuweiFeatures.transform,\n",
    "            ConvertObjectToCategorical.transform,\n",
    "            LogTransformTarget.transform,\n",
    "            ]\n",
    "\n",
    "cat = KToolsTrainer('cat',\n",
    "                    'regression',\n",
    "                    {'predict_type' : 'else'},\n",
    "                    kf,\n",
    "                    train_csv_path,\n",
    "                    test_csv_path,\n",
    "                    sample_sub_csv_path,\n",
    "                    target_col_name,\n",
    "                    model_name='cat_with_fe_tuned',\n",
    "                    output_file_path='data/insurance/oofs/',\n",
    "                    eval_metric=root_mean_squared_error,\n",
    "                    data_transforms=transforms,\n",
    "                    )\n",
    "\n",
    "cat.fit_predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ktools_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
