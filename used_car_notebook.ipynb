{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuwei-1/anaconda3/envs/ktools_2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import seaborn as sns\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "import catboost as cb\n",
    "import optuna\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "import pprint\n",
    "from copy import deepcopy\n",
    "from typing import *\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "from catboost import CatBoostClassifier\n",
    "from functools import reduce\n",
    "from sklearn.preprocessing import TargetEncoder\n",
    "from ktools.preprocessing.categorical_denoiser_prepreprocesser import CategoricalDenoiserPreprocessor\n",
    "from ktools.metrics.fast_matthew_correlation_coefficient import fast_matthews_corr_coeff\n",
    "from ktools.preprocessing.categorical_string_label_error_imputator import CategoricalLabelErrorImputator\n",
    "from ktools.preprocessing.categorical_features_embedder import SortMainCategories\n",
    "from ktools.preprocessing.kaggle_dataset_manager import KaggleDatasetManager\n",
    "from ktools.utils.data_science_pipeline_settings import DataSciencePipelineSettings\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from ktools.fitting.cross_validate_then_test_sklearn_model import CrossValidateTestSklearnModel\n",
    "from sklearn.metrics import root_mean_squared_error, r2_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import xgboost as xgb\n",
    "from ktools.modelling.models.lgbm_model import LGBMModel\n",
    "from ktools.modelling.models.xgb_model import XGBoostModel\n",
    "from ktools.modelling.models.catboost_model import CatBoostModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "path = \"/Users/yuwei-1/Documents/projects/Kaggle-tools/data/used_car_prices/oof_test_predictions\"\n",
    "# dir_list = os.listdir(path)\n",
    "\n",
    "dir_list = ['basic_cat_73101.72',\n",
    "            'rf_lgbm_tuned_72674.08',\n",
    "            'gbdt_lgbm_tuned_72551.05',\n",
    "            # 'goss_lgbm_tuned_72580.14',\n",
    "            'gbdt_xgb_tuned_72510.52',\n",
    "            'basic_xgb_74740.93',\n",
    "            # 'basic_lgbm_72952.96',\n",
    "            # 'cat_tuned_72764.5'\n",
    "            ]\n",
    "\n",
    "oof_list = []\n",
    "test_list = []\n",
    "for i, dir in enumerate(dir_list):\n",
    "    oof_path = os.path.join(path, dir)\n",
    "    df = pd.read_csv(oof_path + \"/oof_predictions.csv\", index_col=0).rename(columns={\"oof_predictions\" : f\"prediction_{i}\"})\n",
    "    testdf = pd.read_csv(oof_path + \"/test_predictions.csv\", index_col=0).rename(columns={\"test_prediction\" : f\"prediction_{i}\"})\n",
    "    oof_list += [df]\n",
    "    test_list += [testdf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['basic_cat_73101.72',\n",
       " 'rf_lgbm_tuned_72674.08',\n",
       " 'gbdt_lgbm_tuned_72551.05',\n",
       " 'gbdt_xgb_tuned_72510.52',\n",
       " 'basic_xgb_74740.93']"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_model_dataset = pd.concat(oof_list, axis=1)\n",
    "meta_model_test = pd.concat(test_list, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = pd.read_csv(\"/Users/yuwei-1/Documents/projects/Kaggle-tools/data/used_car_prices/basic_train.csv\"), pd.read_csv(\"/Users/yuwei-1/Documents/projects/Kaggle-tools/data/used_car_prices/basic_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import make_scorer, root_mean_squared_error\n",
    "# from sklearn.model_selection import RepeatedKFold\n",
    "# from sklearn.pipeline import make_pipeline\n",
    "# from sklearn.svm import SVR\n",
    "\n",
    "\n",
    "# X = pd.read_csv('/Users/yuwei-1/Documents/projects/Kaggle-tools/data/used_car_prices/basic_train.csv', index_col=0)\n",
    "# y = np.log(X.pop('price'))\n",
    "# X['mileage'] = X.pop('milage')//100\n",
    "\n",
    "# kfold = RepeatedKFold(n_splits=5, n_repeats=1, random_state=0)\n",
    "# scoring=make_scorer(lambda y, yh: root_mean_squared_error(np.exp(y), np.exp(yh)))\n",
    "\n",
    "# model = make_pipeline(\n",
    "#     TargetEncoder(random_state=0, target_type='continuous').set_output(transform='pandas'),\n",
    "#     StandardScaler().set_output(transform='pandas'),\n",
    "#     SVR(C=1000, gamma=0.0075, epsilon=0.1, max_iter=5000)\n",
    "# )\n",
    "\n",
    "# scores = -cross_val_score(model, X, y, scoring=scoring, cv=kfold, n_jobs=1)\n",
    "# print(F'CV RMSE score: {np.mean(scores):.5f} ± {np.std(scores):.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Model r2: 0.154777\n",
      "5-fold cross validation r2:  0.1545022531077287\n",
      "Final Model rmse: 72461.102369\n",
      "5-fold cross validation rmse:  72472.89565387438\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression, Lasso\n",
    "\n",
    "\n",
    "model = Lasso(alpha=1e4, positive=True)\n",
    "\n",
    "num_splits = 5\n",
    "eval_metrics = {\"r2\" : r2_score, \"rmse\" : lambda y, yhat : root_mean_squared_error(y, yhat)}\n",
    "skf = KFold(n_splits=num_splits, shuffle=True, random_state=42)\n",
    "\n",
    "cvt = CrossValidateTestSklearnModel(model,\n",
    "                              eval_metrics,\n",
    "                              skf,\n",
    "                              num_splits)\n",
    "\n",
    "modellist, cv_scores, test_scores = cvt.evaluate(meta_model_dataset,\n",
    "                                                 train_df['price'],\n",
    "                                                 meta_model_dataset,\n",
    "                                                 train_df['price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.06181176, 0.13556445, 0.23634809, 0.56638801, 0.05831875])"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvt.model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv('/Users/yuwei-1/Documents/projects/Kaggle-tools/data/used_car_prices/sample_submission.csv', index_col=0)\n",
    "\n",
    "# for i, mdl in enumerate(modellist):\n",
    "#     sub[f'price_{i}'] = mdl.predict(meta_model_test)\n",
    "\n",
    "# sub['price'] = sub.drop(columns=['price']).mean(axis=1)\n",
    "\n",
    "sub['price'] = cvt.model.predict(meta_model_test)\n",
    "sub['price'].to_csv('submissions/used_cars/used_car_submission_v27.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>price_0</th>\n",
       "      <th>price_1</th>\n",
       "      <th>price_2</th>\n",
       "      <th>price_3</th>\n",
       "      <th>price_4</th>\n",
       "      <th>price_5</th>\n",
       "      <th>price_6</th>\n",
       "      <th>price_7</th>\n",
       "      <th>price_8</th>\n",
       "      <th>price_9</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>188533</th>\n",
       "      <td>16609.084353</td>\n",
       "      <td>16419.197917</td>\n",
       "      <td>16608.205909</td>\n",
       "      <td>16710.652168</td>\n",
       "      <td>16663.079840</td>\n",
       "      <td>16714.322255</td>\n",
       "      <td>16526.453652</td>\n",
       "      <td>16609.348374</td>\n",
       "      <td>16643.730147</td>\n",
       "      <td>16574.072374</td>\n",
       "      <td>16621.780894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188534</th>\n",
       "      <td>76614.823365</td>\n",
       "      <td>76288.899316</td>\n",
       "      <td>76208.100212</td>\n",
       "      <td>76246.298526</td>\n",
       "      <td>76847.731892</td>\n",
       "      <td>77012.094606</td>\n",
       "      <td>77203.616040</td>\n",
       "      <td>76293.799346</td>\n",
       "      <td>76548.438336</td>\n",
       "      <td>76818.233142</td>\n",
       "      <td>76681.022237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188535</th>\n",
       "      <td>49832.521073</td>\n",
       "      <td>49231.505745</td>\n",
       "      <td>49611.472638</td>\n",
       "      <td>49701.471493</td>\n",
       "      <td>49991.690462</td>\n",
       "      <td>50276.053693</td>\n",
       "      <td>50306.615955</td>\n",
       "      <td>49511.650845</td>\n",
       "      <td>49897.441144</td>\n",
       "      <td>49985.830087</td>\n",
       "      <td>49811.478666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188536</th>\n",
       "      <td>28706.923083</td>\n",
       "      <td>28509.486010</td>\n",
       "      <td>28372.404035</td>\n",
       "      <td>28558.922445</td>\n",
       "      <td>28884.661036</td>\n",
       "      <td>28920.115602</td>\n",
       "      <td>28903.372022</td>\n",
       "      <td>28370.940156</td>\n",
       "      <td>28736.425801</td>\n",
       "      <td>29034.581896</td>\n",
       "      <td>28778.321824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188537</th>\n",
       "      <td>32750.804033</td>\n",
       "      <td>33010.918871</td>\n",
       "      <td>32916.681099</td>\n",
       "      <td>33035.162869</td>\n",
       "      <td>32726.451074</td>\n",
       "      <td>32515.422509</td>\n",
       "      <td>32459.678827</td>\n",
       "      <td>32974.409617</td>\n",
       "      <td>32822.596982</td>\n",
       "      <td>32531.451688</td>\n",
       "      <td>32515.266795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314218</th>\n",
       "      <td>28055.990604</td>\n",
       "      <td>27892.579438</td>\n",
       "      <td>27823.549204</td>\n",
       "      <td>28043.974465</td>\n",
       "      <td>28193.134893</td>\n",
       "      <td>28210.384959</td>\n",
       "      <td>28182.347660</td>\n",
       "      <td>27784.025140</td>\n",
       "      <td>28162.632289</td>\n",
       "      <td>28295.549530</td>\n",
       "      <td>27971.728464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314219</th>\n",
       "      <td>48824.517498</td>\n",
       "      <td>48010.463321</td>\n",
       "      <td>48351.314572</td>\n",
       "      <td>48551.823880</td>\n",
       "      <td>49129.327256</td>\n",
       "      <td>49569.323615</td>\n",
       "      <td>49334.535988</td>\n",
       "      <td>48679.040309</td>\n",
       "      <td>48922.725875</td>\n",
       "      <td>48992.360494</td>\n",
       "      <td>48704.259672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314220</th>\n",
       "      <td>24824.020671</td>\n",
       "      <td>25364.332187</td>\n",
       "      <td>25049.729640</td>\n",
       "      <td>25150.970786</td>\n",
       "      <td>24763.248885</td>\n",
       "      <td>24404.243715</td>\n",
       "      <td>24318.049462</td>\n",
       "      <td>25160.384617</td>\n",
       "      <td>24841.856391</td>\n",
       "      <td>24542.726928</td>\n",
       "      <td>24644.664100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314221</th>\n",
       "      <td>17937.542230</td>\n",
       "      <td>18240.670050</td>\n",
       "      <td>18067.942752</td>\n",
       "      <td>18166.711536</td>\n",
       "      <td>17917.539920</td>\n",
       "      <td>17682.007738</td>\n",
       "      <td>17565.442418</td>\n",
       "      <td>18119.045143</td>\n",
       "      <td>17941.509606</td>\n",
       "      <td>17789.124791</td>\n",
       "      <td>17885.428342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314222</th>\n",
       "      <td>36397.772820</td>\n",
       "      <td>36001.639685</td>\n",
       "      <td>36463.436615</td>\n",
       "      <td>36496.510606</td>\n",
       "      <td>36448.255375</td>\n",
       "      <td>36707.576287</td>\n",
       "      <td>36534.204099</td>\n",
       "      <td>36408.955055</td>\n",
       "      <td>36472.416474</td>\n",
       "      <td>36161.304074</td>\n",
       "      <td>36283.429930</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>125690 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               price       price_0       price_1       price_2       price_3  \\\n",
       "id                                                                             \n",
       "188533  16609.084353  16419.197917  16608.205909  16710.652168  16663.079840   \n",
       "188534  76614.823365  76288.899316  76208.100212  76246.298526  76847.731892   \n",
       "188535  49832.521073  49231.505745  49611.472638  49701.471493  49991.690462   \n",
       "188536  28706.923083  28509.486010  28372.404035  28558.922445  28884.661036   \n",
       "188537  32750.804033  33010.918871  32916.681099  33035.162869  32726.451074   \n",
       "...              ...           ...           ...           ...           ...   \n",
       "314218  28055.990604  27892.579438  27823.549204  28043.974465  28193.134893   \n",
       "314219  48824.517498  48010.463321  48351.314572  48551.823880  49129.327256   \n",
       "314220  24824.020671  25364.332187  25049.729640  25150.970786  24763.248885   \n",
       "314221  17937.542230  18240.670050  18067.942752  18166.711536  17917.539920   \n",
       "314222  36397.772820  36001.639685  36463.436615  36496.510606  36448.255375   \n",
       "\n",
       "             price_4       price_5       price_6       price_7       price_8  \\\n",
       "id                                                                             \n",
       "188533  16714.322255  16526.453652  16609.348374  16643.730147  16574.072374   \n",
       "188534  77012.094606  77203.616040  76293.799346  76548.438336  76818.233142   \n",
       "188535  50276.053693  50306.615955  49511.650845  49897.441144  49985.830087   \n",
       "188536  28920.115602  28903.372022  28370.940156  28736.425801  29034.581896   \n",
       "188537  32515.422509  32459.678827  32974.409617  32822.596982  32531.451688   \n",
       "...              ...           ...           ...           ...           ...   \n",
       "314218  28210.384959  28182.347660  27784.025140  28162.632289  28295.549530   \n",
       "314219  49569.323615  49334.535988  48679.040309  48922.725875  48992.360494   \n",
       "314220  24404.243715  24318.049462  25160.384617  24841.856391  24542.726928   \n",
       "314221  17682.007738  17565.442418  18119.045143  17941.509606  17789.124791   \n",
       "314222  36707.576287  36534.204099  36408.955055  36472.416474  36161.304074   \n",
       "\n",
       "             price_9  \n",
       "id                    \n",
       "188533  16621.780894  \n",
       "188534  76681.022237  \n",
       "188535  49811.478666  \n",
       "188536  28778.321824  \n",
       "188537  32515.266795  \n",
       "...              ...  \n",
       "314218  27971.728464  \n",
       "314219  48704.259672  \n",
       "314220  24644.664100  \n",
       "314221  17885.428342  \n",
       "314222  36283.429930  \n",
       "\n",
       "[125690 rows x 11 columns]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def update(df):\n",
    "    \n",
    "#     t = 100\n",
    "    \n",
    "#     cat_c = ['brand','model','fuel_type','engine','transmission','ext_col','int_col','accident','clean_title']\n",
    "#     re_ = ['model','engine','transmission','ext_col','int_col']\n",
    "    \n",
    "#     for col in re_:\n",
    "#         df.loc[df[col].value_counts(dropna=False)[df[col]].values < t, col] = \"noise\"\n",
    "        \n",
    "#     for col in cat_c:\n",
    "#         df[col] = df[col].fillna('missing')\n",
    "#         df[col] = df[col].astype('category')\n",
    "        \n",
    "#     return df\n",
    "\n",
    "# train  = update(train)\n",
    "# # test   = update(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvertToLower:\n",
    "    @staticmethod\n",
    "    def transform(original_settings : DataSciencePipelineSettings):\n",
    "        settings = deepcopy(original_settings)\n",
    "        for col_name in settings.categorical_col_names:\n",
    "            settings.combined_df[col_name] = settings.combined_df[col_name].str.lower()\n",
    "        return settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UsedCarSpecificConverter:\n",
    "    @staticmethod\n",
    "    def transform(original_settings : DataSciencePipelineSettings):\n",
    "        settings = deepcopy(original_settings)\n",
    "        def find_pattern(pattern, text):\n",
    "            match = re.search(pattern, text)\n",
    "            if match:\n",
    "                return match.group(1)\n",
    "            else:\n",
    "                return None\n",
    "            \n",
    "        def transmission(text):\n",
    "            if 'a/t' in text or 'at' in text or 'automatic' in text:\n",
    "                return 'automatic transmission'\n",
    "            elif 'm/t' in text or 'mt' in text or 'manual' in text:\n",
    "                return 'manual transmission'\n",
    "            elif 'cvt' in text:\n",
    "                return 'continuously variable transmission'\n",
    "            else:\n",
    "                return 'other'\n",
    "\n",
    "        def camshafts(text):\n",
    "            if 'dohc' in text:\n",
    "                # double overhead camshaft\n",
    "                return 'dohc'\n",
    "            elif 'sohc' in text:\n",
    "                #single overhead camshaft\n",
    "                return 'sohc'\n",
    "            elif 'ohv' in text:\n",
    "                # overhead valve\n",
    "                return 'ohv'\n",
    "            elif 'vtec' in text:\n",
    "                # variable valve timing and lift electronic control\n",
    "                return 'vtec'\n",
    "            else:\n",
    "                return 'other'\n",
    "\n",
    "        def injection(text):\n",
    "            if 'ddi' in text:\n",
    "                #direct diesel injection\n",
    "                return 'ddi'\n",
    "            elif 'gdi' in text:\n",
    "                #gasoline direct injection\n",
    "                return 'gdi'\n",
    "            elif 'mpfi' in text:\n",
    "                # multi-point fuel injection\n",
    "                return 'mpfi'\n",
    "            elif 'pdi' in text:\n",
    "                # port fuel injection\n",
    "                return 'pdi'\n",
    "            elif 'tfsi' in text or 'tsi' in text:\n",
    "                # turbo stratified injection\n",
    "                return 'tfsi'\n",
    "            elif 'gtdi' in text:\n",
    "                # gasoline turbocharged direct injection\n",
    "                return 'gtdi'\n",
    "            elif 'sidi' in text:\n",
    "                # spark ignition direct injection\n",
    "                return 'sidi'\n",
    "            else:\n",
    "                return 'other'\n",
    "            \n",
    "        pattern = r'(\\d*\\.?\\d+)\\s*hp'\n",
    "        settings.combined_df['horsepower'] = settings.combined_df['engine'].apply(lambda x : find_pattern(pattern, x)).astype('float64')\n",
    "        pattern = r'(\\d*\\.?\\d+)\\s*(l|liter)'\n",
    "        settings.combined_df['liters'] = settings.combined_df['engine'].apply(lambda x : find_pattern(pattern, x)).astype('float64')\n",
    "        pattern = r'(\\d*\\.?\\d+)\\s*cylinder'\n",
    "        settings.combined_df['cylinders'] = settings.combined_df['engine'].apply(lambda x : find_pattern(pattern, x)).astype('float64')\n",
    "        pattern = r'(\\d*\\.?\\d+)\\s*(-speed|speed)'\n",
    "        settings.combined_df['speed'] = settings.combined_df['transmission'].apply(lambda x : find_pattern(pattern, x)).astype('float64')\n",
    "        \n",
    "        settings.combined_df['injection'] = settings.combined_df['engine'].apply(lambda x : injection(x)).astype('object')\n",
    "        settings.combined_df['camshaft'] = settings.combined_df['engine'].apply(lambda x : camshafts(x)).astype('object')\n",
    "        settings.combined_df['transmission_clean'] = settings.combined_df['transmission'].apply(lambda x : transmission(x)).astype('object')\n",
    "        settings.combined_df.loc[(settings.combined_df['model'].str.contains('model y|model x|model s|model 3', regex=True)), 'fuel_type'] = 'electric'\n",
    "        settings.combined_df.loc[(settings.combined_df['model'].str.contains('electric')), 'fuel_type'] = 'electric'\n",
    "\n",
    "\n",
    "        expensive_ext_color = ['blue caelum','dark sapphire','bianco monocerus','c / c',\n",
    "                               'ice','tempest','beluga black','bianco icarus metallic','blu eleos',\n",
    "                               'shadow black','nero noctis','sandstone metallic','lizard green','balloon white','onyx',\n",
    "                               'donington grey metallic','china blue','diamond white','rosso corsa',\n",
    "                                'granite','rosso mars metallic',\n",
    "                                'carpathian grey','kemora gray metallic','grigio nimbus','dash','bianco isis','python green',\n",
    "                                'fountain blue','custom color','vega blue','designo magno matte',\n",
    "                                'brands hatch gray metallic',\n",
    "                                'rift metallic','gentian blue metallic',\n",
    "                                'arancio borealis','blue',\n",
    "                                'aventurine green metallic',\n",
    "                                'apex blue','daytona gray pearl effect',\n",
    "                                'daytona gray pearl effect w/ black roof','matte white',\n",
    "                                'carpathian grey premium metallic','blue metallic','santorini black metallic',\n",
    "                                'quartzite grey metallic','carrara white metallic','black',\n",
    "                                'kinetic blue',\n",
    "                                'nero daytona']\n",
    "\n",
    "        expensive_int_color = ['dark auburn',\n",
    "                            'hotspur',\n",
    "                            'cobalt blue',\n",
    "                            'beluga hide',\n",
    "                            'linen',\n",
    "                            'beluga',\n",
    "                            'black / brown',\n",
    "                            'nero ade',\n",
    "                            'sahara tan',\n",
    "                            'portland']\n",
    "        \n",
    "        \n",
    "        \n",
    "        settings.combined_df['expensive_ext_col'] = settings.combined_df['ext_col'].isin(expensive_ext_color).astype(int)\n",
    "        settings.combined_df['expensive_int_col'] = settings.combined_df['int_col'].isin(expensive_int_color).astype(int)\n",
    "        settings.combined_df['twin_turbo'] = settings.combined_df['engine'].str.contains('twin turbo').astype(int)\n",
    "        settings.combined_df['turbo'] = settings.combined_df['engine'].str.contains('turbo').astype(int)\n",
    "        settings.combined_df['length_model'] = settings.combined_df['model'].apply(lambda x : len(x))\n",
    "        settings.combined_df['length_ext_col'] = settings.combined_df['ext_col'].apply(lambda x : len(x))\n",
    "        settings.combined_df['length_int_col'] = settings.combined_df['int_col'].apply(lambda x : len(x))\n",
    "        \n",
    "        clean_colors = ['ext_col', 'int_col']\n",
    "        string_imputator = CategoricalLabelErrorImputator(verbose=True)\n",
    "        settings.combined_df[['basic_ext_color', 'basic_int_color']] = string_imputator.impute(settings.combined_df[clean_colors],\n",
    "                                                                                                                    clean_colors,\n",
    "                                                                                                                    1500)\n",
    "        \n",
    "        settings.combined_df['basic_ext_color'] = settings.combined_df['basic_ext_color'].astype('object')\n",
    "        settings.combined_df['basic_int_color'] = settings.combined_df['basic_int_color'].astype('object')\n",
    "        # threshold = 2.5e6\n",
    "        # print(\"num removed: \", (settings.combined_df['price'] > threshold).sum())\n",
    "        # settings.combined_df = settings.combined_df[(settings.combined_df['price'] < threshold) | (settings.combined_df['price'].isna())]\n",
    "        \n",
    "        # settings.training_col_names += ['horsepower', \n",
    "        #                                 'injection',\n",
    "        #                                 'camshaft', \n",
    "        #                                 'cylinders', \n",
    "        #                                 'expensive_ext_col', \n",
    "        #                                 'expensive_int_col', \n",
    "        #                                 'twin_turbo', \n",
    "        #                                 'turbo',\n",
    "        #                                 'transmission_clean',\n",
    "        #                                 'speed',\n",
    "        #                                 'basic_ext_color',\n",
    "        #                                 'basic_int_color',\n",
    "        #                                 'liters',\n",
    "        #                                 'length_model',\n",
    "        #                                 'length_ext_col',\n",
    "        #                                 'length_int_col'\n",
    "        #                                 ]\n",
    "        \n",
    "        re_ = ['model','engine','transmission','ext_col','int_col']\n",
    "        for col in re_:\n",
    "            settings.combined_df.loc[settings.combined_df[col].value_counts(dropna=False)[settings.combined_df[col]].values < 100, col] = \"noise\"\n",
    "        \n",
    "        # settings.categorical_col_names += ['injection',\n",
    "        #                                    'camshaft',\n",
    "        #                                    'transmission_clean',\n",
    "        #                                    'basic_ext_color',\n",
    "        #                                    'basic_int_color']\n",
    "        \n",
    "        return settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicUsedCarSpecificConverter:\n",
    "    @staticmethod\n",
    "    def transform(original_settings : DataSciencePipelineSettings):\n",
    "        settings = deepcopy(original_settings)\n",
    "        def find_pattern(pattern, text):\n",
    "            match = re.search(pattern, text)\n",
    "            if match:\n",
    "                return match.group(1)\n",
    "            else:\n",
    "                return None\n",
    "\n",
    "        pattern = r'(\\d*\\.?\\d+)\\s*hp'\n",
    "        settings.combined_df['horsepower'] = settings.combined_df['engine'].apply(lambda x : find_pattern(pattern, x)).astype('float64')\n",
    "        pattern = r'(\\d*\\.?\\d+)\\s*(l|liter)'\n",
    "        settings.combined_df['liters'] = settings.combined_df['engine'].apply(lambda x : find_pattern(pattern, x)).astype('float64')\n",
    "        pattern = r'(\\d*\\.?\\d+)\\s*cylinder'\n",
    "        settings.combined_df['cylinders'] = settings.combined_df['engine'].apply(lambda x : find_pattern(pattern, x)).astype('float64')\n",
    "        pattern = r'(\\d*\\.?\\d+)\\s*(-speed|speed)'\n",
    "        settings.combined_df['speed'] = settings.combined_df['transmission'].apply(lambda x : find_pattern(pattern, x)).astype('float64')\n",
    "\n",
    "        settings.training_col_names += ['horsepower',\n",
    "                                        'liters',\n",
    "                                        'cylinders',\n",
    "                                        'speed'\n",
    "                                        ]\n",
    "        return settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BackPackerMinimalProcessing:\n",
    "    @staticmethod\n",
    "    def transform(original_settings : DataSciencePipelineSettings):\n",
    "\n",
    "        settings = deepcopy(original_settings)\n",
    "        settings.update()\n",
    "        def update(df):\n",
    "            t = 100\n",
    "            re_ = ['model','engine','transmission','ext_col','int_col']\n",
    "            for col in re_:\n",
    "                df.loc[df[col].value_counts(dropna=False)[df[col]].values < t, col] = \"noise\"\n",
    "            return df\n",
    "        \n",
    "        settings.train_df = update(settings.train_df)\n",
    "        settings.test_df = update(settings.test_df)\n",
    "        settings.combined_df = pd.concat([settings.train_df, settings.test_df], keys=['train', 'test'])\n",
    "        return settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SergeyConverter:\n",
    "    @staticmethod\n",
    "    def transform(original_settings : DataSciencePipelineSettings):\n",
    "        settings = deepcopy(original_settings)\n",
    "        fuel_type_dict = {\n",
    "            'Gasoline': 0,\n",
    "            'Hybrid': 1,\n",
    "            'E85 Flex Fuel': 2,\n",
    "            'uknown': 3,\n",
    "            'Diesel': 4,\n",
    "            'dash': 5,\n",
    "            'Plug-In Hybrid': 6,\n",
    "            'not supported': 7\n",
    "        }\n",
    "\n",
    "        accident_dict = {\n",
    "            'None reported': 0,\n",
    "            'At least 1 accident or damage reported': 1,\n",
    "            'uknown': 2\n",
    "        }\n",
    "\n",
    "        clean_title_dict = {\n",
    "            'Yes': 0,\n",
    "            'uknown': 1\n",
    "        }\n",
    "\n",
    "        expensive_ext_color = ['Blue Caelum', 'Dark Sapphire', 'Bianco Monocerus', 'C / C', 'Ice',\n",
    "            'Tempest', 'Beluga Black', 'Bianco Icarus Metallic', \n",
    "            'BLU ELEOS', 'Shadow Black', 'Nero Noctis', 'Sandstone Metallic',\n",
    "            'Lizard Green', 'Balloon White', 'Onyx', 'Donington Grey Metallic',\n",
    "            'China Blue', 'Diamond White', 'Rosso Corsa', 'Granite',\n",
    "            'Rosso Mars Metallic', 'Carpathian Grey', 'Kemora Gray Metallic',\n",
    "            'Grigio Nimbus', 'dash', 'Bianco Isis', 'Python Green', 'Fountain Blue',\n",
    "            'Custom Color', 'Vega Blue', 'Designo Magno Matte',\n",
    "            'Brands Hatch Gray Metallic', 'Rift Metallic', 'Gentian Blue Metallic',\n",
    "            'Arancio Borealis', 'BLUE', 'Aventurine Green Metallic', 'Apex Blue',\n",
    "            'Daytona Gray Pearl Effect', 'Daytona Gray Pearl Effect w/ Black Roof',\n",
    "            'Matte White', 'Carpathian Grey Premium Metallic', 'Blue Metallic',\n",
    "            'Santorini Black Metallic', 'Quartzite Grey Metallic',\n",
    "            'Carrara White Metallic', 'BLACK', 'Kinetic Blue', 'Nero Daytona']\n",
    "\n",
    "        expensive_int_color = ['Dark Auburn', 'Hotspur', 'Cobalt Blue', 'Beluga Hide', 'Linen',\n",
    "                            'Beluga', 'Black / Brown', 'Nero Ade', 'Sahara Tan', 'Portland']\n",
    "\n",
    "        expensive_hp = [443.0, 473.0, 493.0, 502.0, 521.0, 542.0, 543.0, 571.0, 572.0, 573.0, 580.0,\n",
    "                        591.0, 602.0, 611.0, 616.0, 620.0, 624.0, 640.0, 641.0, 651.0, 710.0, 715.0, 760.0, 788.0, 797.0]\n",
    "\n",
    "\n",
    "\n",
    "        def encode_columns(df):\n",
    "            df['fuel_type_encoded'] = df['fuel_type'].map(fuel_type_dict)\n",
    "            df['accident_encoded'] = df['accident'].map(accident_dict)\n",
    "            df['clean_title_encoded'] = df['clean_title'].map(clean_title_dict)\n",
    "            df['expensive_color_ext_encoded'] = df.ext_col.isin(expensive_ext_color).astype(int)\n",
    "            df['expensive_color_int_encoded'] = df.int_col.isin(expensive_int_color).astype(int)\n",
    "            df['expensive_hp'] = df['engine'].str.extract(r'(\\d+\\.?\\d*)HP').astype(float).isin(expensive_hp).astype(int)\n",
    "            df['cylinder'] = df['engine'].str.extract(r'(\\d+\\.?\\d*) Cylinder').astype(float)              \n",
    "            df['got_V'] = df['model'].str.extract(r'(\\d+\\.?\\d*) V').notna().astype(int)\n",
    "            return df\n",
    "        settings.combined_df = encode_columns(settings.combined_df)\n",
    "        settings.training_col_names += ['expensive_color_ext_encoded',\n",
    "                                        'expensive_color_int_encoded']\n",
    "        return settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FillNullValues:\n",
    "    @staticmethod\n",
    "    def transform(original_settings : DataSciencePipelineSettings, numeric_fill=-1, category_fill='missing'):\n",
    "        settings = deepcopy(original_settings)\n",
    "        for col_name in settings.training_col_names:\n",
    "            if pd.api.types.is_numeric_dtype(settings.combined_df[col_name]):\n",
    "                settings.combined_df[col_name] = settings.combined_df[col_name].fillna(numeric_fill)\n",
    "            else:\n",
    "                settings.combined_df[col_name] = settings.combined_df[col_name].fillna(category_fill)\n",
    "        return settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvertObjectToCategorical:\n",
    "    @staticmethod\n",
    "    def transform(original_settings : DataSciencePipelineSettings):\n",
    "        settings = deepcopy(original_settings)\n",
    "        cat_cols = settings.categorical_col_names\n",
    "        settings.combined_df[cat_cols] = settings.combined_df[cat_cols].astype('category')\n",
    "        return settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogTransformTarget:\n",
    "    @staticmethod\n",
    "    def transform(original_settings : DataSciencePipelineSettings):\n",
    "        settings = deepcopy(original_settings)\n",
    "        target = settings.target_col_name\n",
    "        settings.combined_df['log_' + target] = np.log(settings.combined_df[target] + 1)\n",
    "        settings.target_col_name = 'log_' + target\n",
    "        settings.logged = True\n",
    "        return settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv_path = \"data/used_car_prices/train_combined.csv\"\n",
    "test_csv_path = \"data/used_car_prices/test.csv\"\n",
    "target_col_name = \"price\"\n",
    "\n",
    "settings = DataSciencePipelineSettings(train_csv_path,\n",
    "                                        test_csv_path,\n",
    "                                        target_col_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0           4200\n",
       "1           4999\n",
       "2          13900\n",
       "3          45000\n",
       "4          97500\n",
       "           ...  \n",
       "192537    349950\n",
       "192538     53900\n",
       "192539     90998\n",
       "192540     62999\n",
       "192541     40000\n",
       "Name: price, Length: 192542, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "settings.train_df[settings.target_col_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   0,   1, ..., 163,   7,  66])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round_thousand = (settings.train_df[settings.target_col_name]//1000)*1000\n",
    "round_thousand, _ = round_thousand.factorize()\n",
    "round_thousand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0           0\n",
       "1           0\n",
       "2           1\n",
       "3           2\n",
       "4           3\n",
       "         ... \n",
       "192537     96\n",
       "192538     44\n",
       "192539    163\n",
       "192540      7\n",
       "192541     66\n",
       "Length: 192542, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(round_thousand, index=settings.train_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16788.199999999997\n",
      "27597.799999999996\n",
      "384.0\n",
      "630.3499999999995\n",
      "26103.5\n",
      "27133.75\n",
      "96.0\n",
      "145.0\n",
      "92125.99999999996\n",
      "185953.99999999985\n",
      "678.4000000000012\n",
      "1223.8999999999965\n",
      "18586.200000000004\n",
      "31270.999999999996\n",
      "345.1000000000001\n",
      "895.3999999999955\n",
      "559.6000000000001\n",
      "4018.1999999999994\n",
      "209019.2\n",
      "226353.09999999998\n",
      "255981.7\n",
      "268979.85\n",
      "253.0\n",
      "472.1999999999989\n"
     ]
    }
   ],
   "source": [
    "for col in settings.combined_df.columns:\n",
    "    counts = settings.combined_df[col].value_counts().to_dict()\n",
    "    # print(col, counts.values())\n",
    "    print(np.quantile(list(counts.values()), 0.9))\n",
    "    print(np.quantile(list(counts.values()), 0.95))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "full_transforms = [ConvertToLower.transform, \n",
    "                   BackPackerMinimalProcessing.transform, \n",
    "                   FillNullValues.transform, \n",
    "                   ConvertObjectToCategorical.transform]\n",
    "# basic_transforms = [ConvertToLower.transform, FillNullValues.transform, ConvertObjectToCategorical.transform, LogTransformTarget.transform]\n",
    "\n",
    "full_settings = reduce(lambda acc, func: func(acc), full_transforms, settings)\n",
    "# basic_settings = reduce(lambda acc, func: func(acc), basic_transforms, settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df, test_df = basic_settings.update()\n",
    "\n",
    "# data_manager = KaggleDatasetManager(train_df,\n",
    "#                                     basic_settings.training_col_names,\n",
    "#                                     basic_settings.target_col_name,\n",
    "#                                     0.9,\n",
    "#                                     0.1,\n",
    "#                                     0)\n",
    "\n",
    "# (X_train, \n",
    "# X_valid, \n",
    "# X_test, \n",
    "# y_train, \n",
    "# y_valid,\n",
    "# y_test) = data_manager.dataset_partition()\n",
    "\n",
    "\n",
    "# # model = XGBRegressor(**{}, \n",
    "# #                     verbosity=0,\n",
    "# #                     eval_metric='logloss',\n",
    "# #                     tree_method='hist',\n",
    "# #                     enable_categorical=True)\n",
    "\n",
    "# model = ()\n",
    "\n",
    "# num_splits = 10\n",
    "# eval_metrics = {\"r2\" : r2_score, \"rmse\" : lambda y, yhat : root_mean_squared_error(np.exp(y), np.exp(yhat))}\n",
    "# skf = KFold(n_splits=num_splits)\n",
    "\n",
    "# model, cv_scores, test_scores = CrossValidateTestSklearnModel(model,\n",
    "#                               eval_metrics,\n",
    "#                               skf,\n",
    "#                               num_splits).evaluate(X_train,\n",
    "#                                                    y_train,\n",
    "#                                                    X_test,\n",
    "#                                                    y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Model r2: 0.638607\n",
    "\n",
    "10-fold cross validation r2:  0.6376903986890982\n",
    "\n",
    "Final Model rmse: 70167.345097\n",
    "\n",
    "10-fold cross validation rmse:  73703.32201325778"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from typing import Dict\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from copy import deepcopy\n",
    "# from sklearn.model_selection import StratifiedKFold\n",
    "# from ktools.fitting.i_sklearn_model import ISklearnModel\n",
    "# import lightgbm as lgb\n",
    "# from lightgbm import log_evaluation, early_stopping\n",
    "\n",
    "\n",
    "# class CrossValidateTestSklearnModel:\n",
    "\n",
    "#     def __init__(self,\n",
    "#                  sklearn_model_instance : ISklearnModel,\n",
    "#                  evaluation_metrics : Dict[str, callable],\n",
    "#                  kfold_object = None,\n",
    "#                  num_splits : int = 5) -> None:\n",
    "#         self.model = sklearn_model_instance\n",
    "#         self._evaluation_metrics = evaluation_metrics\n",
    "#         self._metric_names = list(evaluation_metrics.keys())\n",
    "#         self._kf = kfold_object\n",
    "#         self._num_metrics = len(self._metric_names)\n",
    "#         self._num_splits = num_splits\n",
    "#         self._model_list = []\n",
    "\n",
    "#     def _fit_then_predict(self, X, y, X_test, y_test):\n",
    "#         # model = deepcopy(self.model).fit(X, y)\n",
    "        \n",
    "#         train_data = lgb.Dataset(X, label=y)\n",
    "#         val_data = lgb.Dataset(X_test, label=y_test, reference=train_data)\n",
    "#         model = lgb.train(model_params,\n",
    "#                     train_data,\n",
    "#                     valid_sets=[train_data, val_data],\n",
    "#                     valid_names=['train', 'valid'],\n",
    "#                     callbacks=callbacks    \n",
    "#                     )\n",
    "#         y_pred = model.predict(X_test)\n",
    "#         return y_pred, model\n",
    "\n",
    "#     def evaluate(self,\n",
    "#                  X_train, y_train,\n",
    "#                  X_test, y_test):\n",
    "\n",
    "#         cv_results = np.zeros((self._num_splits, self._num_metrics))\n",
    "#         cv_scores = None\n",
    "\n",
    "#         if self._kf is not None:\n",
    "#             for i, (train_index, val_index) in enumerate(self._kf.split(X_train, y_train)):\n",
    "#                 X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "#                 y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "\n",
    "#                 y_pred, model = self._fit_then_predict(X_train_fold,\n",
    "#                                                        y_train_fold,\n",
    "#                                                        X_val_fold,\n",
    "#                                                        y_val_fold)\n",
    "#                 self._model_list += [model]\n",
    "\n",
    "#                 for j, metric in enumerate(self._metric_names):\n",
    "#                     score = self._evaluation_metrics[metric](np.array(y_val_fold), np.array(y_pred))\n",
    "#                     cv_results[i][j] = score\n",
    "            \n",
    "#             cv_scores = pd.DataFrame(columns=self._metric_names, data=cv_results)\n",
    "#             cv_scores.describe()\n",
    "\n",
    "#         y_pred, self.model = self._fit_then_predict(X_train,\n",
    "#                                                      y_train,\n",
    "#                                                      X_test,\n",
    "#                                                      y_test)\n",
    "#         test_scores = {}\n",
    "#         for j, metric in enumerate(self._metric_names):\n",
    "#             score = self._evaluation_metrics[metric](np.array(y_test), np.array(y_pred))\n",
    "#             test_scores[metric] = score\n",
    "#             print(f\"Final Model {metric}: {score:.6f}\")\n",
    "#             print(f\"{self._num_splits}-fold cross validation {metric}: \", cv_results[:,j].mean())\n",
    "        \n",
    "#         return self._model_list, cv_scores, test_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = full_settings.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuwei-1/anaconda3/envs/ktools_2/lib/python3.10/site-packages/lightgbm/engine.py:172: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "[150]\ttrain's l2: 5.05793e+09\tvalid's l2: 6.90248e+09\n",
      "[300]\ttrain's l2: 4.83999e+09\tvalid's l2: 6.87395e+09\n",
      "[450]\ttrain's l2: 4.69867e+09\tvalid's l2: 6.86895e+09\n",
      "[600]\ttrain's l2: 4.57539e+09\tvalid's l2: 6.86855e+09\n",
      "Early stopping, best iteration is:\n",
      "[506]\ttrain's l2: 4.65092e+09\tvalid's l2: 6.86412e+09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuwei-1/anaconda3/envs/ktools_2/lib/python3.10/site-packages/lightgbm/engine.py:172: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "[150]\ttrain's l2: 5.09179e+09\tvalid's l2: 4.05412e+09\n",
      "[300]\ttrain's l2: 4.87031e+09\tvalid's l2: 4.01722e+09\n",
      "[450]\ttrain's l2: 4.72347e+09\tvalid's l2: 4.0292e+09\n",
      "Early stopping, best iteration is:\n",
      "[294]\ttrain's l2: 4.87767e+09\tvalid's l2: 4.01674e+09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuwei-1/anaconda3/envs/ktools_2/lib/python3.10/site-packages/lightgbm/engine.py:172: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "[150]\ttrain's l2: 5.21987e+09\tvalid's l2: 3.06837e+09\n",
      "[300]\ttrain's l2: 5.00229e+09\tvalid's l2: 3.06955e+09\n",
      "Early stopping, best iteration is:\n",
      "[211]\ttrain's l2: 5.11259e+09\tvalid's l2: 3.05561e+09\n",
      "Training until validation scores don't improve for 200 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuwei-1/anaconda3/envs/ktools_2/lib/python3.10/site-packages/lightgbm/engine.py:172: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[150]\ttrain's l2: 5.17415e+09\tvalid's l2: 7.74212e+09\n",
      "[300]\ttrain's l2: 4.9534e+09\tvalid's l2: 7.71269e+09\n",
      "[450]\ttrain's l2: 4.81124e+09\tvalid's l2: 7.71342e+09\n",
      "Early stopping, best iteration is:\n",
      "[269]\ttrain's l2: 4.98812e+09\tvalid's l2: 7.70838e+09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuwei-1/anaconda3/envs/ktools_2/lib/python3.10/site-packages/lightgbm/engine.py:172: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "[150]\ttrain's l2: 5.07989e+09\tvalid's l2: 6.97472e+09\n",
      "[300]\ttrain's l2: 4.8594e+09\tvalid's l2: 6.92943e+09\n",
      "[450]\ttrain's l2: 4.7136e+09\tvalid's l2: 6.95073e+09\n",
      "Early stopping, best iteration is:\n",
      "[261]\ttrain's l2: 4.90501e+09\tvalid's l2: 6.9266e+09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuwei-1/anaconda3/envs/ktools_2/lib/python3.10/site-packages/lightgbm/engine.py:172: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "[150]\ttrain's l2: 5.35802e+09\tvalid's l2: 2.04103e+09\n",
      "[300]\ttrain's l2: 5.13168e+09\tvalid's l2: 1.99151e+09\n",
      "[450]\ttrain's l2: 4.98074e+09\tvalid's l2: 2.00218e+09\n",
      "Early stopping, best iteration is:\n",
      "[342]\ttrain's l2: 5.08433e+09\tvalid's l2: 1.9886e+09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuwei-1/anaconda3/envs/ktools_2/lib/python3.10/site-packages/lightgbm/engine.py:172: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "[150]\ttrain's l2: 5.02718e+09\tvalid's l2: 5.33621e+09\n",
      "[300]\ttrain's l2: 4.81341e+09\tvalid's l2: 5.30043e+09\n",
      "[450]\ttrain's l2: 4.67121e+09\tvalid's l2: 5.30896e+09\n",
      "Early stopping, best iteration is:\n",
      "[314]\ttrain's l2: 4.79857e+09\tvalid's l2: 5.29754e+09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuwei-1/anaconda3/envs/ktools_2/lib/python3.10/site-packages/lightgbm/engine.py:172: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "[150]\ttrain's l2: 5.22515e+09\tvalid's l2: 3.96986e+09\n",
      "[300]\ttrain's l2: 5.00129e+09\tvalid's l2: 3.9326e+09\n",
      "[450]\ttrain's l2: 4.85167e+09\tvalid's l2: 3.94089e+09\n",
      "Early stopping, best iteration is:\n",
      "[256]\ttrain's l2: 5.05257e+09\tvalid's l2: 3.92941e+09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuwei-1/anaconda3/envs/ktools_2/lib/python3.10/site-packages/lightgbm/engine.py:172: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "[150]\ttrain's l2: 5.15185e+09\tvalid's l2: 5.60105e+09\n",
      "[300]\ttrain's l2: 4.93268e+09\tvalid's l2: 5.5506e+09\n",
      "[450]\ttrain's l2: 4.79175e+09\tvalid's l2: 5.54449e+09\n",
      "Early stopping, best iteration is:\n",
      "[388]\ttrain's l2: 4.84842e+09\tvalid's l2: 5.54318e+09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuwei-1/anaconda3/envs/ktools_2/lib/python3.10/site-packages/lightgbm/engine.py:172: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "[150]\ttrain's l2: 5.01427e+09\tvalid's l2: 5.2856e+09\n",
      "[300]\ttrain's l2: 4.79999e+09\tvalid's l2: 5.24873e+09\n",
      "[450]\ttrain's l2: 4.65762e+09\tvalid's l2: 5.26385e+09\n",
      "Early stopping, best iteration is:\n",
      "[293]\ttrain's l2: 4.80704e+09\tvalid's l2: 5.24728e+09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuwei-1/anaconda3/envs/ktools_2/lib/python3.10/site-packages/lightgbm/engine.py:172: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "[150]\ttrain's l2: 5.11284e+09\tvalid's l2: 5.60351e+09\n",
      "[300]\ttrain's l2: 4.89986e+09\tvalid's l2: 5.51753e+09\n",
      "[450]\ttrain's l2: 4.75628e+09\tvalid's l2: 5.51483e+09\n",
      "Early stopping, best iteration is:\n",
      "[362]\ttrain's l2: 4.83679e+09\tvalid's l2: 5.50999e+09\n",
      "Final Model r2: 0.215973\n",
      "10-fold cross validation r2:  0.15267761774394395\n",
      "Final Model rmse: 69788.662105\n",
      "10-fold cross validation rmse:  72551.05395887098\n"
     ]
    }
   ],
   "source": [
    "# data_manager = KaggleDatasetManager(train_df,\n",
    "#                                     full_settings.training_col_names,\n",
    "#                                     full_settings.target_col_name,\n",
    "#                                     0.9,\n",
    "#                                     0.1,\n",
    "#                                     0)\n",
    "\n",
    "# (X_train, \n",
    "# X_valid, \n",
    "# X_test, \n",
    "# y_train, \n",
    "# y_valid,\n",
    "# y_test) = data_manager.dataset_partition()\n",
    "\n",
    "\n",
    "# model = XGBRegressor(**{}, \n",
    "#                     verbosity=0,\n",
    "#                     eval_metric='rmse',\n",
    "#                     tree_method='hist',\n",
    "#                     enable_categorical=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# model = LGBMModel({    'learning_rate': 0.017521301504983752,\n",
    "#     'max_depth': 42,\n",
    "#     'reg_alpha': 0.06876635751774487, \n",
    "#     'reg_lambda': 9.738899198284985,\n",
    "#     'num_leaves': 131,\n",
    "#     'subsample': 0.2683765421728044,\n",
    "#     'colsample_bytree': 0.44346036599709887,\n",
    "#     'n_estimators': 1000,\n",
    "#     'random_state': 42})\n",
    "\n",
    "model = LGBMModel(\n",
    "    \n",
    "    **{'num_leaves': 426,\n",
    " 'max_depth': 20,\n",
    " 'learning_rate': 0.011353178352988012,\n",
    " 'n_estimators': 884,\n",
    " 'subsample': 0.5772552201954328,\n",
    " 'colsample_bytree': 0.9164865430101521,\n",
    " 'reg_alpha': 1.48699088003429e-06,\n",
    " 'reg_lambda': 0.41539458543414265,\n",
    " 'min_data_in_leaf': 73,\n",
    " 'feature_fraction': 0.751673655170548,\n",
    " 'bagging_fraction': 0.5120415391590843,\n",
    " 'bagging_freq': 2,\n",
    " 'min_child_weight': 0.017236362383443497,\n",
    " 'cat_smooth': 54.81317407769262,\n",
    " 'verbose' : -1}\n",
    " \n",
    " )\n",
    "\n",
    "X = train_df.drop(columns=\"price\")\n",
    "y = train_df[\"price\"]\n",
    "\n",
    "# params = {'max_bin': 403, \n",
    "#         'learning_rate': 0.012720488589018275, \n",
    "#         'max_depth': 14, \n",
    "#         'num_boost_round': 921, \n",
    "#         'gamma': 7.327474792423768, \n",
    "#         'min_child_weight': 99.49960880266693, \n",
    "#         'subsample': 0.6815290497072164, \n",
    "#         'colsample_bytree': 0.6882587387019495, \n",
    "#         'colsample_bylevel': 0.6524817277480367, \n",
    "#         'colsample_bynode': 0.9692708790975624, \n",
    "#         'reg_alpha': 4.444851828081367e-06, \n",
    "#         'reg_lambda': 0.9647173450833559, \n",
    "#         'max_cat_threshold': 350, \n",
    "#         'grow_policy': 'lossguide'}\n",
    "\n",
    "# params = {'max_bin': 431, \n",
    "#           'learning_rate': 0.010289485764739134, \n",
    "#           'max_depth': 21, \n",
    "#           'num_boost_round': 516, \n",
    "#           'gamma': 1.3067453666117153, \n",
    "#           'min_child_weight': 44.43942198348885, \n",
    "#           'subsample': 0.6167156245499883, \n",
    "#           'colsample_bytree': 0.6393032383785869, \n",
    "#           'colsample_bylevel': 0.5805205761409957, \n",
    "#           'colsample_bynode': 0.8741882369740219, \n",
    "#           'reg_alpha': 1.0166569230463967e-05, \n",
    "#           'reg_lambda': 0.46012423487591686, \n",
    "#           'max_cat_threshold': 122, \n",
    "#           'grow_policy': 'lossguide',\n",
    "#           'booster' : 'dart'}\n",
    "\n",
    "# params  ={'max_bin': 279, \n",
    "#           'learning_rate': 0.05411783467468672, \n",
    "#           'depth': 15, \n",
    "#           'iterations': 973, \n",
    "#           'bagging_temperature': 3.2498902422443727, \n",
    "#           'subsample': 0.7472867491636364, \n",
    "#           'colsample_bylevel': 0.9612415950869025, \n",
    "#           'min_data_in_leaf': 677.4758118043563, \n",
    "#           'l2_leaf_reg': 9.475872080718132, \n",
    "#           'grow_policy': 'Depthwise', \n",
    "#           'leaf_estimation_iterations': 1, \n",
    "#           'random_strength': 1.153902049647089, \n",
    "#           'leaf_estimation_method': 'Newton'}\n",
    "\n",
    "\n",
    "# model = XGBoostModel(eval_verbosity=False,\n",
    "#                      stopping_rounds=10,\n",
    "#                      **params)\n",
    "\n",
    "# model = CatBoostModel(stopping_rounds=10)\n",
    "\n",
    "num_splits = 10\n",
    "eval_metrics = {\"r2\" : r2_score, \"rmse\" : lambda y, yhat : root_mean_squared_error(y, yhat)}\n",
    "skf = KFold(n_splits=num_splits, shuffle=True, random_state=42)\n",
    "\n",
    "cvt = CrossValidateTestSklearnModel(model,\n",
    "                              eval_metrics,\n",
    "                              skf,\n",
    "                              num_splits)\n",
    "\n",
    "modellist, cv_scores, test_scores = cvt.evaluate(X,\n",
    "                                                 y,\n",
    "                                                 X,\n",
    "                                                 y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Model r2: 0.201357\n",
      "10-fold cross validation r2:  0.1536242554137971\n",
      "Final Model rmse: 70436.184051\n",
      "10-fold cross validation rmse:  72510.51522461096\n"
     ]
    }
   ],
   "source": [
    "params = {\"max_bin\": 403, \n",
    "                     \"learning_rate\": 0.012720488589018275, \n",
    "                     \"max_depth\": 14, \n",
    "                     \"num_boost_round\": 921, \n",
    "                     \"gamma\": 7.327474792423768, \n",
    "                     \"min_child_weight\": 99.49960880266693, \n",
    "                     \"subsample\": 0.6815290497072164, \n",
    "                     \"colsample_bytree\": 0.6882587387019495, \n",
    "                     \"colsample_bylevel\": 0.6524817277480367, \n",
    "                     \"colsample_bynode\": 0.9692708790975624, \n",
    "                     \"reg_alpha\": 4.444851828081367e-06, \n",
    "                     \"reg_lambda\": 0.9647173450833559, \n",
    "                     \"max_cat_threshold\": 350, \n",
    "                     \"grow_policy\": \"lossguide\",\n",
    "                     \"stopping_rounds\" : 10}\n",
    "\n",
    "\n",
    "model = XGBoostModel(eval_verbosity=False,\n",
    "                     **params)\n",
    "\n",
    "num_splits = 10\n",
    "eval_metrics = {\"r2\" : r2_score, \"rmse\" : lambda y, yhat : root_mean_squared_error(y, yhat)}\n",
    "skf = KFold(n_splits=num_splits, shuffle=True, random_state=42)\n",
    "\n",
    "cvt = CrossValidateTestSklearnModel(model,\n",
    "                              eval_metrics,\n",
    "                              skf,\n",
    "                              num_splits)\n",
    "\n",
    "modellist, cv_scores, test_scores = cvt.evaluate(X,\n",
    "                                                 y,\n",
    "                                                 X,\n",
    "                                                 y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv('/Users/yuwei-1/Documents/projects/Kaggle-tools/data/used_car_prices/sample_submission.csv', index_col=0)\n",
    "\n",
    "for i, mdl in enumerate(modellist):\n",
    "    sub[f'price_{i}'] = mdl.predict(test_df[full_settings.training_col_names])\n",
    "\n",
    "sub['price'] = sub.drop(columns=['price']).mean(axis=1)\n",
    "sub['price'].to_csv('submissions/used_cars/used_car_submission_v19.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "188533    18345.165615\n",
       "188534    79120.161055\n",
       "188535    54667.929227\n",
       "188536    31480.856848\n",
       "188537    29701.401953\n",
       "              ...     \n",
       "314218    30720.193571\n",
       "314219    53239.669271\n",
       "314220    20085.596915\n",
       "314221    15467.033823\n",
       "314222    42003.546320\n",
       "Name: price, Length: 125690, dtype: float64"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv('/Users/yuwei-1/Documents/projects/Kaggle-tools/data/used_car_prices/sample_submission.csv', index_col=0)\n",
    "sub1 = pd.read_csv('/Users/yuwei-1/Documents/projects/Kaggle-tools/submissions/used_cars/used_car_submission_v14.csv', index_col=0)\n",
    "sub2 = pd.read_csv('/Users/yuwei-1/Documents/projects/Kaggle-tools/submissions/used_cars/used_car_submission_v18.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "(sub1*0.8+sub2*0.2).to_csv('/Users/yuwei-1/Documents/projects/Kaggle-tools/submissions/used_cars/used_car_submission_v24.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>188533</th>\n",
       "      <td>18237.890692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188534</th>\n",
       "      <td>79004.353844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188535</th>\n",
       "      <td>54261.162182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188536</th>\n",
       "      <td>31170.069879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188537</th>\n",
       "      <td>29735.015363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314218</th>\n",
       "      <td>30973.086857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314219</th>\n",
       "      <td>52579.858017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314220</th>\n",
       "      <td>20182.102932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314221</th>\n",
       "      <td>15621.178259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314222</th>\n",
       "      <td>41696.369856</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>125690 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               price\n",
       "id                  \n",
       "188533  18237.890692\n",
       "188534  79004.353844\n",
       "188535  54261.162182\n",
       "188536  31170.069879\n",
       "188537  29735.015363\n",
       "...              ...\n",
       "314218  30973.086857\n",
       "314219  52579.858017\n",
       "314220  20182.102932\n",
       "314221  15621.178259\n",
       "314222  41696.369856\n",
       "\n",
       "[125690 rows x 1 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(sub1*0.8+sub2*0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ktools",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
