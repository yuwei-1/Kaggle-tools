{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ktools.utils.data_science_pipeline_settings import DataSciencePipelineSettings\n",
    "from ktools.preprocessing.i_feature_transformer import IFeatureTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import functools\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from copy import deepcopy\n",
    "from ktools.preprocessing.basic_feature_transformers import ConvertToLower\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv_path = \"/Users/yuwei-1/Documents/projects/Kaggle-tools/data/used_car_prices/train.csv\"\n",
    "test_csv_path = \"/Users/yuwei-1/Documents/projects/Kaggle-tools/data/used_car_prices/test.csv\"\n",
    "target_col_name = \"price\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = DataSciencePipelineSettings(train_csv_path,\n",
    "                                        test_csv_path,\n",
    "                                        target_col_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "\n",
    "class CategorizeFeatures(IFeatureTransformer):\n",
    "    @staticmethod\n",
    "    def transform(original_settings : DataSciencePipelineSettings, features : List[str]):\n",
    "        settings = deepcopy(original_settings)\n",
    "        settings.combined_df[features] = settings.combined_df[features].astype('object')\n",
    "        settings.categorical_col_names += features\n",
    "        return settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StandardizeNumerical(IFeatureTransformer):\n",
    "    @staticmethod\n",
    "    def transform(original_settings : DataSciencePipelineSettings):\n",
    "        settings = deepcopy(original_settings)\n",
    "        numerical_columns = settings.combined_df.select_dtypes(include=['number']).columns.tolist()\n",
    "        numerical_columns.remove(settings.target_col_name)\n",
    "\n",
    "        numerical_scaler = StandardScaler()\n",
    "        settings.combined_df[numerical_columns] = numerical_scaler.fit_transform(settings.combined_df[numerical_columns])\n",
    "        return settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RemoveRareCategories(IFeatureTransformer):\n",
    "    @staticmethod\n",
    "    def transform(original_settings : DataSciencePipelineSettings, threshold=40):\n",
    "        \n",
    "        global CAT_SIZE\n",
    "        global CAT_EMB  \n",
    "        global RARE\n",
    "\n",
    "        CAT_SIZE = []\n",
    "        CAT_EMB = []\n",
    "        RARE = []\n",
    "\n",
    "        settings = deepcopy(original_settings)\n",
    "        categorical_columns = settings.combined_df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "        for c in categorical_columns:\n",
    "            settings.combined_df[c], _ = settings.combined_df[c].factorize()\n",
    "            settings.combined_df[c] -= settings.combined_df[c].min()\n",
    "            vc = settings.combined_df[c].value_counts()\n",
    "            \n",
    "            RARE.append(vc.loc[vc<threshold].index.values )\n",
    "            n = settings.combined_df[c].nunique()\n",
    "            mn = settings.combined_df[c].min()\n",
    "            mx = settings.combined_df[c].max()\n",
    "            r = len(RARE[-1])\n",
    "            print(f'{c}: nunique={n}, min={mn}, max={mx}, rare_ct={r}')\n",
    "            \n",
    "            CAT_SIZE.append(mx+1 +1)\n",
    "            CAT_EMB.append( int(np.ceil( np.sqrt(mx+1 +1))) )\n",
    "            settings.combined_df[c] += 1\n",
    "            settings.combined_df.loc[settings.combined_df[c].isin(RARE[-1]),c] = 0\n",
    "        \n",
    "        return settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RemoveCategoriesNotInTrain(IFeatureTransformer):\n",
    "    @staticmethod\n",
    "    def transform(original_settings : DataSciencePipelineSettings):\n",
    "        settings = deepcopy(original_settings)\n",
    "        train_df, test_df = settings.update()\n",
    "        \n",
    "        for c in settings.categorical_col_names:\n",
    "            A = train_df[c].unique()\n",
    "            B = test_df[c].unique()\n",
    "            C = np.setdiff1d(B,A)\n",
    "            print(f\"{c}: Test has label encodes = {C} which are not in train.\")\n",
    "                \n",
    "            # RELABEL UNSEEN TEST VALUES AS ZERO\n",
    "            test_df.loc[test_df[c].isin(C), c] = 0\n",
    "\n",
    "        settings.combined_df = pd.concat([train_df, test_df], keys=['train', 'test'])\n",
    "        return settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "brand: nunique=57, min=0, max=56, rare_ct=8\n",
      "model: nunique=1898, min=0, max=1897, rare_ct=551\n",
      "model_year: nunique=36, min=0, max=35, rare_ct=4\n",
      "fuel_type: nunique=8, min=0, max=7, rare_ct=1\n",
      "engine: nunique=1118, min=0, max=1117, rare_ct=308\n",
      "transmission: nunique=52, min=0, max=51, rare_ct=8\n",
      "ext_col: nunique=319, min=0, max=318, rare_ct=99\n",
      "int_col: nunique=156, min=0, max=155, rare_ct=48\n",
      "accident: nunique=3, min=0, max=2, rare_ct=0\n",
      "clean_title: nunique=2, min=0, max=1, rare_ct=0\n",
      "brand: Test has label encodes = [] which are not in train.\n",
      "model: Test has label encodes = [1898] which are not in train.\n",
      "fuel_type: Test has label encodes = [] which are not in train.\n",
      "engine: Test has label encodes = [1118] which are not in train.\n",
      "transmission: Test has label encodes = [] which are not in train.\n",
      "ext_col: Test has label encodes = [] which are not in train.\n",
      "int_col: Test has label encodes = [] which are not in train.\n",
      "accident: Test has label encodes = [] which are not in train.\n",
      "clean_title: Test has label encodes = [] which are not in train.\n",
      "model_year: Test has label encodes = [36] which are not in train.\n"
     ]
    }
   ],
   "source": [
    "full_transforms = [ #ConvertToLower.transform,\n",
    "                    lambda x : CategorizeFeatures.transform(x, features=['model_year']),\n",
    "                    StandardizeNumerical.transform,\n",
    "                    RemoveRareCategories.transform,\n",
    "                    RemoveCategoriesNotInTrain.transform]\n",
    "\n",
    "full_settings = functools.reduce(lambda acc, func: func(acc), full_transforms, settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = full_settings.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_idcs = np.where(full_settings.combined_df.dtypes.to_numpy() == 'int64')[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 4, 5, 6, 7, 8, 9, 10]\n",
      "[8, 44, 7, 3, 34, 8, 18, 13, 2, 2]\n",
      "[58, 1899, 37, 9, 1119, 53, 320, 157, 4, 3]\n"
     ]
    }
   ],
   "source": [
    "print(cat_idcs)\n",
    "print(CAT_EMB)\n",
    "print(CAT_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "\n",
    "class BasicFeedForwardNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_dim : int,\n",
    "                 output_dim : int,\n",
    "                 categorical_idcs : List[int],\n",
    "                 categorical_sizes : List[int],\n",
    "                 categorical_embedding : List[int],\n",
    "                 activation : str,\n",
    "                 num_hidden_layers : int = 1,\n",
    "                 largest_hidden_dim :int = 256,\n",
    "                 dim_decay : float = 1.0,\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self._input_dim = input_dim\n",
    "        self._output_dim = output_dim\n",
    "        \n",
    "        self._categorical_idcs = categorical_idcs\n",
    "        self._categorical_sizes = categorical_sizes\n",
    "        self._categorical_embedding = categorical_embedding\n",
    "        self._num_categories = len(categorical_idcs)\n",
    "        self._activation = activation\n",
    "\n",
    "        self._expanded_dim = self._input_dim - self._num_categories + sum(self._categorical_embedding)\n",
    "        self._largest_hidden_dim = largest_hidden_dim\n",
    "        self._num_hidden_layers = num_hidden_layers\n",
    "        self._dim_decay = dim_decay\n",
    "        \n",
    "        self.embedding_layers = self._create_embedding_layers()\n",
    "        self.model = self._create_dense_layers()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.forward_embeddings(x)\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "      \n",
    "    def forward_embeddings(self, x):\n",
    "        inputs = ()\n",
    "        for i in range(self._input_dim):\n",
    "            if i in self._categorical_idcs:\n",
    "                feature = x[:, i].long()\n",
    "            else:\n",
    "                feature = x[:, i:i+1]\n",
    "            inputs += (self.embedding_layers[i](feature),)\n",
    "        x = torch.cat(inputs, dim=1)\n",
    "        return x\n",
    "    \n",
    "    def _create_dense_layers(self):\n",
    "        layers = OrderedDict()\n",
    "        prev_dim = self._expanded_dim\n",
    "        curr_dim = self._largest_hidden_dim\n",
    "\n",
    "        for l in range(self._num_hidden_layers):\n",
    "            layers[f'layer_{l}'] = nn.Linear(prev_dim, curr_dim)\n",
    "            layers[f'activation_{l}'] = self._get_activation()\n",
    "            prev_dim = curr_dim\n",
    "            curr_dim = max(int(curr_dim*self._dim_decay), self._output_dim)\n",
    "        \n",
    "        layers['last_layer'] = nn.Linear(prev_dim, self._output_dim)\n",
    "        model = nn.Sequential(layers)\n",
    "        return model\n",
    "\n",
    "    def _create_embedding_layers(self):\n",
    "        embeddings = []\n",
    "        for i in range(self._input_dim):\n",
    "            if i in self._categorical_idcs:\n",
    "                j = self._categorical_idcs.index(i)\n",
    "                embeddings += [nn.Embedding(self._categorical_sizes[j], self._categorical_embedding[j])]\n",
    "            else:\n",
    "                embeddings += [nn.Identity()]\n",
    "        return embeddings\n",
    "    \n",
    "    def _get_activation(self):\n",
    "        if self._activation == 'relu':\n",
    "            return nn.ReLU()\n",
    "        elif self._activation == 'gelu':\n",
    "            return nn.GELU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X.to_numpy(), dtype=torch.float32)\n",
    "        self.y = torch.tensor(y.to_numpy(), dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "    \n",
    "\n",
    "def prep_torch_dataset(X, y, batch_size):\n",
    "    torch_dataset = MyDataset(X, y)\n",
    "    dataloader = DataLoader(torch_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BasicFeedForwardNetwork(\n",
      "  (model): Sequential(\n",
      "    (layer_0): Linear(in_features=140, out_features=256, bias=True)\n",
      "    (activation_0): ReLU()\n",
      "    (layer_1): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (activation_1): ReLU()\n",
      "    (layer_2): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (activation_2): ReLU()\n",
      "    (last_layer): Linear(in_features=256, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = BasicFeedForwardNetwork(11,\n",
    "                            1,\n",
    "                            cat_idcs,\n",
    "                            CAT_SIZE,\n",
    "                            CAT_EMB,\n",
    "                            'relu',\n",
    "                            num_hidden_layers=3)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(model):\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.xavier_uniform_(module.weight)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_torch_nn(X_train, y_train, X_test, y_test, epochs=3, activation='relu'):\n",
    "    model = BasicFeedForwardNetwork(11,\n",
    "                                1,\n",
    "                                cat_idcs,\n",
    "                                CAT_SIZE,\n",
    "                                CAT_EMB,\n",
    "                                activation,\n",
    "                                largest_hidden_dim=256,\n",
    "                                dim_decay=1,\n",
    "                                num_hidden_layers=3)\n",
    "    initialize_weights(model)\n",
    "    print(model)\n",
    "    criterion = torch.nn.MSELoss()  # Binary Cross-Entropy Loss for binary classification\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "    BATCH_SIZE = 64\n",
    "    dataloader = prep_torch_dataset(X_train, y_train, BATCH_SIZE)\n",
    "    \n",
    "    print(dataloader)\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs = epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        cum_loss = 0\n",
    "        nb = 0\n",
    "        for batch_features, batch_target in dataloader:\n",
    "            nb+=1\n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass: compute predictions\n",
    "            predictions = model(batch_features).squeeze()  # Model output\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(predictions, batch_target)\n",
    "            cum_loss += loss.item()\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        scheduler.step()\n",
    "        current_lr = scheduler.get_last_lr()[0]\n",
    "        print(\"Current learning rate: \", current_lr)\n",
    "        print(f'Epoch {epoch+1}, Loss: {np.sqrt(cum_loss/nb)}')\n",
    "\n",
    "        testx = torch.tensor(X_test.to_numpy(), dtype=torch.float32)\n",
    "        testy = torch.tensor(y_test.to_numpy(), dtype=torch.float32)\n",
    "\n",
    "        model.eval()\n",
    "        oof_pred = model(testx).squeeze()\n",
    "        print(\"oof performance: \", torch.sqrt(criterion(oof_pred, testy)))\n",
    "\n",
    "    return oof_pred.detach().numpy(), model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = train_df.drop(columns='price'), train_df['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BasicFeedForwardNetwork(\n",
      "  (model): Sequential(\n",
      "    (layer_0): Linear(in_features=140, out_features=256, bias=True)\n",
      "    (activation_0): ReLU()\n",
      "    (layer_1): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (activation_1): ReLU()\n",
      "    (layer_2): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (activation_2): ReLU()\n",
      "    (last_layer): Linear(in_features=256, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "Current learning rate:  0.001\n",
      "Epoch 1, Loss: 76487.0156562648\n",
      "oof performance:  tensor(69369.2578, grad_fn=<SqrtBackward0>)\n",
      "Current learning rate:  0.001\n",
      "Epoch 2, Loss: 75025.95014766863\n",
      "oof performance:  tensor(69311.0078, grad_fn=<SqrtBackward0>)\n",
      "Current learning rate:  0.0001\n",
      "Epoch 3, Loss: 74696.63951007155\n",
      "oof performance:  tensor(68992.5938, grad_fn=<SqrtBackward0>)\n",
      "BasicFeedForwardNetwork(\n",
      "  (model): Sequential(\n",
      "    (layer_0): Linear(in_features=140, out_features=256, bias=True)\n",
      "    (activation_0): ReLU()\n",
      "    (layer_1): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (activation_1): ReLU()\n",
      "    (layer_2): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (activation_2): ReLU()\n",
      "    (last_layer): Linear(in_features=256, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m X_train_fold, X_val_fold \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39miloc[train_index], X\u001b[38;5;241m.\u001b[39miloc[val_index]\n\u001b[1;32m     10\u001b[0m y_train_fold, y_val_fold \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39miloc[train_index], y\u001b[38;5;241m.\u001b[39miloc[val_index]\n\u001b[0;32m---> 12\u001b[0m oof_pred, model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_torch_nn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_fold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_fold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val_fold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val_fold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m model_list \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [model]\n\u001b[1;32m     14\u001b[0m oof_preds[val_index] \u001b[38;5;241m=\u001b[39m oof_pred\n",
      "Cell \u001b[0;32mIn[19], line 39\u001b[0m, in \u001b[0;36mtrain_torch_nn\u001b[0;34m(X_train, y_train, X_test, y_test, epochs, activation)\u001b[0m\n\u001b[1;32m     36\u001b[0m     cum_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;66;03m# Backward pass and optimization\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     42\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/anaconda3/envs/ktools_2/lib/python3.10/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ktools_2/lib/python3.10/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ktools_2/lib/python3.10/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "model_list = []\n",
    "oof_preds = np.zeros(X.shape[0])\n",
    "kf = KFold(5, shuffle=True, random_state=42)\n",
    "\n",
    "for i, (train_index, val_index) in enumerate(kf.split(X,y)):\n",
    "    X_train_fold, X_val_fold = X.iloc[train_index], X.iloc[val_index]\n",
    "    y_train_fold, y_val_fold = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "    oof_pred, model = train_torch_nn(X_train_fold, y_train_fold, X_val_fold, y_val_fold)\n",
    "    model_list += [model]\n",
    "    oof_preds[val_index] = oof_pred\n",
    "\n",
    "print(\"OOF score: \", root_mean_squared_error(y.to_numpy().squeeze(), oof_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BasicFeedForwardNetwork(\n",
      "  (model): Sequential(\n",
      "    (layer_0): Linear(in_features=140, out_features=256, bias=True)\n",
      "    (activation_0): GELU(approximate='none')\n",
      "    (layer_1): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (activation_1): GELU(approximate='none')\n",
      "    (layer_2): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (activation_2): GELU(approximate='none')\n",
      "    (last_layer): Linear(in_features=256, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "Current learning rate:  0.001\n",
      "Epoch 1, Loss: 76556.93201581527\n",
      "oof performance:  tensor(69819.7500, grad_fn=<SqrtBackward0>)\n",
      "Current learning rate:  0.001\n",
      "Epoch 2, Loss: 75008.55708973424\n",
      "oof performance:  tensor(69157.1719, grad_fn=<SqrtBackward0>)\n",
      "Current learning rate:  0.0001\n",
      "Epoch 3, Loss: 74552.47022405844\n",
      "oof performance:  tensor(68957.1875, grad_fn=<SqrtBackward0>)\n",
      "Current learning rate:  0.0001\n",
      "Epoch 4, Loss: 74055.85730419285\n",
      "oof performance:  tensor(68810.0547, grad_fn=<SqrtBackward0>)\n",
      "Current learning rate:  0.0001\n",
      "Epoch 5, Loss: 73963.31731304908\n",
      "oof performance:  tensor(68852.6719, grad_fn=<SqrtBackward0>)\n",
      "BasicFeedForwardNetwork(\n",
      "  (model): Sequential(\n",
      "    (layer_0): Linear(in_features=140, out_features=256, bias=True)\n",
      "    (activation_0): GELU(approximate='none')\n",
      "    (layer_1): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (activation_1): GELU(approximate='none')\n",
      "    (layer_2): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (activation_2): GELU(approximate='none')\n",
      "    (last_layer): Linear(in_features=256, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "Current learning rate:  0.001\n",
      "Epoch 1, Loss: 76410.99035798112\n",
      "oof performance:  tensor(70283.6016, grad_fn=<SqrtBackward0>)\n",
      "Current learning rate:  0.001\n",
      "Epoch 2, Loss: 74657.22942514755\n",
      "oof performance:  tensor(69699.0781, grad_fn=<SqrtBackward0>)\n",
      "Current learning rate:  0.0001\n",
      "Epoch 3, Loss: 74162.90461857987\n",
      "oof performance:  tensor(69486.4219, grad_fn=<SqrtBackward0>)\n",
      "Current learning rate:  0.0001\n",
      "Epoch 4, Loss: 73591.31899607807\n",
      "oof performance:  tensor(69396.5000, grad_fn=<SqrtBackward0>)\n",
      "Current learning rate:  0.0001\n",
      "Epoch 5, Loss: 73452.41440490031\n",
      "oof performance:  tensor(69409.6797, grad_fn=<SqrtBackward0>)\n",
      "BasicFeedForwardNetwork(\n",
      "  (model): Sequential(\n",
      "    (layer_0): Linear(in_features=140, out_features=256, bias=True)\n",
      "    (activation_0): GELU(approximate='none')\n",
      "    (layer_1): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (activation_1): GELU(approximate='none')\n",
      "    (layer_2): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (activation_2): GELU(approximate='none')\n",
      "    (last_layer): Linear(in_features=256, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "Current learning rate:  0.001\n",
      "Epoch 1, Loss: 75107.83496410001\n",
      "oof performance:  tensor(74949.9297, grad_fn=<SqrtBackward0>)\n",
      "Current learning rate:  0.001\n",
      "Epoch 2, Loss: 73488.58718554155\n",
      "oof performance:  tensor(74617.0703, grad_fn=<SqrtBackward0>)\n",
      "Current learning rate:  0.0001\n",
      "Epoch 3, Loss: 72997.71105141491\n",
      "oof performance:  tensor(74580.0391, grad_fn=<SqrtBackward0>)\n",
      "Current learning rate:  0.0001\n",
      "Epoch 4, Loss: 72401.17748841486\n",
      "oof performance:  tensor(74414.3047, grad_fn=<SqrtBackward0>)\n",
      "Current learning rate:  0.0001\n",
      "Epoch 5, Loss: 72304.20892621901\n",
      "oof performance:  tensor(74384.3047, grad_fn=<SqrtBackward0>)\n",
      "BasicFeedForwardNetwork(\n",
      "  (model): Sequential(\n",
      "    (layer_0): Linear(in_features=140, out_features=256, bias=True)\n",
      "    (activation_0): GELU(approximate='none')\n",
      "    (layer_1): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (activation_1): GELU(approximate='none')\n",
      "    (layer_2): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (activation_2): GELU(approximate='none')\n",
      "    (last_layer): Linear(in_features=256, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "Current learning rate:  0.001\n",
      "Epoch 1, Loss: 74570.84670885024\n",
      "oof performance:  tensor(78064.0938, grad_fn=<SqrtBackward0>)\n",
      "Current learning rate:  0.001\n",
      "Epoch 2, Loss: 73172.83463960337\n",
      "oof performance:  tensor(77498.8672, grad_fn=<SqrtBackward0>)\n",
      "Current learning rate:  0.0001\n",
      "Epoch 3, Loss: 72649.41966281687\n",
      "oof performance:  tensor(77089.9766, grad_fn=<SqrtBackward0>)\n",
      "Current learning rate:  0.0001\n",
      "Epoch 4, Loss: 72101.08543547815\n",
      "oof performance:  tensor(76952.6953, grad_fn=<SqrtBackward0>)\n",
      "Current learning rate:  0.0001\n",
      "Epoch 5, Loss: 71976.96495328416\n",
      "oof performance:  tensor(76910.1641, grad_fn=<SqrtBackward0>)\n",
      "BasicFeedForwardNetwork(\n",
      "  (model): Sequential(\n",
      "    (layer_0): Linear(in_features=140, out_features=256, bias=True)\n",
      "    (activation_0): GELU(approximate='none')\n",
      "    (layer_1): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (activation_1): GELU(approximate='none')\n",
      "    (layer_2): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (activation_2): GELU(approximate='none')\n",
      "    (last_layer): Linear(in_features=256, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "Current learning rate:  0.001\n",
      "Epoch 1, Loss: 74630.6486695082\n",
      "oof performance:  tensor(77670.4531, grad_fn=<SqrtBackward0>)\n",
      "Current learning rate:  0.001\n",
      "Epoch 2, Loss: 73060.03396146605\n",
      "oof performance:  tensor(77189.2031, grad_fn=<SqrtBackward0>)\n",
      "Current learning rate:  0.0001\n",
      "Epoch 3, Loss: 72543.89759096617\n",
      "oof performance:  tensor(77019.0625, grad_fn=<SqrtBackward0>)\n",
      "Current learning rate:  0.0001\n",
      "Epoch 4, Loss: 71998.36631701479\n",
      "oof performance:  tensor(76830.4766, grad_fn=<SqrtBackward0>)\n",
      "Current learning rate:  0.0001\n",
      "Epoch 5, Loss: 71876.37619383536\n",
      "oof performance:  tensor(76806.1797, grad_fn=<SqrtBackward0>)\n",
      "OOF score:  73356.32800469182\n"
     ]
    }
   ],
   "source": [
    "model_list = []\n",
    "oof_preds = np.zeros(X.shape[0])\n",
    "kf = KFold(5, shuffle=True, random_state=42)\n",
    "\n",
    "for i, (train_index, val_index) in enumerate(kf.split(X,y)):\n",
    "    X_train_fold, X_val_fold = X.iloc[train_index], X.iloc[val_index]\n",
    "    y_train_fold, y_val_fold = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "    oof_pred, model = train_torch_nn(X_train_fold, y_train_fold, X_val_fold, y_val_fold, epochs=5, activation='gelu')\n",
    "    model_list += [model]\n",
    "    oof_preds[val_index] = oof_pred\n",
    "\n",
    "print(\"OOF score: \", root_mean_squared_error(y.to_numpy().squeeze(), oof_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ktools_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
