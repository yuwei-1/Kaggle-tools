{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ktools.utils.data_science_pipeline_settings import DataSciencePipelineSettings\n",
    "from ktools.preprocessing.i_feature_transformer import IFeatureTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import functools\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from copy import deepcopy\n",
    "from ktools.preprocessing.basic_feature_transformers import ConvertToLower\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv_path = \"/Users/yuwei-1/Documents/projects/Kaggle-tools/data/used_car_prices/train.csv\"\n",
    "test_csv_path = \"/Users/yuwei-1/Documents/projects/Kaggle-tools/data/used_car_prices/test.csv\"\n",
    "target_col_name = \"price\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = DataSciencePipelineSettings(train_csv_path,\n",
    "                                        test_csv_path,\n",
    "                                        target_col_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "\n",
    "class CategorizeFeatures(IFeatureTransformer):\n",
    "    @staticmethod\n",
    "    def transform(original_settings : DataSciencePipelineSettings, features : List[str]):\n",
    "        settings = deepcopy(original_settings)\n",
    "        settings.combined_df[features] = settings.combined_df[features].astype('object')\n",
    "        settings.categorical_col_names += features\n",
    "        return settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StandardizeNumerical(IFeatureTransformer):\n",
    "    @staticmethod\n",
    "    def transform(original_settings : DataSciencePipelineSettings):\n",
    "        settings = deepcopy(original_settings)\n",
    "        numerical_columns = settings.combined_df.select_dtypes(include=['number']).columns.tolist()\n",
    "        numerical_columns.remove(settings.target_col_name)\n",
    "\n",
    "        numerical_scaler = StandardScaler()\n",
    "        settings.combined_df[numerical_columns] = numerical_scaler.fit_transform(settings.combined_df[numerical_columns])\n",
    "        return settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RemoveRareCategories(IFeatureTransformer):\n",
    "    @staticmethod\n",
    "    def transform(original_settings : DataSciencePipelineSettings, threshold=40):\n",
    "        \n",
    "        global CAT_SIZE\n",
    "        global CAT_EMB  \n",
    "        global RARE\n",
    "\n",
    "        CAT_SIZE = []\n",
    "        CAT_EMB = []\n",
    "        RARE = []\n",
    "\n",
    "        settings = deepcopy(original_settings)\n",
    "        categorical_columns = settings.combined_df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "        for c in categorical_columns:\n",
    "            settings.combined_df[c], _ = settings.combined_df[c].factorize()\n",
    "            settings.combined_df[c] -= settings.combined_df[c].min()\n",
    "            vc = settings.combined_df[c].value_counts()\n",
    "            \n",
    "            RARE.append(vc.loc[vc<threshold].index.values )\n",
    "            n = settings.combined_df[c].nunique()\n",
    "            mn = settings.combined_df[c].min()\n",
    "            mx = settings.combined_df[c].max()\n",
    "            r = len(RARE[-1])\n",
    "            print(f'{c}: nunique={n}, min={mn}, max={mx}, rare_ct={r}')\n",
    "            \n",
    "            CAT_SIZE.append(mx+1 +1)\n",
    "            CAT_EMB.append( int(np.ceil( np.sqrt(mx+1 +1))) )\n",
    "            settings.combined_df[c] += 1\n",
    "            settings.combined_df.loc[settings.combined_df[c].isin(RARE[-1]),c] = 0\n",
    "        \n",
    "        return settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RemoveCategoriesNotInTrain(IFeatureTransformer):\n",
    "    @staticmethod\n",
    "    def transform(original_settings : DataSciencePipelineSettings):\n",
    "        settings = deepcopy(original_settings)\n",
    "        train_df, test_df = settings.update()\n",
    "        \n",
    "        for c in settings.categorical_col_names:\n",
    "            A = train_df[c].unique()\n",
    "            B = test_df[c].unique()\n",
    "            C = np.setdiff1d(B,A)\n",
    "            print(f\"{c}: Test has label encodes = {C} which are not in train.\")\n",
    "                \n",
    "            # RELABEL UNSEEN TEST VALUES AS ZERO\n",
    "            test_df.loc[test_df[c].isin(C), c] = 0\n",
    "\n",
    "        settings.combined_df = pd.concat([train_df, test_df], keys=['train', 'test'])\n",
    "        return settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "brand: nunique=57, min=0, max=56, rare_ct=8\n",
      "model: nunique=1898, min=0, max=1897, rare_ct=551\n",
      "model_year: nunique=36, min=0, max=35, rare_ct=4\n",
      "fuel_type: nunique=8, min=0, max=7, rare_ct=1\n",
      "engine: nunique=1118, min=0, max=1117, rare_ct=308\n",
      "transmission: nunique=52, min=0, max=51, rare_ct=8\n",
      "ext_col: nunique=319, min=0, max=318, rare_ct=99\n",
      "int_col: nunique=156, min=0, max=155, rare_ct=48\n",
      "accident: nunique=3, min=0, max=2, rare_ct=0\n",
      "clean_title: nunique=2, min=0, max=1, rare_ct=0\n",
      "brand: Test has label encodes = [] which are not in train.\n",
      "model: Test has label encodes = [1898] which are not in train.\n",
      "fuel_type: Test has label encodes = [] which are not in train.\n",
      "engine: Test has label encodes = [1118] which are not in train.\n",
      "transmission: Test has label encodes = [] which are not in train.\n",
      "ext_col: Test has label encodes = [] which are not in train.\n",
      "int_col: Test has label encodes = [] which are not in train.\n",
      "accident: Test has label encodes = [] which are not in train.\n",
      "clean_title: Test has label encodes = [] which are not in train.\n",
      "model_year: Test has label encodes = [36] which are not in train.\n"
     ]
    }
   ],
   "source": [
    "full_transforms = [ #ConvertToLower.transform,\n",
    "                    lambda x : CategorizeFeatures.transform(x, features=['model_year']),\n",
    "                    StandardizeNumerical.transform,\n",
    "                    RemoveRareCategories.transform,\n",
    "                    RemoveCategoriesNotInTrain.transform]\n",
    "\n",
    "full_settings = functools.reduce(lambda acc, func: func(acc), full_transforms, settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = full_settings.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_idcs = np.where(full_settings.combined_df.dtypes.to_numpy() == 'int64')[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 4, 5, 6, 7, 8, 9, 10]\n",
      "[8, 44, 7, 3, 34, 8, 18, 13, 2, 2]\n",
      "[58, 1899, 37, 9, 1119, 53, 320, 157, 4, 3]\n"
     ]
    }
   ],
   "source": [
    "print(cat_idcs)\n",
    "print(CAT_EMB)\n",
    "print(CAT_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "\n",
    "class BasicFeedForwardNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_dim : int,\n",
    "                 output_dim : int,\n",
    "                 categorical_idcs : List[int],\n",
    "                 categorical_sizes : List[int],\n",
    "                 categorical_embedding : List[int],\n",
    "                 activation : str,\n",
    "                 num_hidden_layers : int = 1,\n",
    "                 largest_hidden_dim :int = 256,\n",
    "                 dim_decay : float = 1.0,\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self._input_dim = input_dim\n",
    "        self._output_dim = output_dim\n",
    "        \n",
    "        self._categorical_idcs = categorical_idcs\n",
    "        self._categorical_sizes = categorical_sizes\n",
    "        self._categorical_embedding = categorical_embedding\n",
    "        self._num_categories = len(categorical_idcs)\n",
    "        self._activation = activation\n",
    "\n",
    "        self._expanded_dim = self._input_dim - self._num_categories + sum(self._categorical_embedding)\n",
    "        self._largest_hidden_dim = largest_hidden_dim\n",
    "        self._num_hidden_layers = num_hidden_layers\n",
    "        self._dim_decay = dim_decay\n",
    "        \n",
    "        self.embedding_layers = self._create_embedding_layers()\n",
    "        self.model = self._create_dense_layers()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.forward_embeddings(x)\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "      \n",
    "    def forward_embeddings(self, x):\n",
    "        inputs = ()\n",
    "        for i in range(self._input_dim):\n",
    "            if i in self._categorical_idcs:\n",
    "                feature = x[:, i].long()\n",
    "            else:\n",
    "                feature = x[:, i:i+1]\n",
    "            inputs += (self.embedding_layers[i](feature),)\n",
    "        x = torch.cat(inputs, dim=1)\n",
    "        return x\n",
    "    \n",
    "    def _create_dense_layers(self):\n",
    "        layers = OrderedDict()\n",
    "        prev_dim = self._expanded_dim\n",
    "        curr_dim = self._largest_hidden_dim\n",
    "\n",
    "        for l in range(self._num_hidden_layers):\n",
    "            layers[f'layer_{l}'] = nn.Linear(prev_dim, curr_dim)\n",
    "            layers[f'activation_{l}'] = self._get_activation()\n",
    "            prev_dim = curr_dim\n",
    "            curr_dim = max(int(curr_dim*self._dim_decay), self._output_dim)\n",
    "        \n",
    "        layers['last_layer'] = nn.Linear(prev_dim, self._output_dim)\n",
    "        model = nn.Sequential(layers)\n",
    "        return model\n",
    "\n",
    "    def _create_embedding_layers(self):\n",
    "        embeddings = []\n",
    "        for i in range(self._input_dim):\n",
    "            if i in self._categorical_idcs:\n",
    "                j = self._categorical_idcs.index(i)\n",
    "                embeddings += [nn.Embedding(self._categorical_sizes[j], self._categorical_embedding[j])]\n",
    "            else:\n",
    "                embeddings += [nn.Identity()]\n",
    "        return embeddings\n",
    "    \n",
    "    def _get_activation(self):\n",
    "        if self._activation == 'relu':\n",
    "            return nn.ReLU()\n",
    "        elif self._activation == 'gelu':\n",
    "            return nn.GELU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X.to_numpy(), dtype=torch.float32)\n",
    "        self.y = torch.tensor(y.to_numpy(), dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "    \n",
    "\n",
    "def prep_torch_dataset(X, y, target_col_name, batch_size):\n",
    "    torch_dataset = MyDataset(X, y)\n",
    "    dataloader = DataLoader(torch_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BasicFeedForwardNetwork(\n",
      "  (model): Sequential(\n",
      "    (layer_0): Linear(in_features=140, out_features=256, bias=True)\n",
      "    (activation_0): ReLU()\n",
      "    (layer_1): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (activation_1): ReLU()\n",
      "    (layer_2): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (activation_2): ReLU()\n",
      "    (last_layer): Linear(in_features=256, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = BasicFeedForwardNetwork(11,\n",
    "                            1,\n",
    "                            cat_idcs,\n",
    "                            CAT_SIZE,\n",
    "                            CAT_EMB,\n",
    "                            'relu',\n",
    "                            num_hidden_layers=3)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(model):\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.xavier_uniform_(module.weight)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_torch_nn(X_train, y_train, X_test, y_test, epochs=3, activation='relu'):\n",
    "    model = BasicFeedForwardNetwork(11,\n",
    "                                1,\n",
    "                                cat_idcs,\n",
    "                                CAT_SIZE,\n",
    "                                CAT_EMB,\n",
    "                                activation,\n",
    "                                largest_hidden_dim=256,\n",
    "                                dim_decay=1,\n",
    "                                num_hidden_layers=3)\n",
    "    initialize_weights(model)\n",
    "    print(model)\n",
    "    criterion = torch.nn.MSELoss()  # Binary Cross-Entropy Loss for binary classification\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "    BATCH_SIZE = 64\n",
    "    dataloader = prep_torch_dataset(X_train, y_train, None, BATCH_SIZE)\n",
    "\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs = epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        cum_loss = 0\n",
    "        nb = 0\n",
    "        for batch_features, batch_target in dataloader:\n",
    "            nb+=1\n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass: compute predictions\n",
    "            predictions = model(batch_features).squeeze()  # Model output\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(predictions, batch_target)\n",
    "            cum_loss += loss.item()\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        scheduler.step()\n",
    "        current_lr = scheduler.get_last_lr()[0]\n",
    "        print(\"Current learning rate: \", current_lr)\n",
    "        print(f'Epoch {epoch+1}, Loss: {np.sqrt(cum_loss/nb)}')\n",
    "\n",
    "        testx = torch.tensor(X_test.to_numpy(), dtype=torch.float32)\n",
    "        testy = torch.tensor(y_test.to_numpy(), dtype=torch.float32)\n",
    "\n",
    "        model.eval()\n",
    "        oof_pred = model(testx).squeeze()\n",
    "        print(\"oof performance: \", torch.sqrt(criterion(oof_pred, testy)))\n",
    "\n",
    "    return oof_pred.detach().numpy(), model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = train_df.drop(columns='price'), train_df['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BasicFeedForwardNetwork(\n",
      "  (model): Sequential(\n",
      "    (layer_0): Linear(in_features=140, out_features=256, bias=True)\n",
      "    (activation_0): ReLU()\n",
      "    (layer_1): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (activation_1): ReLU()\n",
      "    (layer_2): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (activation_2): ReLU()\n",
      "    (last_layer): Linear(in_features=256, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "Current learning rate:  0.001\n",
      "Epoch 1, Loss: 76505.68561537763\n",
      "oof performance:  tensor(69483.8047, grad_fn=<SqrtBackward0>)\n",
      "Current learning rate:  0.001\n",
      "Epoch 2, Loss: 74905.92668495052\n",
      "oof performance:  tensor(69065.3906, grad_fn=<SqrtBackward0>)\n",
      "Current learning rate:  0.0001\n",
      "Epoch 3, Loss: 74474.25939099415\n",
      "oof performance:  tensor(69021.6484, grad_fn=<SqrtBackward0>)\n",
      "BasicFeedForwardNetwork(\n",
      "  (model): Sequential(\n",
      "    (layer_0): Linear(in_features=140, out_features=256, bias=True)\n",
      "    (activation_0): ReLU()\n",
      "    (layer_1): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (activation_1): ReLU()\n",
      "    (layer_2): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (activation_2): ReLU()\n",
      "    (last_layer): Linear(in_features=256, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "Current learning rate:  0.001\n",
      "Epoch 1, Loss: 76397.17189140359\n",
      "oof performance:  tensor(70070.1094, grad_fn=<SqrtBackward0>)\n",
      "Current learning rate:  0.001\n",
      "Epoch 2, Loss: 74765.28513375674\n",
      "oof performance:  tensor(69661.2344, grad_fn=<SqrtBackward0>)\n",
      "Current learning rate:  0.0001\n",
      "Epoch 3, Loss: 74331.03169229685\n",
      "oof performance:  tensor(69604.9219, grad_fn=<SqrtBackward0>)\n",
      "BasicFeedForwardNetwork(\n",
      "  (model): Sequential(\n",
      "    (layer_0): Linear(in_features=140, out_features=256, bias=True)\n",
      "    (activation_0): ReLU()\n",
      "    (layer_1): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (activation_1): ReLU()\n",
      "    (layer_2): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (activation_2): ReLU()\n",
      "    (last_layer): Linear(in_features=256, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "Current learning rate:  0.001\n",
      "Epoch 1, Loss: 75129.59585633248\n",
      "oof performance:  tensor(75071.4062, grad_fn=<SqrtBackward0>)\n",
      "Current learning rate:  0.001\n",
      "Epoch 2, Loss: 73620.41433063713\n",
      "oof performance:  tensor(74881.4219, grad_fn=<SqrtBackward0>)\n",
      "Current learning rate:  0.0001\n",
      "Epoch 3, Loss: 73260.65929253054\n",
      "oof performance:  tensor(74763.0391, grad_fn=<SqrtBackward0>)\n",
      "BasicFeedForwardNetwork(\n",
      "  (model): Sequential(\n",
      "    (layer_0): Linear(in_features=140, out_features=256, bias=True)\n",
      "    (activation_0): ReLU()\n",
      "    (layer_1): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (activation_1): ReLU()\n",
      "    (layer_2): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (activation_2): ReLU()\n",
      "    (last_layer): Linear(in_features=256, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "Current learning rate:  0.001\n",
      "Epoch 1, Loss: 74467.40075582039\n",
      "oof performance:  tensor(77760.6875, grad_fn=<SqrtBackward0>)\n",
      "Current learning rate:  0.001\n",
      "Epoch 2, Loss: 72883.24245956406\n",
      "oof performance:  tensor(77510.9688, grad_fn=<SqrtBackward0>)\n",
      "Current learning rate:  0.0001\n",
      "Epoch 3, Loss: 72525.7694415779\n",
      "oof performance:  tensor(77253.6719, grad_fn=<SqrtBackward0>)\n",
      "BasicFeedForwardNetwork(\n",
      "  (model): Sequential(\n",
      "    (layer_0): Linear(in_features=140, out_features=256, bias=True)\n",
      "    (activation_0): ReLU()\n",
      "    (layer_1): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (activation_1): ReLU()\n",
      "    (layer_2): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (activation_2): ReLU()\n",
      "    (last_layer): Linear(in_features=256, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "Current learning rate:  0.001\n",
      "Epoch 1, Loss: 74533.39953908435\n",
      "oof performance:  tensor(77575.3281, grad_fn=<SqrtBackward0>)\n",
      "Current learning rate:  0.001\n",
      "Epoch 2, Loss: 72953.47867602024\n",
      "oof performance:  tensor(77242.6875, grad_fn=<SqrtBackward0>)\n",
      "Current learning rate:  0.0001\n",
      "Epoch 3, Loss: 72553.2543565478\n",
      "oof performance:  tensor(77051.3750, grad_fn=<SqrtBackward0>)\n",
      "OOF score:  73625.21447040727\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "model_list = []\n",
    "oof_preds = np.zeros(X.shape[0])\n",
    "kf = KFold(5, shuffle=True, random_state=42)\n",
    "\n",
    "for i, (train_index, val_index) in enumerate(kf.split(X,y)):\n",
    "    X_train_fold, X_val_fold = X.iloc[train_index], X.iloc[val_index]\n",
    "    y_train_fold, y_val_fold = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "    oof_pred, model = train_torch_nn(X_train_fold, y_train_fold, X_val_fold, y_val_fold)\n",
    "    model_list += [model]\n",
    "    oof_preds[val_index] = oof_pred\n",
    "\n",
    "print(\"OOF score: \", root_mean_squared_error(y.to_numpy().squeeze(), oof_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BasicFeedForwardNetwork(\n",
      "  (model): Sequential(\n",
      "    (layer_0): Linear(in_features=140, out_features=256, bias=True)\n",
      "    (activation_0): GELU(approximate='none')\n",
      "    (layer_1): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (activation_1): GELU(approximate='none')\n",
      "    (layer_2): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (activation_2): GELU(approximate='none')\n",
      "    (last_layer): Linear(in_features=256, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "Current learning rate:  0.001\n",
      "Epoch 1, Loss: 76556.93201581527\n",
      "oof performance:  tensor(69819.7500, grad_fn=<SqrtBackward0>)\n",
      "Current learning rate:  0.001\n",
      "Epoch 2, Loss: 75008.55708973424\n",
      "oof performance:  tensor(69157.1719, grad_fn=<SqrtBackward0>)\n",
      "Current learning rate:  0.0001\n",
      "Epoch 3, Loss: 74552.47022405844\n",
      "oof performance:  tensor(68957.1875, grad_fn=<SqrtBackward0>)\n",
      "Current learning rate:  0.0001\n",
      "Epoch 4, Loss: 74055.85730419285\n",
      "oof performance:  tensor(68810.0547, grad_fn=<SqrtBackward0>)\n",
      "Current learning rate:  0.0001\n",
      "Epoch 5, Loss: 73963.31731304908\n",
      "oof performance:  tensor(68852.6719, grad_fn=<SqrtBackward0>)\n",
      "BasicFeedForwardNetwork(\n",
      "  (model): Sequential(\n",
      "    (layer_0): Linear(in_features=140, out_features=256, bias=True)\n",
      "    (activation_0): GELU(approximate='none')\n",
      "    (layer_1): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (activation_1): GELU(approximate='none')\n",
      "    (layer_2): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (activation_2): GELU(approximate='none')\n",
      "    (last_layer): Linear(in_features=256, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "Current learning rate:  0.001\n",
      "Epoch 1, Loss: 76410.99035798112\n",
      "oof performance:  tensor(70283.6016, grad_fn=<SqrtBackward0>)\n",
      "Current learning rate:  0.001\n",
      "Epoch 2, Loss: 74657.22942514755\n",
      "oof performance:  tensor(69699.0781, grad_fn=<SqrtBackward0>)\n",
      "Current learning rate:  0.0001\n",
      "Epoch 3, Loss: 74162.90461857987\n",
      "oof performance:  tensor(69486.4219, grad_fn=<SqrtBackward0>)\n",
      "Current learning rate:  0.0001\n",
      "Epoch 4, Loss: 73591.31899607807\n",
      "oof performance:  tensor(69396.5000, grad_fn=<SqrtBackward0>)\n",
      "Current learning rate:  0.0001\n",
      "Epoch 5, Loss: 73452.41440490031\n",
      "oof performance:  tensor(69409.6797, grad_fn=<SqrtBackward0>)\n",
      "BasicFeedForwardNetwork(\n",
      "  (model): Sequential(\n",
      "    (layer_0): Linear(in_features=140, out_features=256, bias=True)\n",
      "    (activation_0): GELU(approximate='none')\n",
      "    (layer_1): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (activation_1): GELU(approximate='none')\n",
      "    (layer_2): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (activation_2): GELU(approximate='none')\n",
      "    (last_layer): Linear(in_features=256, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "Current learning rate:  0.001\n",
      "Epoch 1, Loss: 75107.83496410001\n",
      "oof performance:  tensor(74949.9297, grad_fn=<SqrtBackward0>)\n",
      "Current learning rate:  0.001\n",
      "Epoch 2, Loss: 73488.58718554155\n",
      "oof performance:  tensor(74617.0703, grad_fn=<SqrtBackward0>)\n",
      "Current learning rate:  0.0001\n",
      "Epoch 3, Loss: 72997.71105141491\n",
      "oof performance:  tensor(74580.0391, grad_fn=<SqrtBackward0>)\n",
      "Current learning rate:  0.0001\n",
      "Epoch 4, Loss: 72401.17748841486\n",
      "oof performance:  tensor(74414.3047, grad_fn=<SqrtBackward0>)\n",
      "Current learning rate:  0.0001\n",
      "Epoch 5, Loss: 72304.20892621901\n",
      "oof performance:  tensor(74384.3047, grad_fn=<SqrtBackward0>)\n",
      "BasicFeedForwardNetwork(\n",
      "  (model): Sequential(\n",
      "    (layer_0): Linear(in_features=140, out_features=256, bias=True)\n",
      "    (activation_0): GELU(approximate='none')\n",
      "    (layer_1): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (activation_1): GELU(approximate='none')\n",
      "    (layer_2): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (activation_2): GELU(approximate='none')\n",
      "    (last_layer): Linear(in_features=256, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "Current learning rate:  0.001\n",
      "Epoch 1, Loss: 74570.84670885024\n",
      "oof performance:  tensor(78064.0938, grad_fn=<SqrtBackward0>)\n",
      "Current learning rate:  0.001\n",
      "Epoch 2, Loss: 73172.83463960337\n",
      "oof performance:  tensor(77498.8672, grad_fn=<SqrtBackward0>)\n",
      "Current learning rate:  0.0001\n",
      "Epoch 3, Loss: 72649.41966281687\n",
      "oof performance:  tensor(77089.9766, grad_fn=<SqrtBackward0>)\n",
      "Current learning rate:  0.0001\n",
      "Epoch 4, Loss: 72101.08543547815\n",
      "oof performance:  tensor(76952.6953, grad_fn=<SqrtBackward0>)\n",
      "Current learning rate:  0.0001\n",
      "Epoch 5, Loss: 71976.96495328416\n",
      "oof performance:  tensor(76910.1641, grad_fn=<SqrtBackward0>)\n",
      "BasicFeedForwardNetwork(\n",
      "  (model): Sequential(\n",
      "    (layer_0): Linear(in_features=140, out_features=256, bias=True)\n",
      "    (activation_0): GELU(approximate='none')\n",
      "    (layer_1): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (activation_1): GELU(approximate='none')\n",
      "    (layer_2): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (activation_2): GELU(approximate='none')\n",
      "    (last_layer): Linear(in_features=256, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "Current learning rate:  0.001\n",
      "Epoch 1, Loss: 74630.6486695082\n",
      "oof performance:  tensor(77670.4531, grad_fn=<SqrtBackward0>)\n",
      "Current learning rate:  0.001\n",
      "Epoch 2, Loss: 73060.03396146605\n",
      "oof performance:  tensor(77189.2031, grad_fn=<SqrtBackward0>)\n",
      "Current learning rate:  0.0001\n",
      "Epoch 3, Loss: 72543.89759096617\n",
      "oof performance:  tensor(77019.0625, grad_fn=<SqrtBackward0>)\n",
      "Current learning rate:  0.0001\n",
      "Epoch 4, Loss: 71998.36631701479\n",
      "oof performance:  tensor(76830.4766, grad_fn=<SqrtBackward0>)\n",
      "Current learning rate:  0.0001\n",
      "Epoch 5, Loss: 71876.37619383536\n",
      "oof performance:  tensor(76806.1797, grad_fn=<SqrtBackward0>)\n",
      "OOF score:  73356.32800469182\n"
     ]
    }
   ],
   "source": [
    "model_list = []\n",
    "oof_preds = np.zeros(X.shape[0])\n",
    "kf = KFold(5, shuffle=True, random_state=42)\n",
    "\n",
    "for i, (train_index, val_index) in enumerate(kf.split(X,y)):\n",
    "    X_train_fold, X_val_fold = X.iloc[train_index], X.iloc[val_index]\n",
    "    y_train_fold, y_val_fold = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "    oof_pred, model = train_torch_nn(X_train_fold, y_train_fold, X_val_fold, y_val_fold, epochs=5, activation='gelu')\n",
    "    model_list += [model]\n",
    "    oof_preds[val_index] = oof_pred\n",
    "\n",
    "print(\"OOF score: \", root_mean_squared_error(y.to_numpy().squeeze(), oof_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ktools_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
