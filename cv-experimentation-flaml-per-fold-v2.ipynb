{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d441d2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FOLD PARAMETER - SET THIS BEFORE RUNNING\n",
    "# =============================================================================\n",
    "CURRENT_FOLD = 0  # Set to 0, 1, 2, 3, or 4 for 5-fold CV\n",
    "N_FOLDS = 5\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# FLAML specific parameters\n",
    "FLAML_TIME_BUDGET = 60  # seconds per model search\n",
    "FLAML_ESTIMATOR_LIST = [\"lgbm\", \"xgboost\", \"xgb_limitdepth\", \"catboost\", \"rf\", \"extra_tree\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e194cbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# IMPORTS\n",
    "# =============================================================================\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Tuple, Union\n",
    "from dataclasses import dataclass\n",
    "from abc import ABC, abstractmethod\n",
    "from enum import Enum\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler\n",
    "from flaml import AutoML\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcc7547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# KTOOLS DEPENDENCIES (INLINED FOR KAGGLE)\n",
    "# =============================================================================\n",
    "\n",
    "T = Union[np.ndarray, pd.DataFrame]\n",
    "pd_to_np = lambda x: x.to_numpy() if isinstance(x, pd.DataFrame) else x\n",
    "\n",
    "\n",
    "def infer_task(y: np.ndarray | pd.Series) -> str:\n",
    "    \"\"\"\n",
    "    Will infer binary, multiclass classification or regression based on the target values.\n",
    "    \"\"\"\n",
    "    if isinstance(y, pd.Series):\n",
    "        y = y.to_numpy()\n",
    "    y = y.flatten()\n",
    "\n",
    "    nuniques = np.unique(y).shape[0]\n",
    "    has_floats = np.any(y % 1 != 0)\n",
    "\n",
    "    if has_floats:\n",
    "        print(\"Target contains float values. Inferring regression task.\")\n",
    "        return \"regression\"\n",
    "    elif nuniques == 2:\n",
    "        print(\"Target contains two unique values. Inferring binary classification task.\")\n",
    "        return \"binary_classification\"\n",
    "    elif nuniques > 2:\n",
    "        print(\"Target contains more than two unique values. Inferring multiclass classification task.\")\n",
    "        return \"multiclass_classification\"\n",
    "\n",
    "    raise ValueError(\n",
    "        \"Unable to infer task type from target values. Is there only one target value?\"\n",
    "    )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DatasetConfig:\n",
    "    training_col_names: List[str]\n",
    "    target_col_name: str\n",
    "    numerical_col_names: List[str]\n",
    "    categorical_col_names: List[str]\n",
    "    name: Optional[str] = None\n",
    "\n",
    "\n",
    "class BasePreprocessor(ABC):\n",
    "    name = \"base-preprocessor\"\n",
    "\n",
    "    def __init__(self, config: DatasetConfig):\n",
    "        self._fitted = False\n",
    "        self.config = config\n",
    "\n",
    "    @abstractmethod\n",
    "    def fit(self, data: pd.DataFrame) -> \"BasePreprocessor\":\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def transform(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        pass\n",
    "\n",
    "    def fit_transform(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        return self.fit(data).transform(data)\n",
    "\n",
    "    @property\n",
    "    def fitted(self) -> bool:\n",
    "        return self._fitted\n",
    "\n",
    "\n",
    "class CategoricalEncoder(BasePreprocessor):\n",
    "    name = \"categorical-encoder\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: DatasetConfig,\n",
    "        handle_unknown: str = \"use_encoded_value\",\n",
    "        unknown_value: int = -2,\n",
    "        encoded_missing_value: int = -1,\n",
    "        **encoder_kwargs,\n",
    "    ) -> None:\n",
    "        super().__init__(config)\n",
    "        self.encode_missing_value = encoded_missing_value\n",
    "        self.encoder = OrdinalEncoder(\n",
    "            handle_unknown=handle_unknown,\n",
    "            unknown_value=unknown_value,\n",
    "            encoded_missing_value=encoded_missing_value,\n",
    "            **encoder_kwargs,\n",
    "        )\n",
    "\n",
    "    def fit(self, data: pd.DataFrame) -> \"CategoricalEncoder\":\n",
    "        self.encoder.fit(data[self.config.categorical_col_names])\n",
    "        self._fitted = True\n",
    "        return self\n",
    "\n",
    "    def transform(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        copy = data.copy()\n",
    "        mask = copy[self.config.categorical_col_names].isna()\n",
    "        copy[self.config.categorical_col_names] = self.encoder.transform(\n",
    "            copy[self.config.categorical_col_names]\n",
    "        ).astype(int)\n",
    "        copy[self.config.categorical_col_names] = (\n",
    "            copy[self.config.categorical_col_names]\n",
    "            .where(~mask, self.encode_missing_value)\n",
    "            .astype(\"category\")\n",
    "        )\n",
    "        return copy\n",
    "\n",
    "\n",
    "class StandardScale(BasePreprocessor):\n",
    "    name = \"standard-scaler\"\n",
    "\n",
    "    def __init__(self, config: DatasetConfig) -> None:\n",
    "        super().__init__(config)\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def fit(self, data: pd.DataFrame) -> \"StandardScale\":\n",
    "        self.scaler.fit(data[self.config.numerical_col_names])\n",
    "        self._fitted = True\n",
    "        return self\n",
    "\n",
    "    def transform(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        copy = data.copy()\n",
    "        copy[self.config.numerical_col_names] = self.scaler.transform(\n",
    "            copy[self.config.numerical_col_names]\n",
    "        )\n",
    "        return copy\n",
    "\n",
    "\n",
    "class PreprocessingPipeline:\n",
    "    def __init__(self, preprocessors: List[BasePreprocessor]) -> None:\n",
    "        self.preprocessors = preprocessors\n",
    "\n",
    "    def train_pipe(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        for preprocessor in self.preprocessors:\n",
    "            data = preprocessor.fit_transform(data)\n",
    "        return data\n",
    "\n",
    "    def inference_pipe(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        for preprocessor in self.preprocessors:\n",
    "            data = preprocessor.transform(data)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda41cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FLAML MODEL (INLINED)\n",
    "# =============================================================================\n",
    "\n",
    "task_to_default_metric = {\n",
    "    \"regression\": \"mse\",\n",
    "    \"binary_classification\": \"roc_auc\",\n",
    "    \"multiclass_classification\": \"accuracy\"\n",
    "}\n",
    "\n",
    "\n",
    "class DefaultObjective(Enum):\n",
    "    regression = \"regression\"\n",
    "    binary_classification = \"classification\"\n",
    "    multiclass_classification = \"classification\"\n",
    "\n",
    "\n",
    "class FLAMLModel:\n",
    "    def __init__(\n",
    "        self,\n",
    "        time_budget: float = 60,\n",
    "        metric: Optional[str] = None,\n",
    "        task: Optional[str] = None,\n",
    "        n_jobs: int = -1,\n",
    "        estimator_list: List[str] = [\n",
    "            \"lgbm\",\n",
    "            \"xgboost\",\n",
    "            \"xgb_limitdepth\",\n",
    "            \"catboost\",\n",
    "            \"rf\",\n",
    "            \"extra_tree\",\n",
    "        ],\n",
    "        verbose: int = 0,\n",
    "        seed: int = 42,\n",
    "        **extra_params,\n",
    "    ) -> None:\n",
    "        self._random_state = seed\n",
    "        self._task = task\n",
    "        self._time_budget = time_budget\n",
    "        self._fitted = False\n",
    "        self.model = None\n",
    "        self._model_parameters = {\n",
    "            \"time_budget\": time_budget,\n",
    "            \"metric\": metric,\n",
    "            \"task\": task,\n",
    "            \"n_jobs\": n_jobs,\n",
    "            \"estimator_list\": estimator_list,\n",
    "            \"verbose\": verbose,\n",
    "            \"seed\": seed,\n",
    "            **extra_params,\n",
    "        }\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        X: T,\n",
    "        y: T,\n",
    "        validation_set: Optional[Tuple[T, T]] = None,\n",
    "        weights: Optional[T] = None,\n",
    "    ) -> \"FLAMLModel\":\n",
    "\n",
    "        if self._task is None:\n",
    "            task_id = infer_task(y)\n",
    "            self._task = task_id\n",
    "            self._model_parameters[\"task\"] = DefaultObjective[task_id].value\n",
    "            self._model_parameters[\"metric\"] = task_to_default_metric[task_id]\n",
    "\n",
    "        if weights is not None:\n",
    "            print(\"Warning: FLAML does not currently support sample weights. Ignoring weights.\")\n",
    "\n",
    "        self.model = AutoML(**self._model_parameters)\n",
    "\n",
    "        fitting_kwargs = {}\n",
    "\n",
    "        if validation_set is not None:\n",
    "            X_val, y_val = validation_set\n",
    "            X_val, y_val = pd_to_np(X_val), pd_to_np(y_val)\n",
    "            fitting_kwargs[\"X_val\"] = X_val\n",
    "            fitting_kwargs[\"y_val\"] = y_val\n",
    "\n",
    "        X, y = pd_to_np(X), pd_to_np(y)\n",
    "        fitting_kwargs[\"X_train\"] = X\n",
    "        fitting_kwargs[\"y_train\"] = y\n",
    "\n",
    "        self.model.fit(\n",
    "            **fitting_kwargs,\n",
    "            time_budget=self._time_budget,\n",
    "        )\n",
    "        self._fitted = True\n",
    "        return self\n",
    "\n",
    "    def predict(self, X: T) -> np.ndarray:\n",
    "        X = pd_to_np(X)\n",
    "        if self._task == \"regression\":\n",
    "            y_pred = self.model.predict(X)\n",
    "        elif self._task == \"binary_classification\":\n",
    "            y_pred = self.model.predict_proba(X)[:, 1]\n",
    "        elif self._task == \"multiclass_classification\":\n",
    "            y_pred = self.model.predict_proba(X)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        return y_pred\n",
    "\n",
    "    @property\n",
    "    def fitted(self) -> bool:\n",
    "        return self._fitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b11f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MODEL PIPELINE (INLINED)\n",
    "# =============================================================================\n",
    "\n",
    "class ModelPipeline:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        config: DatasetConfig,\n",
    "        preprocessor: PreprocessingPipeline = None,\n",
    "    ) -> None:\n",
    "        self.model = model\n",
    "        self.config = config\n",
    "        self.preprocessor = preprocessor if preprocessor is not None else PreprocessingPipeline([])\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        train_data: pd.DataFrame,\n",
    "        validation_data: Optional[pd.DataFrame] = None,\n",
    "        weights: Optional[Union[pd.Series, np.ndarray]] = None,\n",
    "    ) -> \"ModelPipeline\":\n",
    "        train_data = self.preprocessor.train_pipe(train_data)\n",
    "        X_train = train_data.drop(columns=[self.config.target_col_name])\n",
    "        y_train = train_data[self.config.target_col_name]\n",
    "\n",
    "        if validation_data is not None:\n",
    "            validation_data = self.preprocessor.inference_pipe(validation_data)\n",
    "            X_valid = validation_data.drop(columns=[self.config.target_col_name])\n",
    "            y_valid = validation_data[self.config.target_col_name]\n",
    "            validation_data = (X_valid, y_valid)\n",
    "\n",
    "        self.model.fit(\n",
    "            X=X_train, y=y_train, validation_set=validation_data, weights=weights\n",
    "        )\n",
    "        return self\n",
    "\n",
    "    def predict(self, data: pd.DataFrame) -> np.ndarray:\n",
    "        data = self.preprocessor.inference_pipe(data)\n",
    "        X_test = data[self.config.training_col_names]\n",
    "        return self.model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a50050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DATA LOADING\n",
    "# =============================================================================\n",
    "\n",
    "DATA_PATH = Path(\"./data/diabetes_prediction/\")\n",
    "TARGET = \"diagnosed_diabetes\"\n",
    "\n",
    "# id split for stratification\n",
    "split_id = 678000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3ee9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_data = pd.read_csv(DATA_PATH / \"original.csv\")\n",
    "train_data = pd.read_csv(DATA_PATH / \"train.csv\", index_col=0)\n",
    "test_data = pd.read_csv(DATA_PATH / \"test.csv\", index_col=0).assign(data=0)\n",
    "\n",
    "test_data[\"data\"] = test_data[\"data\"].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70df9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (train_data.columns == train_data.columns.intersection(orig_data.columns)).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d105a684",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_data[orig_data.columns.difference(train_data.columns)].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ec1103",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.assign(data=np.nan)\n",
    "train_data.iloc[:split_id, -1] = 1\n",
    "train_data.iloc[split_id:, -1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf19042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FEATURE ENGINEERING\n",
    "# =============================================================================\n",
    "\n",
    "TARGET = 'diagnosed_diabetes'\n",
    "BASE = [col for col in train_data.columns if col not in ['id', TARGET, 'data']]\n",
    "\n",
    "ORIG = []\n",
    "\n",
    "for col in BASE:\n",
    "    # MEAN encoding from original data\n",
    "    for tgt in [TARGET, 'glucose_fasting', 'glucose_postprandial', 'hba1c']:\n",
    "        \n",
    "        mean_map = orig_data.groupby(col)[tgt].mean()\n",
    "        new_mean_col_name = f\"orig_mean_{tgt}_grouped_by_{col}\"\n",
    "        mean_map.name = new_mean_col_name\n",
    "        \n",
    "        print(col, tgt)\n",
    "        train_data = train_data.merge(mean_map, on=col, how='left')\n",
    "        test_data = test_data.merge(mean_map, on=col, how='left')\n",
    "        ORIG.append(new_mean_col_name)\n",
    "\n",
    "print(len(ORIG), 'Orig Features Created!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a4e049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DATASET CONFIG\n",
    "# =============================================================================\n",
    "\n",
    "training_col_names = train_data.drop(columns=TARGET).columns.tolist()\n",
    "\n",
    "numerical_col_names = (\n",
    "    train_data.drop(columns=TARGET)\n",
    "    .select_dtypes(include=[\"number\"])\n",
    "    .columns.tolist()\n",
    ")\n",
    "categorical_col_names = train_data.select_dtypes(\n",
    "    include=[\"object\"]\n",
    ").columns.tolist()\n",
    "\n",
    "config = DatasetConfig(\n",
    "    training_col_names=training_col_names,\n",
    "    numerical_col_names=numerical_col_names + ORIG,\n",
    "    categorical_col_names=categorical_col_names,\n",
    "    target_col_name=TARGET,\n",
    ")\n",
    "\n",
    "print(f\"Training columns: {len(training_col_names)}\")\n",
    "print(f\"Numerical columns: {len(numerical_col_names)}\")\n",
    "print(f\"Categorical columns: {len(categorical_col_names)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f107f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratification categories\n",
    "categories_of_interest = train_data[\"diagnosed_diabetes\"].astype(str) + \"_\" + train_data[\"data\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747eee05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SINGLE FOLD TRAINING\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"TRAINING FOLD {CURRENT_FOLD} of {N_FOLDS}\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "preprocessors = [StandardScale(config), CategoricalEncoder(config)]\n",
    "\n",
    "# Get fold indices\n",
    "kfold = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n",
    "fold_splits = list(kfold.split(train_data, categories_of_interest))\n",
    "\n",
    "train_index, val_index = fold_splits[CURRENT_FOLD]\n",
    "\n",
    "print(f\"Train size: {len(train_index)}, Validation size: {len(val_index)}\")\n",
    "\n",
    "# Prepare fold data\n",
    "train_fold: pd.DataFrame = train_data.iloc[train_index].copy()\n",
    "val_fold: pd.DataFrame = train_data.iloc[val_index].copy()\n",
    "subsetval_fold = val_fold[val_fold[\"data\"] == 0.0]\n",
    "\n",
    "train_fold[\"data\"] = train_fold[\"data\"].astype(\"category\")\n",
    "weights = np.where(train_fold[\"data\"] == 0, 1.0, 1.0)\n",
    "\n",
    "# Create pipeline with FLAML model\n",
    "pipe = ModelPipeline(\n",
    "    model=FLAMLModel(\n",
    "        time_budget=FLAML_TIME_BUDGET,\n",
    "        estimator_list=FLAML_ESTIMATOR_LIST,\n",
    "        seed=RANDOM_STATE,\n",
    "        verbose=1,\n",
    "    ),\n",
    "    config=config,\n",
    "    preprocessor=PreprocessingPipeline(preprocessors=preprocessors),\n",
    ")\n",
    "\n",
    "# Fit model\n",
    "print(f\"\\nFitting FLAML model (time budget: {FLAML_TIME_BUDGET}s)...\")\n",
    "pipe.fit(train_fold, validation_data=subsetval_fold, weights=weights)\n",
    "\n",
    "# Predictions\n",
    "y_pred = pipe.predict(subsetval_fold)\n",
    "test_pred = pipe.predict(test_data)\n",
    "oof_pred = pipe.predict(val_fold)\n",
    "\n",
    "# Score\n",
    "score = roc_auc_score(subsetval_fold[TARGET], y_pred)\n",
    "print(f\"\\nFold {CURRENT_FOLD} ROC AUC Score: {score:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6acf219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SAVE PREDICTIONS\n",
    "# =============================================================================\n",
    "\n",
    "save_path = Path(\"./data/diabetes_prediction/\")\n",
    "\n",
    "# Create directories if they don't exist\n",
    "(save_path / \"oofs\" / \"flaml\").mkdir(parents=True, exist_ok=True)\n",
    "(save_path / \"test_preds\" / \"flaml\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save OOF predictions with fold indices\n",
    "oof_df = pd.DataFrame({\n",
    "    \"index\": val_index,\n",
    "    f\"oof_pred_fold_{CURRENT_FOLD}\": oof_pred,\n",
    "})\n",
    "oof_df.to_csv(save_path / \"oofs\" / \"flaml\" / f\"oof_preds_fold_{CURRENT_FOLD}.csv\", index=False)\n",
    "\n",
    "# Save test predictions\n",
    "test_df = pd.DataFrame({\n",
    "    f\"test_pred_fold_{CURRENT_FOLD}\": test_pred,\n",
    "})\n",
    "test_df.to_csv(save_path / \"test_preds\" / \"flaml\" / f\"test_preds_fold_{CURRENT_FOLD}.csv\", index=False)\n",
    "\n",
    "# Save fold score\n",
    "score_df = pd.DataFrame({\n",
    "    \"fold\": [CURRENT_FOLD],\n",
    "    \"roc_auc_score\": [score],\n",
    "    \"time_budget\": [FLAML_TIME_BUDGET],\n",
    "})\n",
    "score_df.to_csv(save_path / \"oofs\" / \"flaml\" / f\"score_fold_{CURRENT_FOLD}.csv\", index=False)\n",
    "\n",
    "print(f\"\\nSaved predictions for fold {CURRENT_FOLD}:\")\n",
    "print(f\"  - OOF: {save_path / 'oofs' / 'flaml' / f'oof_preds_fold_{CURRENT_FOLD}.csv'}\")\n",
    "print(f\"  - Test: {save_path / 'test_preds' / 'flaml' / f'test_preds_fold_{CURRENT_FOLD}.csv'}\")\n",
    "print(f\"  - Score: {save_path / 'oofs' / 'flaml' / f'score_fold_{CURRENT_FOLD}.csv'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820ad802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# HELPER: COMBINE ALL FOLDS (RUN AFTER ALL FOLDS ARE COMPLETE)\n",
    "# =============================================================================\n",
    "\n",
    "def combine_all_folds(save_path: Path, n_folds: int = 5):\n",
    "    \"\"\"\n",
    "    Combine OOF and test predictions from all folds.\n",
    "    Run this after all folds have been processed.\n",
    "    \"\"\"\n",
    "    # Combine OOF predictions\n",
    "    oof_dfs = []\n",
    "    for fold in range(n_folds):\n",
    "        oof_path = save_path / \"oofs\" / \"flaml\" / f\"oof_preds_fold_{fold}.csv\"\n",
    "        if oof_path.exists():\n",
    "            oof_dfs.append(pd.read_csv(oof_path))\n",
    "    \n",
    "    if len(oof_dfs) == n_folds:\n",
    "        combined_oof = pd.concat(oof_dfs, ignore_index=True)\n",
    "        combined_oof = combined_oof.sort_values(\"index\").reset_index(drop=True)\n",
    "        combined_oof.to_csv(save_path / \"oofs\" / \"flaml\" / \"combined_oof_preds.csv\", index=False)\n",
    "        print(f\"Combined OOF predictions saved: {combined_oof.shape}\")\n",
    "    else:\n",
    "        print(f\"Missing folds. Found {len(oof_dfs)} of {n_folds} folds.\")\n",
    "    \n",
    "    # Combine test predictions (average)\n",
    "    test_dfs = []\n",
    "    for fold in range(n_folds):\n",
    "        test_path = save_path / \"test_preds\" / \"flaml\" / f\"test_preds_fold_{fold}.csv\"\n",
    "        if test_path.exists():\n",
    "            test_dfs.append(pd.read_csv(test_path))\n",
    "    \n",
    "    if len(test_dfs) == n_folds:\n",
    "        combined_test = pd.concat([df.iloc[:, 0] for df in test_dfs], axis=1)\n",
    "        combined_test[\"test_pred_mean\"] = combined_test.mean(axis=1)\n",
    "        combined_test.to_csv(save_path / \"test_preds\" / \"flaml\" / \"combined_test_preds.csv\", index=False)\n",
    "        print(f\"Combined test predictions saved: {combined_test.shape}\")\n",
    "    else:\n",
    "        print(f\"Missing folds. Found {len(test_dfs)} of {n_folds} folds.\")\n",
    "    \n",
    "    # Combine scores\n",
    "    score_dfs = []\n",
    "    for fold in range(n_folds):\n",
    "        score_path = save_path / \"oofs\" / \"flaml\" / f\"score_fold_{fold}.csv\"\n",
    "        if score_path.exists():\n",
    "            score_dfs.append(pd.read_csv(score_path))\n",
    "    \n",
    "    if len(score_dfs) == n_folds:\n",
    "        combined_scores = pd.concat(score_dfs, ignore_index=True)\n",
    "        mean_score = combined_scores[\"roc_auc_score\"].mean()\n",
    "        combined_scores.to_csv(save_path / \"oofs\" / \"flaml\" / \"combined_scores.csv\", index=False)\n",
    "        print(f\"\\nMean ROC AUC across all folds: {mean_score:.6f}\")\n",
    "        print(combined_scores)\n",
    "    else:\n",
    "        print(f\"Missing folds. Found {len(score_dfs)} of {n_folds} folds.\")\n",
    "\n",
    "# Uncomment to run after all folds are complete:\n",
    "# combine_all_folds(save_path, N_FOLDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d4b515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print best model info from FLAML\n",
    "# print(f\"\\n{'='*60}\")\n",
    "# print(\"FLAML BEST MODEL INFO\")\n",
    "# print(f\"{'='*60}\")\n",
    "# print(f\"Best estimator: {pipe.model.model.best_estimator}\")\n",
    "# print(f\"Best config: {pipe.model.model.best_config}\")\n",
    "# print(f\"Best validation score: {pipe.model.model.best_loss}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
