{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d186e38e",
   "metadata": {},
   "source": [
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374b7d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if not already available\n",
    "!pip install optuna lightgbm xgboost catboost scikit-learn pandas numpy pyyaml -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d01f1e",
   "metadata": {},
   "source": [
    "## Import Standard Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd6b1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import catboost as cat\n",
    "from catboost import Pool\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler\n",
    "from typing import List, Optional, Tuple, Callable, Union, Any, Dict\n",
    "from dataclasses import dataclass\n",
    "from abc import ABC, abstractmethod\n",
    "from copy import deepcopy\n",
    "from enum import Enum\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e48043",
   "metadata": {},
   "source": [
    "## Configuration Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f23d466",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DatasetConfig:\n",
    "    \"\"\"Configuration for dataset column information\"\"\"\n",
    "    training_col_names: List[str]\n",
    "    target_col_name: str\n",
    "    numerical_col_names: List[str]\n",
    "    categorical_col_names: List[str]\n",
    "    name: Optional[str] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca465228",
   "metadata": {},
   "source": [
    "## Base Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266d9ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseKtoolsModel(ABC):\n",
    "    \"\"\"Base class for all models\"\"\"\n",
    "    def __init__(self) -> None:\n",
    "        self._fitted = False\n",
    "        self.model = None\n",
    "\n",
    "    @abstractmethod\n",
    "    def fit(\n",
    "        self,\n",
    "        X,\n",
    "        y,\n",
    "        validation_set: Optional[Tuple] = None,\n",
    "        weights: Optional[np.ndarray] = None,\n",
    "    ) -> \"BaseKtoolsModel\":\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict(self, X) -> np.ndarray:\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    def fitted(self) -> bool:\n",
    "        return self._fitted\n",
    "\n",
    "\n",
    "class BasePreprocessor(ABC):\n",
    "    \"\"\"Base class for all preprocessors\"\"\"\n",
    "    name = \"base-preprocessor\"\n",
    "\n",
    "    def __init__(self, config: DatasetConfig):\n",
    "        self._fitted = False\n",
    "        self.config = config\n",
    "\n",
    "    @abstractmethod\n",
    "    def fit(self, data: pd.DataFrame) -> \"BasePreprocessor\":\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def transform(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        pass\n",
    "\n",
    "    def fit_transform(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        return self.fit(data).transform(data)\n",
    "\n",
    "    @property\n",
    "    def fitted(self) -> bool:\n",
    "        return self._fitted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b27a810",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0309c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_task(y: Union[np.ndarray, pd.Series]) -> str:\n",
    "    \"\"\"Infer the task type from target values\"\"\"\n",
    "    if isinstance(y, pd.Series):\n",
    "        y = y.to_numpy()\n",
    "    y = y.flatten()\n",
    "\n",
    "    nuniques = np.unique(y).shape[0]\n",
    "    has_floats = np.any(y % 1 != 0)\n",
    "\n",
    "    if has_floats:\n",
    "        print(\"Target contains float values. Inferring regression task.\")\n",
    "        return \"regression\"\n",
    "    elif nuniques == 2:\n",
    "        print(\"Target contains two unique values. Inferring binary classification task.\")\n",
    "        return \"binary_classification\"\n",
    "    elif nuniques > 2:\n",
    "        print(\"Target contains more than two unique values. Inferring multiclass classification task.\")\n",
    "        return \"multiclass_classification\"\n",
    "\n",
    "    raise ValueError(\n",
    "        \"Unable to infer task type from target values. Is there only one target value?\"\n",
    "    )\n",
    "\n",
    "\n",
    "def load_optuna_grid(path: str, model_type: str) -> Callable:\n",
    "    \"\"\"Load parameter grid from YAML file and create getter function\"\"\"\n",
    "    with open(path, \"r\") as f:\n",
    "        param_grid_all = yaml.safe_load(f)\n",
    "    param_grid = param_grid_all.get(model_type, {})\n",
    "    if len(param_grid) == 0:\n",
    "        raise ValueError(f\"No parameter grid found for model type: {model_type}\")\n",
    "\n",
    "    def param_grid_getter(trial: optuna.Trial) -> Dict:\n",
    "        unpacked = {}\n",
    "        for param_name, param_info in param_grid.items():\n",
    "            dtype = param_info.get(\"type\")\n",
    "            if dtype == \"int\":\n",
    "                unpacked[param_name] = trial.suggest_int(\n",
    "                    param_name,\n",
    "                    param_info[\"low\"],\n",
    "                    param_info[\"high\"],\n",
    "                )\n",
    "            elif dtype == \"float\":\n",
    "                unpacked[param_name] = trial.suggest_float(\n",
    "                    param_name,\n",
    "                    param_info[\"low\"],\n",
    "                    param_info[\"high\"],\n",
    "                    log=param_info.get(\"log\", False),\n",
    "                )\n",
    "            elif dtype == \"categorical\":\n",
    "                unpacked[param_name] = trial.suggest_categorical(\n",
    "                    param_name,\n",
    "                    param_info[\"choices\"],\n",
    "                )\n",
    "            elif dtype == \"fixed\":\n",
    "                unpacked[param_name] = param_info[\"value\"]\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported parameter type: {dtype}\")\n",
    "        return unpacked\n",
    "    \n",
    "    return param_grid_getter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f2aa23",
   "metadata": {},
   "source": [
    "## Preprocessing Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6e8270",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategoricalEncoder(BasePreprocessor):\n",
    "    \"\"\"Encoder for categorical features\"\"\"\n",
    "    name = \"categorical-encoder\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: DatasetConfig,\n",
    "        handle_unknown: str = \"use_encoded_value\",\n",
    "        unknown_value: int = -2,\n",
    "        encoded_missing_value: int = -1,\n",
    "        **encoder_kwargs,\n",
    "    ) -> None:\n",
    "        super().__init__(config)\n",
    "        self.encode_missing_value = encoded_missing_value\n",
    "        self.encoder = OrdinalEncoder(\n",
    "            handle_unknown=handle_unknown,\n",
    "            unknown_value=unknown_value,\n",
    "            encoded_missing_value=encoded_missing_value,\n",
    "            **encoder_kwargs,\n",
    "        )\n",
    "\n",
    "    def fit(self, data: pd.DataFrame) -> \"CategoricalEncoder\":\n",
    "        self.encoder.fit(data[self.config.categorical_col_names])\n",
    "        self._fitted = True\n",
    "        return self\n",
    "\n",
    "    def transform(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        copy = data.copy()\n",
    "        mask = copy[self.config.categorical_col_names].isna()\n",
    "        copy[self.config.categorical_col_names] = self.encoder.transform(\n",
    "            copy[self.config.categorical_col_names]\n",
    "        ).astype(int)\n",
    "        copy[self.config.categorical_col_names] = (\n",
    "            copy[self.config.categorical_col_names]\n",
    "            .where(~mask, self.encode_missing_value)\n",
    "            .astype(\"category\")\n",
    "        )\n",
    "        return copy\n",
    "\n",
    "\n",
    "class StandardScale(BasePreprocessor):\n",
    "    \"\"\"Standard scaler for numerical features\"\"\"\n",
    "    name = \"standard-scaler\"\n",
    "\n",
    "    def __init__(self, config: DatasetConfig) -> None:\n",
    "        super().__init__(config)\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def fit(self, data: pd.DataFrame) -> \"StandardScale\":\n",
    "        self.scaler.fit(data[self.config.numerical_col_names])\n",
    "        self._fitted = True\n",
    "        return self\n",
    "\n",
    "    def transform(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        copy = data.copy()\n",
    "        copy[self.config.numerical_col_names] = self.scaler.transform(\n",
    "            copy[self.config.numerical_col_names]\n",
    "        )\n",
    "        return copy\n",
    "\n",
    "\n",
    "class PreprocessingPipeline:\n",
    "    \"\"\"Pipeline for preprocessing steps\"\"\"\n",
    "    def __init__(self, preprocessors: List[BasePreprocessor]) -> None:\n",
    "        self.preprocessors = preprocessors\n",
    "\n",
    "    def train_pipe(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        for preprocessor in self.preprocessors:\n",
    "            data = preprocessor.fit_transform(data)\n",
    "        return data\n",
    "\n",
    "    def inference_pipe(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        for preprocessor in self.preprocessors:\n",
    "            data = preprocessor.transform(data)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6a6aba",
   "metadata": {},
   "source": [
    "## Model Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033ce8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DefaultObjective(Enum):\n",
    "    \"\"\"Default objectives for different tasks\"\"\"\n",
    "    regression = \"regression\"\n",
    "    binary_classification = \"binary\"\n",
    "    multiclass_classification = \"multiclass\"\n",
    "\n",
    "\n",
    "class LGBMModel(BaseKtoolsModel):\n",
    "    \"\"\"LightGBM model wrapper\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_boost_round: int = 100,\n",
    "        early_stopping_rounds: Union[int, None] = 20,\n",
    "        random_state: int = 129,\n",
    "        verbose: int = -1,\n",
    "        n_jobs: int = 1,\n",
    "        callbacks: List[Any] = [],\n",
    "        **lgb_param_grid,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self._num_boost_round = num_boost_round\n",
    "        self._verbose = verbose\n",
    "        self._n_jobs = n_jobs\n",
    "        self._callbacks = callbacks\n",
    "        self.early_stopping_rounds = early_stopping_rounds\n",
    "\n",
    "        self._lgb_param_grid = {\n",
    "            \"verbose\": verbose,\n",
    "            \"random_state\": random_state,\n",
    "            \"n_jobs\": n_jobs,\n",
    "            **lgb_param_grid,\n",
    "        }\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        X,\n",
    "        y,\n",
    "        validation_set: Optional[Tuple] = None,\n",
    "        weights: Optional[np.ndarray] = None,\n",
    "    ) -> \"LGBMModel\":\n",
    "        if \"objective\" not in self._lgb_param_grid:\n",
    "            task_id = infer_task(y)\n",
    "            self._lgb_param_grid[\"objective\"] = DefaultObjective[task_id].value\n",
    "            if task_id == \"multiclass_classification\":\n",
    "                self._lgb_param_grid[\"num_class\"] = np.unique(y).shape[0]\n",
    "\n",
    "        train_data = lgb.Dataset(X, label=y, weight=weights)\n",
    "        eval_sets = [train_data]\n",
    "        eval_names = [\"train\"]\n",
    "        if validation_set is not None:\n",
    "            X_val, y_val = validation_set\n",
    "            val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n",
    "            eval_sets += [val_data]\n",
    "            eval_names += [\"valid\"]\n",
    "            self._lgb_param_grid[\"early_stopping_rounds\"] = self.early_stopping_rounds\n",
    "\n",
    "        train_params = {\n",
    "            \"params\": self._lgb_param_grid,\n",
    "            \"train_set\": train_data,\n",
    "            \"num_boost_round\": self._num_boost_round,\n",
    "            \"valid_sets\": eval_sets,\n",
    "            \"valid_names\": eval_names,\n",
    "            \"callbacks\": self._callbacks,\n",
    "        }\n",
    "\n",
    "        self.model = lgb.train(**train_params)\n",
    "        self._fitted = True\n",
    "        return self\n",
    "\n",
    "    def predict(self, X) -> np.ndarray:\n",
    "        y_pred = self.model.predict(X)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b655e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XGBoostModel(BaseKtoolsModel):\n",
    "    \"\"\"XGBoost model wrapper\"\"\"\n",
    "    class DefaultObjective(Enum):\n",
    "        regression = \"reg:squarederror\"\n",
    "        binary_classification = \"binary:logistic\"\n",
    "        multiclass_classification = \"multi:softprob\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        eval_verbosity: bool = False,\n",
    "        num_boost_round: int = 100,\n",
    "        early_stopping_rounds: Union[int, None] = 20,\n",
    "        random_state: int = 129,\n",
    "        verbosity: int = 0,\n",
    "        n_jobs: int = 1,\n",
    "        **xgb_param_grid,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self._eval_verbosity = eval_verbosity\n",
    "        self._num_boost_round = num_boost_round\n",
    "        self._verbosity = verbosity\n",
    "        self._n_jobs = n_jobs\n",
    "        self._early_stopping_rounds = early_stopping_rounds\n",
    "\n",
    "        self._xgb_param_grid = {\n",
    "            \"verbosity\": verbosity,\n",
    "            \"random_state\": random_state,\n",
    "            \"n_jobs\": n_jobs,\n",
    "            **xgb_param_grid,\n",
    "        }\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        X,\n",
    "        y,\n",
    "        validation_set: Optional[Tuple] = None,\n",
    "        weights: Optional[np.ndarray] = None,\n",
    "    ) -> \"XGBoostModel\":\n",
    "        train_params = {}\n",
    "        if \"objective\" not in self._xgb_param_grid:\n",
    "            task_id = infer_task(y)\n",
    "            self._xgb_param_grid[\"objective\"] = self.DefaultObjective[task_id].value\n",
    "            if task_id == \"multiclass_classification\":\n",
    "                self._xgb_param_grid[\"num_class\"] = np.unique(y).shape[0]\n",
    "\n",
    "        train_data = xgb.DMatrix(X, label=y, enable_categorical=True, weight=weights)\n",
    "        eval_data = [(train_data, \"train\")]\n",
    "        if validation_set is not None:\n",
    "            X_val, y_val = validation_set\n",
    "            valid_data = xgb.DMatrix(X_val, label=y_val, enable_categorical=True)\n",
    "            eval_data += [(valid_data, \"eval\")]\n",
    "            train_params[\"early_stopping_rounds\"] = self._early_stopping_rounds\n",
    "\n",
    "        train_params = {\n",
    "            \"params\": self._xgb_param_grid,\n",
    "            \"dtrain\": train_data,\n",
    "            \"evals\": eval_data,\n",
    "            \"num_boost_round\": self._num_boost_round,\n",
    "            \"verbose_eval\": self._eval_verbosity,\n",
    "            **train_params,\n",
    "        }\n",
    "\n",
    "        self.model = xgb.train(**train_params)\n",
    "        self._fitted = True\n",
    "        return self\n",
    "\n",
    "    def predict(self, X) -> np.ndarray:\n",
    "        test_data = xgb.DMatrix(X, enable_categorical=True)\n",
    "        y_pred = self.model.predict(test_data)\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "class CatBoostModel(BaseKtoolsModel):\n",
    "    \"\"\"CatBoost model wrapper\"\"\"\n",
    "    class DefaultObjective(Enum):\n",
    "        regression = \"RMSE\"\n",
    "        binary_classification = \"Logloss\"\n",
    "        multiclass_classification = \"MultiClass\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_boost_round: int = 100,\n",
    "        early_stopping_rounds: Optional[int] = 20,\n",
    "        random_state: int = 129,\n",
    "        verbose: bool = False,\n",
    "        allow_writing_files: bool = False,\n",
    "        **catboost_params,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.model: Union[cat.CatBoost, None] = None\n",
    "        self._task: bool = False\n",
    "        self._num_boost_round = num_boost_round\n",
    "        self._verbose = verbose\n",
    "        self._allow_writing_files = allow_writing_files\n",
    "        self._early_stopping_rounds = early_stopping_rounds\n",
    "\n",
    "        self._catboost_params = {\n",
    "            \"random_seed\": random_state,\n",
    "            \"verbose\": verbose,\n",
    "            \"allow_writing_files\": allow_writing_files,\n",
    "            **catboost_params,\n",
    "        }\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        X,\n",
    "        y,\n",
    "        validation_set: Optional[Tuple] = None,\n",
    "        weights: Optional[np.ndarray] = None,\n",
    "    ) -> \"CatBoostModel\":\n",
    "        task_id = infer_task(y)\n",
    "        self._task = task_id\n",
    "        if \"loss_function\" not in self._catboost_params:\n",
    "            self._catboost_params[\"loss_function\"] = self.DefaultObjective[task_id].value\n",
    "\n",
    "        self.cat_col_names = (\n",
    "            [col for col in X.columns if X[col].dtype == \"category\"]\n",
    "            if isinstance(X, pd.DataFrame)\n",
    "            else []\n",
    "        )\n",
    "        train_params: Dict[Any, Any] = {\"eval_set\": None}\n",
    "        train_pool = Pool(\n",
    "            data=X, label=y, cat_features=self.cat_col_names, weight=weights\n",
    "        )\n",
    "        if validation_set is not None:\n",
    "            X_val, y_val = validation_set\n",
    "            train_params[\"eval_set\"] = Pool(\n",
    "                data=X_val, label=y_val, cat_features=self.cat_col_names\n",
    "            )\n",
    "            train_params[\"early_stopping_rounds\"] = self._early_stopping_rounds\n",
    "\n",
    "        train_params = {\n",
    "            \"params\": self._catboost_params,\n",
    "            \"dtrain\": train_pool,\n",
    "            \"num_boost_round\": self._num_boost_round,\n",
    "            **train_params,\n",
    "        }\n",
    "        self.model = cat.train(**train_params)\n",
    "        self._fitted = True\n",
    "        return self\n",
    "\n",
    "    def predict(self, X) -> np.ndarray:\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model is not fitted yet. Please call 'fit' first.\")\n",
    "        test_pool = Pool(data=X, cat_features=self.cat_col_names)\n",
    "        if self._task == \"binary_classification\":\n",
    "            y_pred = self.model.predict(test_pool, prediction_type=\"Probability\")[:, 1]\n",
    "        elif self._task == \"multiclass_classification\":\n",
    "            y_pred = self.model.predict(test_pool, prediction_type=\"Probability\")\n",
    "        else:\n",
    "            y_pred = self.model.predict(test_pool)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920f4d1c",
   "metadata": {},
   "source": [
    "## Model Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9e37a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelPipeline:\n",
    "    \"\"\"Pipeline for model training with preprocessing\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: BaseKtoolsModel,\n",
    "        config: DatasetConfig,\n",
    "        preprocessor: PreprocessingPipeline = PreprocessingPipeline([]),\n",
    "    ) -> None:\n",
    "        self.model = model\n",
    "        self.config = config\n",
    "        self.preprocessor = preprocessor\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        train_data: pd.DataFrame,\n",
    "        validation_data: Optional[pd.DataFrame] = None,\n",
    "        weights: Optional[Union[pd.Series, np.ndarray]] = None,\n",
    "    ) -> \"ModelPipeline\":\n",
    "        train_data = self.preprocessor.train_pipe(train_data)\n",
    "        X_train = train_data.drop(columns=[self.config.target_col_name])\n",
    "        y_train = train_data[self.config.target_col_name]\n",
    "\n",
    "        if validation_data is not None:\n",
    "            validation_data = self.preprocessor.inference_pipe(validation_data)\n",
    "            X_valid = validation_data.drop(columns=[self.config.target_col_name])\n",
    "            y_valid = validation_data[self.config.target_col_name]\n",
    "            validation_data = (X_valid, y_valid)\n",
    "\n",
    "        self.model.fit(\n",
    "            X=X_train, y=y_train, validation_set=validation_data, weights=weights\n",
    "        )\n",
    "        return self\n",
    "\n",
    "    def predict(self, data: pd.DataFrame) -> np.ndarray:\n",
    "        data = self.preprocessor.inference_pipe(data)\n",
    "        X_test = data[self.config.training_col_names]\n",
    "        return self.model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f658b294",
   "metadata": {},
   "source": [
    "## Cross-Validation Executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff69f6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossValidationExecutor:\n",
    "    \"\"\"Execute cross-validation with model pipeline\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: DatasetConfig,\n",
    "        model_pipeline: ModelPipeline,\n",
    "        evaluation_metric: Callable,\n",
    "        kfold_object,\n",
    "    ) -> None:\n",
    "        self.config = config\n",
    "        self.model_pipeline = model_pipeline\n",
    "        self._evaluation_metric = evaluation_metric\n",
    "        self._splitter = kfold_object\n",
    "        self._num_splits = kfold_object.get_n_splits()\n",
    "\n",
    "    def run(\n",
    "        self,\n",
    "        train_data: pd.DataFrame,\n",
    "        weights: Optional[np.ndarray] = None,\n",
    "        val_data: Optional[pd.DataFrame] = None,\n",
    "        test_data: Optional[pd.DataFrame] = None,\n",
    "        groups=None,\n",
    "        additional_data: Optional[pd.DataFrame] = None,\n",
    "    ) -> Tuple[float, np.ndarray, List[ModelPipeline], np.ndarray]:\n",
    "        train_oof_preds = np.empty(train_data.shape[0])\n",
    "        test_oof_preds = np.zeros(test_data.shape[0])\n",
    "\n",
    "        mean_score: int = 0\n",
    "        pipelist: List[ModelPipeline] = []\n",
    "        for train_index, val_index in self._splitter.split(\n",
    "            train_data, train_data[self.config.target_col_name]\n",
    "        ):\n",
    "            train_fold = train_data.iloc[train_index]\n",
    "            val_fold = train_data.iloc[val_index]\n",
    "\n",
    "            pipe = deepcopy(self.model_pipeline)\n",
    "            all_training_data = (\n",
    "                pd.concat([train_fold, additional_data])\n",
    "                if additional_data is not None\n",
    "                else train_fold\n",
    "            )\n",
    "            validation_data = val_fold if val_data is None else val_data\n",
    "            pipe.fit(\n",
    "                all_training_data, validation_data=validation_data, weights=weights\n",
    "            )\n",
    "            pipelist.append(pipe)\n",
    "\n",
    "            y_pred = pipe.predict(val_fold)\n",
    "            test_pred = pipe.predict(test_data)\n",
    "\n",
    "            score = self._evaluation_metric(\n",
    "                val_fold[self.config.target_col_name], y_pred\n",
    "            )\n",
    "            train_oof_preds[val_index] = y_pred\n",
    "            test_oof_preds += test_pred / self._num_splits\n",
    "\n",
    "            mean_score += score / self._num_splits\n",
    "\n",
    "        return mean_score, train_oof_preds, pipelist, test_oof_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9236cef3",
   "metadata": {},
   "source": [
    "## Optuna Hyperparameter Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e889c2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptunaHyperparameterOptimizer:\n",
    "    \"\"\"Hyperparameter optimization using Optuna\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        grid_yaml_path: str,\n",
    "        model_type: str,\n",
    "        config: DatasetConfig,\n",
    "        evaluation_metric: Callable,\n",
    "        kfold_object,\n",
    "        preprocessor: PreprocessingPipeline,\n",
    "        timeout: int = 3600,\n",
    "        direction: str = \"maximize\",\n",
    "        n_trials: int = 100,\n",
    "        study_name: str = \"ml_experiment\",\n",
    "        explore_fraction: float = 0.1,\n",
    "        save_study: bool = False,\n",
    "        verbose=False,\n",
    "        random_state=42,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self._param_grid_getter = load_optuna_grid(grid_yaml_path, model_type)\n",
    "        self.config = config\n",
    "        self._evaluation_metric = evaluation_metric\n",
    "        self._kfold_object = kfold_object\n",
    "        self._preprocessor = preprocessor\n",
    "        self._timeout = timeout\n",
    "        self._direction = direction\n",
    "        self._n_trials = n_trials\n",
    "        self._study_name = study_name\n",
    "        self._explore_fraction = explore_fraction\n",
    "        self._save_study = save_study\n",
    "        self._verbose = verbose\n",
    "        self._random_state = random_state\n",
    "\n",
    "    def optimize(\n",
    "        self,\n",
    "        *cv_args,\n",
    "        **cv_kwargs,\n",
    "    ):\n",
    "        if self._verbose:\n",
    "            print(\"#\" * 100)\n",
    "            print(\"Starting Optuna Optimizer\")\n",
    "            print(\"#\" * 100)\n",
    "\n",
    "        sampler = TPESampler(\n",
    "            n_startup_trials=int(self._n_trials * self._explore_fraction),\n",
    "            seed=self._random_state,\n",
    "        )\n",
    "\n",
    "        storage_name = (\n",
    "            \"sqlite:///{}.db\".format(self._study_name) if self._save_study else None\n",
    "        )\n",
    "        self.study = study = optuna.create_study(\n",
    "            sampler=sampler,\n",
    "            study_name=self._study_name,\n",
    "            direction=self._direction,\n",
    "            storage=storage_name,\n",
    "            load_if_exists=True,\n",
    "        )\n",
    "\n",
    "        def objective(trial: optuna.Trial):\n",
    "            parameters = self._param_grid_getter(trial)\n",
    "            model = self.model(**parameters)\n",
    "\n",
    "            cv_executor = CrossValidationExecutor(\n",
    "                config=self.config,\n",
    "                model_pipeline=ModelPipeline(\n",
    "                    model=model,\n",
    "                    config=self.config,\n",
    "                    preprocessor=self._preprocessor,\n",
    "                ),\n",
    "                evaluation_metric=self._evaluation_metric,\n",
    "                kfold_object=self._kfold_object,\n",
    "            )\n",
    "            score, _, _, _ = cv_executor.run(\n",
    "                *cv_args,\n",
    "                **cv_kwargs,\n",
    "            )\n",
    "            return score\n",
    "\n",
    "        study.optimize(\n",
    "            objective,\n",
    "            n_trials=self._n_trials,\n",
    "            timeout=self._timeout,\n",
    "        )\n",
    "        optimal_params = study.best_params\n",
    "        return optimal_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a49da9",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "Load your training and test data. Adjust the paths according to your Kaggle competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307fb3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update these paths to match your Kaggle dataset location\n",
    "DATA_PATH = \"/kaggle/input/your-competition-name/\"\n",
    "TARGET = \"target_column_name\"  # Update with your target column name\n",
    "\n",
    "# Load data\n",
    "train_data = pd.read_csv(DATA_PATH + \"train.csv\", index_col=0)\n",
    "test_data = pd.read_csv(DATA_PATH + \"test.csv\", index_col=0)\n",
    "\n",
    "print(f\"Train shape: {train_data.shape}\")\n",
    "print(f\"Test shape: {test_data.shape}\")\n",
    "print(f\"\\nTrain columns: {train_data.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a42fb0d",
   "metadata": {},
   "source": [
    "## Configure Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750b758f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify column types\n",
    "training_col_names = train_data.drop(columns=TARGET).columns.tolist()\n",
    "numerical_col_names = (\n",
    "    train_data.drop(columns=TARGET)\n",
    "    .select_dtypes(include=[\"number\"])\n",
    "    .columns.tolist()\n",
    ")\n",
    "categorical_col_names = train_data.select_dtypes(\n",
    "    include=[\"object\"]\n",
    ").columns.tolist()\n",
    "\n",
    "# Create dataset configuration\n",
    "config = DatasetConfig(\n",
    "    training_col_names=training_col_names,\n",
    "    numerical_col_names=numerical_col_names,\n",
    "    categorical_col_names=categorical_col_names,\n",
    "    target_col_name=TARGET,\n",
    ")\n",
    "\n",
    "print(f\"Numerical features ({len(numerical_col_names)}): {numerical_col_names}\")\n",
    "print(f\"\\nCategorical features ({len(categorical_col_names)}): {categorical_col_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6c13fd",
   "metadata": {},
   "source": [
    "## Setup Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24ed588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create preprocessors\n",
    "preprocessors = [StandardScale(config), CategoricalEncoder(config)]\n",
    "preprocessor = PreprocessingPipeline(preprocessors=preprocessors)\n",
    "\n",
    "print(\"Preprocessing pipeline created with:\")\n",
    "for p in preprocessors:\n",
    "    print(f\"  - {p.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c813697a",
   "metadata": {},
   "source": [
    "## Load Parameter Grid from YAML\n",
    "\n",
    "Upload your parameter grid YAML file to Kaggle and specify the path. The YAML file should follow this format:\n",
    "\n",
    "```yaml\n",
    "base:  # or 'model_type' you specify\n",
    "  num_leaves:\n",
    "    type: int\n",
    "    low: 20\n",
    "    high: 150\n",
    "  max_depth:\n",
    "    type: int\n",
    "    low: 3\n",
    "    high: 12\n",
    "  learning_rate:\n",
    "    type: float\n",
    "    low: 0.01\n",
    "    high: 0.3\n",
    "    log: true\n",
    "  # ... more parameters\n",
    "```\n",
    "\n",
    "Supported types: `int`, `float`, `categorical`, `fixed`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3810532e",
   "metadata": {},
   "source": [
    "## Run Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f339bc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to your parameter grid YAML file\n",
    "GRID_YAML_PATH = \"/kaggle/input/your-dataset/param_grid.yml\"\n",
    "MODEL_TYPE = \"base\"  # The key in the YAML file (e.g., \"base\", \"lightgbm\", \"xgboost\", \"catboost\")\n",
    "MODEL_CLASS = LGBMModel  # Change to XGBoostModel or CatBoostModel as needed\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = OptunaHyperparameterOptimizer(\n",
    "    model=MODEL_CLASS,\n",
    "    grid_yaml_path=GRID_YAML_PATH,\n",
    "    model_type=MODEL_TYPE,\n",
    "    config=config,\n",
    "    evaluation_metric=roc_auc_score,  # Update with your metric\n",
    "    kfold_object=StratifiedKFold(n_splits=3, shuffle=True, random_state=42),\n",
    "    preprocessor=preprocessor,\n",
    "    timeout=3600,  # 3600 seconds = 1 hour, adjust as needed\n",
    "    direction=\"maximize\",  # Use \"minimize\" for loss metrics\n",
    "    n_trials=100,  # Increase for better optimization (e.g., 100+)\n",
    "    study_name=\"kaggle_optuna_optimizer\",\n",
    "    explore_fraction=0.1,\n",
    "    save_study=False,\n",
    "    verbose=True,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "print(\"Starting hyperparameter optimization...\")\n",
    "print(f\"Model: {MODEL_CLASS.__name__}\")\n",
    "print(f\"This will run for max {optimizer._timeout} seconds or {optimizer._n_trials} trials\")\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355b7633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run optimization\n",
    "optimal_params = optimizer.optimize(\n",
    "    train_data=train_data,\n",
    "    test_data=test_data,\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"OPTIMIZATION COMPLETE!\")\n",
    "print(\"=\"*100)\n",
    "print(f\"\\nBest Score: {optimizer.study.best_value:.6f}\")\n",
    "print(f\"\\nBest Parameters:\")\n",
    "for key, value in optimal_params.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83330bd",
   "metadata": {},
   "source": [
    "## Visualization (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed82f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize optimization history\n",
    "from optuna.visualization import plot_optimization_history, plot_param_importances\n",
    "\n",
    "try:\n",
    "    # Plot optimization history\n",
    "    fig1 = plot_optimization_history(optimizer.study)\n",
    "    fig1.show()\n",
    "    \n",
    "    # Plot parameter importances\n",
    "    fig2 = plot_param_importances(optimizer.study)\n",
    "    fig2.show()\n",
    "except Exception as e:\n",
    "    print(f\"Visualization error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2d931c",
   "metadata": {},
   "source": [
    "## Train Final Model with Best Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8583b58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create final model with best parameters\n",
    "# final_model = LGBMModel(**optimal_params)\n",
    "\n",
    "# # Create final pipeline\n",
    "# final_pipeline = ModelPipeline(\n",
    "#     model=final_model,\n",
    "#     config=config,\n",
    "#     preprocessor=PreprocessingPipeline([StandardScale(config), CategoricalEncoder(config)])\n",
    "# )\n",
    "\n",
    "# # Train on full training data\n",
    "# print(\"Training final model on full training data...\")\n",
    "# final_pipeline.fit(train_data)\n",
    "\n",
    "# # Generate predictions\n",
    "# print(\"Generating predictions...\")\n",
    "# predictions = final_pipeline.predict(test_data)\n",
    "\n",
    "# print(f\"Predictions shape: {predictions.shape}\")\n",
    "# print(f\"Sample predictions: {predictions[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51df88e9",
   "metadata": {},
   "source": [
    "## Create Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405b3ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create submission dataframe\n",
    "# submission = pd.DataFrame({\n",
    "#     'id': test_data.index,  # Update 'id' to match your competition's submission format\n",
    "#     TARGET: predictions\n",
    "# })\n",
    "\n",
    "# # Save submission\n",
    "# submission.to_csv('submission.csv', index=False)\n",
    "# print(\"Submission file created: submission.csv\")\n",
    "# print(f\"\\nSubmission preview:\")\n",
    "# print(submission.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eafb1aa",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook provides a complete workflow for:\n",
    "1. Loading and configuring your dataset\n",
    "2. Setting up preprocessing pipelines\n",
    "3. Running Optuna hyperparameter optimization\n",
    "4. Training a final model with optimal parameters\n",
    "5. Generating predictions and creating a submission file\n",
    "\n",
    "All dependencies are embedded in the notebook, making it suitable for Kaggle's isolated environment.\n",
    "\n",
    "**Supported Models:**\n",
    "- LGBMModel (LightGBM)\n",
    "- XGBoostModel (XGBoost)\n",
    "- CatBoostModel (CatBoost)\n",
    "\n",
    "**To use this notebook:**\n",
    "1. Upload your parameter grid YAML file to Kaggle as a dataset\n",
    "2. Update the `DATA_PATH`, `TARGET`, `GRID_YAML_PATH`, and `MODEL_TYPE` variables\n",
    "3. Choose your model by setting `MODEL_CLASS` (LGBMModel, XGBoostModel, or CatBoostModel)\n",
    "4. Modify the evaluation metric if needed\n",
    "5. Increase `n_trials` and `timeout` for better optimization\n",
    "6. Run all cells sequentially\n",
    "\n",
    "**Parameter Grid YAML Format:**\n",
    "```yaml\n",
    "base:  # or your model_type\n",
    "  param_name:\n",
    "    type: int  # or float, categorical, fixed\n",
    "    low: 1\n",
    "    high: 10\n",
    "    # For float, add: log: true (optional)\n",
    "    # For categorical, use: choices: [val1, val2, ...]\n",
    "    # For fixed, use: value: some_value\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
