{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import torch\n","from sklearn.impute import SimpleImputer\n","from sklearn.preprocessing import LabelEncoder, StandardScaler\n","from torch.utils.data import TensorDataset\n","import json\n","import pytorch_lightning as pl\n","import numpy as np, pandas as pd\n","import matplotlib.pyplot as plt\n","import torch\n","from pytorch_lightning.callbacks import LearningRateMonitor, TQDMProgressBar, EarlyStopping\n","from pytorch_lightning.callbacks import StochasticWeightAveraging\n","from sklearn.model_selection import StratifiedKFold\n","import random\n","import sys\n","sys.path.append('..')\n","\n","from functools import reduce\n","from ktools.preprocessing.basic_feature_transformers import *\n","from ktools.utils.data_science_pipeline_settings import DataSciencePipelineSettings"]},{"cell_type":"markdown","metadata":{},"source":["## Prepare data\n","\n","Below are a few utility functions to load and prepare the data for training with pytorch."]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["RANDOM_SEED = 32\n","ORIGINAL_DATA=False"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2025-01-28T15:57:44.925672Z","iopub.status.busy":"2025-01-28T15:57:44.925334Z","iopub.status.idle":"2025-01-28T15:57:49.117879Z","shell.execute_reply":"2025-01-28T15:57:49.116984Z","shell.execute_reply.started":"2025-01-28T15:57:44.925635Z"},"trusted":true},"outputs":[],"source":["from ktools.modelling.model_transform_wrappers.survival_model_wrapper import transform_quantile\n","\n","\n","def set_seed(seed=42):\n","    random.seed(seed)  # Python random module\n","    np.random.seed(seed)  # NumPy\n","    torch.manual_seed(seed)  # PyTorch CPU\n","    torch.cuda.manual_seed_all(seed)  # PyTorch GPU (all devices)\n","    torch.backends.cudnn.deterministic = True  # Ensures deterministic behavior\n","    torch.backends.cudnn.benchmark = False  # Disables auto-tuning for convolutions\n","    pl.seed_everything(seed)\n","\n","train_csv_path = \"../data/post_hct_survival/train.csv\"\n","test_csv_path = \"../data/post_hct_survival/test.csv\"\n","sub_csv_path = \"../data/post_hct_survival/sample_submission.csv\"\n","target_col_name = ['efs', 'efs_time']\n","\n","def get_X_cat(df, cat_cols, transformers=None):\n","    \"\"\"\n","    Apply a specific categorical data transformer or a LabelEncoder if None.\n","    \"\"\"\n","    if transformers is None:\n","        transformers = [LabelEncoder().fit(df[col]) for col in cat_cols]\n","    return transformers, np.array(\n","        [transformer.transform(df[col]) for col, transformer in zip(cat_cols, transformers)]\n","    ).T\n","\n","def preprocess_data(train, val):\n","    \"\"\"\n","    Standardize numerical variables and transform (Label-encode) categoricals.\n","    Fill NA values with mean for numerical.\n","    Create torch dataloaders to prepare data for training and evaluation.\n","    \"\"\"\n","    X_cat_train, X_cat_val, numerical, transformers = get_categoricals(train, val)\n","    scaler = StandardScaler()\n","    imp = SimpleImputer(missing_values=np.nan, strategy='mean', add_indicator=True)\n","    X_num_train = imp.fit_transform(train[numerical])\n","    X_num_train = scaler.fit_transform(X_num_train)\n","    X_num_val = imp.transform(val[numerical])\n","    X_num_val = scaler.transform(X_num_val)\n","    dl_train = init_dl(X_cat_train, X_num_train, train, training=True)\n","    dl_val = init_dl(X_cat_val, X_num_val, val)\n","    return X_cat_val, X_num_train, X_num_val, dl_train, dl_val, transformers\n","\n","def _preprocess_data(train, val):\n","    X_cat_train, X_cat_val, numerical, transformers = get_categoricals(train, val)\n","    scaler = StandardScaler()\n","    imp = SimpleImputer(missing_values=np.nan, strategy='mean', add_indicator=True)\n","    X_num_train = imp.fit_transform(train[numerical])\n","    X_num_train = scaler.fit_transform(X_num_train)\n","    X_num_val = imp.transform(val[numerical])\n","    X_num_val = scaler.transform(X_num_val)\n","    return X_cat_train, X_cat_val, X_num_train, X_num_val\n","\n","def init_ktools_dl(X : pd.DataFrame, y : pd.DataFrame, training=False):\n","    \"\"\"\n","    Initialize data loaders with 4 dimensions : categorical dataframe, numerical dataframe and target values (efs and efs_time).\n","    Notice that efs_time is log-transformed.\n","    Fix batch size to 2048 and return dataloader for training or validation depending on training value.\n","    \"\"\"\n","    quant_y = transform_quantile(y['efs_time'], y['efs'])\n","    X_cat = X.select_dtypes('category').values\n","    X_num = X.select_dtypes('number').values\n","    ds_train = TensorDataset(\n","        torch.tensor(X_cat, dtype=torch.long),\n","        torch.tensor(X_num, dtype=torch.float32),\n","        torch.tensor(y.efs_time.values, dtype=torch.float32).log(),\n","        torch.tensor(y.efs.values, dtype=torch.long),\n","        torch.tensor((y.efs_time.values - y.efs_time.values.mean())/y.efs_time.values.std(), dtype=torch.float32),\n","        # torch.tensor(quant_y, dtype=torch.float32),\n","    )\n","    bs = 2048\n","    set_seed(RANDOM_SEED)\n","    dl_train = torch.utils.data.DataLoader(ds_train, batch_size=bs, pin_memory=True, shuffle=training)\n","    return dl_train, X_cat, X_num\n","\n","def get_categoricals(train, val):\n","    \"\"\"\n","    Remove constant categorical columns and transform them using LabelEncoder.\n","    Return the label-transformers for each categorical column, categorical dataframes and numerical columns.\n","    \"\"\"\n","    categorical_cols, numerical = get_feature_types(train)\n","    remove = []\n","    for col in categorical_cols:\n","        if train[col].nunique() == 1:\n","            remove.append(col)\n","        ind = ~val[col].isin(train[col])\n","        if ind.any():\n","            val.loc[ind, col] = np.nan\n","    categorical_cols = [col for col in categorical_cols if col not in remove]\n","    transformers, X_cat_train = get_X_cat(train, categorical_cols)\n","    _, X_cat_val = get_X_cat(val, categorical_cols, transformers)\n","    return X_cat_train, X_cat_val, numerical, transformers\n","\n","\n","def init_dl(X_cat, X_num, df, training=False):\n","    \"\"\"\n","    Initialize data loaders with 4 dimensions : categorical dataframe, numerical dataframe and target values (efs and efs_time).\n","    Notice that efs_time is log-transformed.\n","    Fix batch size to 2048 and return dataloader for training or validation depending on training value.\n","    \"\"\"\n","    ds_train = TensorDataset(\n","        torch.tensor(X_cat, dtype=torch.long),\n","        torch.tensor(X_num, dtype=torch.float32),\n","        torch.tensor(df.efs_time.values, dtype=torch.float32).log(),\n","        torch.tensor(df.efs.values, dtype=torch.long)\n","    )\n","    bs = 2048\n","    set_seed(RANDOM_SEED)\n","    dl_train = torch.utils.data.DataLoader(ds_train, batch_size=bs, pin_memory=True, shuffle=training)\n","    return dl_train\n","\n","\n","def get_feature_types(train):\n","    \"\"\"\n","    Utility function to return categorical and numerical column names.\n","    \"\"\"\n","    categorical_cols = [col for i, col in enumerate(train.columns) if ((train[col].dtype == \"object\") | (2 < train[col].nunique() < 25))]\n","    RMV = [\"ID\", \"efs\", \"efs_time\", \"y\"]\n","    FEATURES = [c for c in train.columns if not c in RMV]\n","    print(f\"There are {len(FEATURES)} FEATURES: {FEATURES}\")\n","    numerical = [i for i in FEATURES if i not in categorical_cols]\n","    return categorical_cols, numerical\n","\n","\n","def add_features(df):\n","    \"\"\"\n","    Create some new features to help the model focus on specific patterns.\n","    \"\"\"\n","    df['is_cyto_score_same'] = (df['cyto_score'] == df['cyto_score_detail']).astype(int)\n","    df['year_hct'] -= 2000\n","    \n","    return df\n","\n","\n","def load_data():\n","    \"\"\"\n","    Load data and add features.\n","    \"\"\"\n","    test = pd.read_csv(test_csv_path)\n","    test = add_features(test)\n","    print(\"Test shape:\", test.shape)\n","    train = pd.read_csv(train_csv_path)\n","    train = add_features(train)\n","    print(\"Train shape:\", train.shape)\n","    return test, train"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["def get_cats():\n","    df = pd.read_csv(train_csv_path)\n","    cats = [col for col in df.columns if (2 < df[col].nunique() < 25) | (df[col].dtype == 'object')]\n","    return cats\n","categoricals = get_cats()"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["from post_HCT_survival_notebooks.hct_utils import score\n","\n","\n","def scci_metric(y_test, y_pred, id_col_name : str = \"ID\",\n","               survived_col_name : str = \"efs\",\n","               survival_time_col_name : str = \"efs_time\",\n","               stratify_col_name : str = \"race_group\"):\n","    idcs = y_test.index\n","    og_train = pd.read_csv(train_csv_path)\n","    \n","    y_true = og_train.loc[idcs, [id_col_name, survived_col_name, survival_time_col_name, stratify_col_name]].copy()\n","    y_pred_df = og_train.loc[idcs, [id_col_name]].copy()\n","    y_pred_df[\"prediction\"] = y_pred\n","    scci = score(y_true.copy(), y_pred_df.copy(), id_col_name)\n","    return scci"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["from sklearn.metrics import accuracy_score\n","from ktools.fitting.cross_validation_executor import CrossValidationExecutor\n","from ktools.modelling.ktools_models.xgb_model import XGBoostModel\n","from ktools.modelling.model_transform_wrappers.survival_model_wrapper import transform_quantile\n","\n","\n","class AddHCTFeatures(IFeatureTransformer):\n","    @staticmethod\n","    def transform(original_settings : DataSciencePipelineSettings):\n","        settings = deepcopy(original_settings)\n","        settings.combined_df['is_cyto_score_same'] = (settings.combined_df['cyto_score'] == settings.combined_df['cyto_score_detail']).astype(int)\n","        settings.combined_df['year_hct'] -= 2000\n","        settings.training_col_names += ['is_cyto_score_same']\n","        return settings\n","\n","class AddOOFFeatures():\n","    @staticmethod\n","    def transform(original_settings : DataSciencePipelineSettings):\n","        settings = deepcopy(original_settings)\n","        kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)\n","        # tuned_params = {\"booster\" : \"gbtree\", \"grow_policy\" : \"lossguide\", 'max_bin': 124, 'learning_rate': 0.023029064087026294, 'max_depth': 5, 'gamma': 0.24355098020060434, 'min_child_weight': 50.3800425529006, 'subsample': 0.8044335464015183, 'colsample_bytree': 0.8495543555662095, 'colsample_bylevel': 0.8507911069372948, 'colsample_bynode': 0.8624484718036373, 'reg_alpha': 0.12756458744896684, 'reg_lambda': 1.8762127198417706, 'max_cat_threshold': 327}\n","        # xgb_regression_base_params = {\"objective\": \"reg:squarederror\", \"eval_metric\": \"rmse\", \"num_boost_round\" : 10000, \"early_stopping_rounds\" : 100}\n","        model = XGBoostModel(**{'objective' : 'binary:logistic', 'eval_metric' : 'logloss', \"num_boost_round\" : 10000, \"early_stopping_rounds\" : 100})\n","\n","        train, test_df = settings.update()\n","        test_df = test_df.drop(columns=target_col_name)\n","        X, y = train.drop(columns=settings.target_col_name), train[settings.target_col_name]\n","        # quant_y = transform_quantile(y['efs_time'], y['efs'])\n","        score_tuple, oofs, model_list, test_preds = CrossValidationExecutor(model,\n","                                                                          accuracy_score,\n","                                                                          kf,\n","                                                                          verbose=2).run(X, y['efs'], test_data=test_df, groups=X['race_group'].values, output_transform_list=[lambda x : (x[-1]>0.5).astype(int)])\n","        settings.combined_df.loc['train', 'xgb_oof'] = oofs\n","        settings.combined_df.loc['test', 'xgb_oof'] = test_preds\n","        return settings\n","\n","\n","settings = DataSciencePipelineSettings(train_csv_path,\n","                                        test_csv_path,\n","                                        target_col_name,\n","                                        categorical_col_names=categoricals\n","                                        )\n","transforms = [\n","            # AddHCTFeatures.transform,\n","            ImputeNumericalAddIndicator.transform,\n","            StandardScaleNumerical.transform,\n","            FillNullValues.transform,\n","            OrdinalEncode.transform,\n","            ConvertObjectToCategorical.transform,\n","            # AddOOFFeatures.transform\n","            ]\n","\n","settings = reduce(lambda acc, func: func(acc), transforms, settings)\n","settings.update()\n","\n","train, test_df = settings.update()\n","test_df.drop(columns=target_col_name, inplace=True)\n","X, y = train.drop(columns=settings.target_col_name), train[settings.target_col_name]"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["# test, train_original = load_data()\n","# test['efs_time'] = 1\n","# test['efs'] = 1\n","# X_cat_train, X_cat_val, X_num_train, X_num_val = _preprocess_data(train_original, test)\n","\n","# assert np.allclose(X.select_dtypes('number').to_numpy(), X_num_train)\n","\n","# def encode_in_order(array):\n","#     d = {}\n","#     idx = 0\n","#     for i, n in enumerate(array):\n","#         if n not in d:\n","#             d[n] = idx\n","#             idx += 1\n","#         array[i] = d[array[i]]\n","#     return array\n","\n","# my_array = X.select_dtypes('category').values\n","\n","# for i in range(X_cat_train.shape[1]):\n","#     inv = encode_in_order(my_array[:, i])\n","#     inv2 = encode_in_order(X_cat_train[:, i])\n","#     assert np.allclose(inv, inv2)\n","\n","# assert np.allclose(test_df.select_dtypes('number').to_numpy(), X_num_val)\n","# my_array = test_df.select_dtypes('category').values\n","\n","# for i in range(X_cat_val.shape[1]):\n","#     inv = encode_in_order(my_array[:, i])\n","#     inv2 = encode_in_order(X_cat_val[:, i])\n","#     assert np.allclose(inv, inv2)"]},{"cell_type":"markdown","metadata":{},"source":["## Define models with pairwise ranking loss\n","\n","The model is defined in 3 steps :\n","* Embedding class for categorical data\n","* MLP for numerical and categorical data\n","* Final model trained with pairwise ranking loss with selection of valid pairs"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["import functools\n","import torch.nn.functional as F\n","\n","@functools.lru_cache\n","def combinations(N):\n","    with torch.no_grad():\n","        ind = torch.arange(N)\n","        comb = torch.combinations(ind, r=2)\n","    return comb\n","\n","def pairwise_loss(race : torch.tensor, event :torch.Tensor, event_time:torch.Tensor, risk:torch.Tensor, margin=0.2, weight_class:bool = False):\n","    n = event.shape[0]\n","    # unq_races, race_counts = torch.unique(race, return_counts=True)\n","    pairwise_combinations = combinations(n)\n","\n","    # Find mask\n","    # first_of_pair, second_of_pair = pairwise_combinations.T\n","    pairwise_combinations = pairwise_combinations.clone().detach()\n","    first_of_pair, second_of_pair = pairwise_combinations[:, 0], pairwise_combinations[:, 1]\n","    valid_mask = False\n","    valid_mask |= ((event[first_of_pair] == 1) & (event[second_of_pair] == 1))\n","    valid_mask |= ((event[first_of_pair] == 1) & (event_time[first_of_pair] < event_time[second_of_pair]))\n","    valid_mask |= ((event[second_of_pair] == 1) & (event_time[second_of_pair] < event_time[first_of_pair]))\n","    # pariwise hinge loss\n","    direction = 2*(event_time[first_of_pair] > event_time[second_of_pair]).int() - 1\n","    margin_loss = F.relu(-direction*(risk[first_of_pair] - risk[second_of_pair]) + margin)\n","\n","    weights = torch.ones_like(margin_loss)\n","    if weight_class:\n","        first_race = race[first_of_pair]\n","        second_race = race[second_of_pair]\n","        same_race_mask = first_race == second_race\n","        weights[same_race_mask] = 1.3\n","\n","    # if weight_class:\n","    #     margin_loss = margin_loss * race_loss_weight\n","    return (margin_loss.double()*weights.double()*valid_mask.double()).sum()/(weights.double()*valid_mask).sum()\n","\n","\n","def race_equality_loss(race, event, event_time, risk, margin=0.2):\n","    unq_races, race_counts = torch.unique(race, return_counts=True)\n","    race_specific_loss = torch.zeros(len(unq_races), dtype=torch.double).to(race.device)\n","    for i, r in enumerate(unq_races):\n","        idcs = race == r\n","        race_specific_loss[i] = pairwise_loss(race, event[idcs], event_time[idcs], risk[idcs], margin=margin, weight_class=False)\n","    return torch.std(race_specific_loss)"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2025-01-28T15:57:49.120081Z","iopub.status.busy":"2025-01-28T15:57:49.119582Z","iopub.status.idle":"2025-01-28T15:57:55.872936Z","shell.execute_reply":"2025-01-28T15:57:55.87208Z","shell.execute_reply.started":"2025-01-28T15:57:49.120047Z"},"trusted":true},"outputs":[],"source":["import functools\n","from typing import List\n","\n","import pytorch_lightning as pl\n","import numpy as np\n","import torch\n","from lifelines.utils import concordance_index\n","from pytorch_lightning.cli import ReduceLROnPlateau\n","from pytorch_tabular.models.common.layers import ODST\n","from torch import nn\n","from pytorch_lightning.utilities import grad_norm\n","\n","\n","class CatEmbeddings(nn.Module):\n","    \"\"\"\n","    Embedding module for the categorical dataframe.\n","    \"\"\"\n","    def __init__(\n","        self,\n","        projection_dim: int,\n","        categorical_cardinality: List[int],\n","        embedding_dim: int\n","    ):\n","        \"\"\"\n","        projection_dim: The dimension of the final output after projecting the concatenated embeddings into a lower-dimensional space.\n","        categorical_cardinality: A list where each element represents the number of unique categories (cardinality) in each categorical feature.\n","        embedding_dim: The size of the embedding space for each categorical feature.\n","        self.embeddings: list of embedding layers for each categorical feature.\n","        self.projection: sequential neural network that goes from the embedding to the output projection dimension with GELU activation.\n","        \"\"\"\n","        super(CatEmbeddings, self).__init__()\n","        self.embeddings = nn.ModuleList([\n","            nn.Embedding(cardinality, embedding_dim)\n","            for cardinality in categorical_cardinality\n","        ])\n","        self.projection = nn.Sequential(\n","            nn.Linear(embedding_dim * len(categorical_cardinality), projection_dim),\n","            nn.GELU(),\n","            nn.Linear(projection_dim, projection_dim)\n","        )\n","\n","    def forward(self, x_cat):\n","        \"\"\"\n","        Apply the projection on concatened embeddings that contains all categorical features.\n","        \"\"\"\n","        x_cat = [embedding(x_cat[:, i]) for i, embedding in enumerate(self.embeddings)]\n","        x_cat = torch.cat(x_cat, dim=1)\n","        return self.projection(x_cat)\n","\n","\n","class NN(nn.Module):\n","    \"\"\"\n","    Train a model on both categorical embeddings and numerical data.\n","    \"\"\"\n","    def __init__(\n","            self,\n","            continuous_dim: int,\n","            categorical_cardinality: List[int],\n","            embedding_dim: int,\n","            projection_dim: int,\n","            hidden_dim: int,\n","            dropout: float = 0\n","    ):\n","        \"\"\"\n","        continuous_dim: The number of continuous features.\n","        categorical_cardinality: A list of integers representing the number of unique categories in each categorical feature.\n","        embedding_dim: The dimensionality of the embedding space for each categorical feature.\n","        projection_dim: The size of the projected output space for the categorical embeddings.\n","        hidden_dim: The number of neurons in the hidden layer of the MLP.\n","        dropout: The dropout rate applied in the network.\n","        self.embeddings: previous embeddings for categorical data.\n","        self.mlp: defines an MLP model with an ODST layer followed by batch normalization and dropout.\n","        self.out: linear output layer that maps the output of the MLP to a single value\n","        self.dropout: defines dropout\n","        Weights initialization with xavier normal algorithm and biases with zeros.\n","        \"\"\"\n","        super(NN, self).__init__()\n","        self.embeddings = CatEmbeddings(projection_dim, categorical_cardinality, embedding_dim)\n","        self.mlp = nn.Sequential(\n","            ODST(projection_dim + continuous_dim, hidden_dim),\n","            nn.BatchNorm1d(hidden_dim),\n","            nn.Dropout(dropout)\n","        )\n","        self.out = nn.Linear(hidden_dim, 1)\n","        self.dropout = nn.Dropout(dropout)\n","\n","        g = torch.Generator()\n","        g.manual_seed(RANDOM_SEED)\n","        # initialize weights\n","        for m in self.modules():\n","            if isinstance(m, nn.Linear):\n","                nn.init.xavier_normal_(m.weight, generator=g)\n","                nn.init.zeros_(m.bias)\n","\n","    def forward(self, x_cat, x_cont):\n","        \"\"\"\n","        Create embedding layers for categorical data, concatenate with continous variables.\n","        Add dropout and goes through MLP and return raw output and 1-dimensional output as well.\n","        \"\"\"\n","        x = self.embeddings(x_cat)\n","        x = torch.cat([x, x_cont], dim=1)\n","        x = self.dropout(x)\n","        x = self.mlp(x)\n","        return self.out(x), x\n","\n","\n","@functools.lru_cache\n","def combinations(N):\n","    \"\"\"\n","    calculates all possible 2-combinations (pairs) of a tensor of indices from 0 to N-1, \n","    and caches the result using functools.lru_cache for optimization\n","    \"\"\"\n","    ind = torch.arange(N)\n","    comb = torch.combinations(ind, r=2)\n","    return comb #.cuda()\n","\n","\n","class LitNN(pl.LightningModule):\n","    \"\"\"\n","    Main Model creation and losses definition to fully train the model.\n","    \"\"\"\n","    def __init__(\n","            self,\n","            continuous_dim: int,\n","            categorical_cardinality: List[int],\n","            embedding_dim: int,\n","            projection_dim: int,\n","            hidden_dim: int,\n","            lr: float = 1e-3,\n","            dropout: float = 0.2,\n","            weight_decay: float = 1e-3,\n","            aux_weight: float = 0.1,\n","            margin: float = 0.5,\n","            race_index: int = 0\n","    ):\n","        \"\"\"\n","        continuous_dim: The number of continuous input features.\n","        categorical_cardinality: A list of integers, where each element corresponds to the number of unique categories for each categorical feature.\n","        embedding_dim: The dimension of the embeddings for the categorical features.\n","        projection_dim: The dimension of the projected space after embedding concatenation.\n","        hidden_dim: The size of the hidden layers in the feedforward network (MLP).\n","        lr: The learning rate for the optimizer.\n","        dropout: Dropout probability to avoid overfitting.\n","        weight_decay: The L2 regularization term for the optimizer.\n","        aux_weight: Weight used for auxiliary tasks.\n","        margin: Margin used in some loss functions.\n","        race_index: An index that refer to race_group in the input data.\n","        \"\"\"\n","        super(LitNN, self).__init__()\n","        self.save_hyperparameters()\n","\n","        # Creates an instance of the NN model defined above\n","        self.model = NN(\n","            continuous_dim=self.hparams.continuous_dim,\n","            categorical_cardinality=self.hparams.categorical_cardinality,\n","            embedding_dim=self.hparams.embedding_dim,\n","            projection_dim=self.hparams.projection_dim,\n","            hidden_dim=self.hparams.hidden_dim,\n","            dropout=self.hparams.dropout\n","        )\n","        self.targets = []\n","\n","        # Defines a small feedforward neural network that performs an auxiliary task with 1-dimensional output\n","        self.aux_cls = nn.Sequential(\n","            nn.Linear(self.hparams.hidden_dim, self.hparams.hidden_dim // 3),\n","            nn.GELU(),\n","            nn.Linear(self.hparams.hidden_dim // 3, 1)\n","        )\n","\n","    def on_before_optimizer_step(self, optimizer):\n","        \"\"\"\n","        Compute the 2-norm for each layer\n","        If using mixed precision, the gradients are already unscaled here\n","        \"\"\"\n","        norms = grad_norm(self.model, norm_type=2)\n","        self.log_dict(norms)\n","\n","    def forward(self, x_cat, x_cont):\n","        \"\"\"\n","        Forward pass that outputs the 1-dimensional prediction and the embeddings (raw output)\n","        \"\"\"\n","        x, emb = self.model(x_cat, x_cont)\n","        return x.squeeze(1), emb\n","\n","    def training_step(self, batch, batch_idx):\n","        \"\"\"\n","        defines how the model processes each batch of data during training.\n","        A batch is a combination of : categorical data, continuous data, efs_time (y) and efs event.\n","        y_hat is the efs_time prediction on all data and aux_pred is auxiliary prediction on embeddings.\n","        Calculates loss and race_group loss on full data.\n","        Auxiliary loss is calculated with an event mask, ignoring efs=0 predictions and taking the average.\n","        Returns loss and aux_loss multiplied by weight defined above.\n","        \"\"\"\n","        x_cat, x_cont, y, efs, y_aux = batch\n","        y_hat, emb = self(x_cat, x_cont)\n","        aux_pred = self.aux_cls(emb).squeeze(1)\n","        loss, race_loss = self.get_full_loss(efs, x_cat, y, y_hat)\n","        aux_loss = nn.functional.mse_loss(aux_pred, y_aux, reduction='none')\n","        aux_mask = efs == 1\n","        aux_loss = (aux_loss * aux_mask).sum() / aux_mask.sum()\n","        self.log(\"train_loss\", loss, on_epoch=True, prog_bar=True, logger=True)\n","        self.log(\"race_loss\", race_loss, on_epoch=True, prog_bar=True, logger=True, on_step=False)\n","        self.log(\"aux_loss\", aux_loss, on_epoch=True, prog_bar=True, logger=True, on_step=False)\n","        return loss + aux_loss * self.hparams.aux_weight\n","\n","    def get_full_loss(self, efs, x_cat, y, y_hat):\n","        \"\"\"\n","        Output loss and race_group loss.\n","        \"\"\"\n","        loss = self.calc_loss(x_cat, y, y_hat, efs)\n","        race_loss = self.get_race_losses(efs, x_cat, y, y_hat)\n","        loss += 0.1 * race_loss\n","        return loss, race_loss\n","\n","    def get_race_losses(self, efs, x_cat, y, y_hat):\n","        \"\"\"\n","        Calculate loss for each race_group based on deviation/variance.\n","        \"\"\"\n","        # races = torch.unique(x_cat[:, self.hparams.race_index])\n","        # race_losses = []\n","        # for race in races:\n","        #     ind = x_cat[:, self.hparams.race_index] == race\n","        #     race_losses.append(self.calc_loss(y[ind], y_hat[ind], efs[ind]))\n","        # race_loss = sum(race_losses) / len(race_losses)\n","        # races_loss_std = torch.sqrt(sum((r - race_loss)**2 for r in race_losses) / len(race_losses))\n","        ktools_race_loss = race_equality_loss(x_cat[:, self.hparams.race_index], efs, y, y_hat, margin=self.hparams.margin)\n","        # print(ktools_race_loss.item(), races_loss_std.item())\n","        # assert abs(ktools_race_loss.item() - races_loss_std.item()) < 1e-7\n","        return ktools_race_loss\n","\n","    def calc_loss(self, x_cat, y, y_hat, efs):\n","        \"\"\"\n","        Most important part of the model : loss function used for training.\n","        We face survival data with event indicators along with time-to-event.\n","\n","        This function computes the main loss by the following the steps :\n","        * create all data pairs with \"combinations\" function (= all \"two subjects\" combinations)\n","        * make sure that we have at least 1 event in each pair\n","        * convert y to +1 or -1 depending on the correct ranking\n","        * loss is computed using a margin-based hinge loss\n","        * mask is applied to ensure only valid pairs are being used (censored data can't be ranked with event in some cases)\n","        * average loss on all pairs is returned\n","        \"\"\"\n","        # N = y.shape[0]\n","        # comb = combinations(N).to(y_hat.device)\n","        # comb = comb[(efs[comb[:, 0]] == 1) | (efs[comb[:, 1]] == 1)]\n","        # pred_left = y_hat[comb[:, 0]]\n","        # pred_right = y_hat[comb[:, 1]]\n","        # y_left = y[comb[:, 0]]\n","        # y_right = y[comb[:, 1]]\n","        ktools_loss = pairwise_loss(x_cat[:, self.hparams.race_index], efs, y, y_hat, margin=self.hparams.margin, weight_class=False)\n","        # y = 2 * (y_left > y_right).int() - 1\n","        # loss = nn.functional.relu(-y * (pred_left - pred_right) + self.hparams.margin)\n","        # mask = self.get_mask(comb, efs, y_left, y_right)\n","        # loss = (loss.float() * (mask.float())).sum() / mask.sum()\n","        # print(loss.item(), ktools_loss.item())\n","        # assert loss.item() - ktools_loss.item() < 1e-7\n","        return ktools_loss\n","\n","    def get_mask(self, comb, efs, y_left, y_right):\n","        \"\"\"\n","        Defines all invalid comparisons :\n","        * Case 1: \"Left outlived Right\" but Right is censored\n","        * Case 2: \"Right outlived Left\" but Left is censored\n","        Masks for case 1 and case 2 are combined using |= operator and inverted using ~ to create a \"valid pair mask\"\n","        \"\"\"\n","        left_outlived = y_left >= y_right\n","        left_1_right_0 = (efs[comb[:, 0]] == 1) & (efs[comb[:, 1]] == 0)\n","        mask2 = (left_outlived & left_1_right_0)\n","        right_outlived = y_right >= y_left\n","        right_1_left_0 = (efs[comb[:, 1]] == 1) & (efs[comb[:, 0]] == 0)\n","        mask2 |= (right_outlived & right_1_left_0)\n","        mask2 = ~mask2\n","        mask = mask2\n","        return mask\n","\n","    def validation_step(self, batch, batch_idx):\n","        \"\"\"\n","        This method defines how the model processes each batch during validation\n","        \"\"\"\n","        x_cat, x_cont, y, efs, y_aux = batch\n","        y_hat, emb = self(x_cat, x_cont)\n","        loss, race_loss = self.get_full_loss(efs, x_cat, y, y_hat)\n","        self.targets.append([y, y_hat.detach(), efs, x_cat[:, self.hparams.race_index]])\n","        self.log(\"val_loss\", loss, on_epoch=True, prog_bar=True, logger=True)\n","        return loss\n","\n","    def on_validation_epoch_end(self):\n","        \"\"\"\n","        At the end of the validation epoch, it computes and logs the concordance index\n","        \"\"\"\n","        cindex, metric = self._calc_cindex()\n","        self.log(\"cindex\", metric, on_epoch=True, prog_bar=True, logger=True)\n","        self.log(\"cindex_simple\", cindex, on_epoch=True, prog_bar=True, logger=True)\n","        self.targets.clear()\n","\n","    def _calc_cindex(self):\n","        \"\"\"\n","        Calculate c-index accounting for each race_group or global.\n","        \"\"\"\n","        y = torch.cat([t[0] for t in self.targets]).cpu().numpy()\n","        y_hat = torch.cat([t[1] for t in self.targets]).cpu().numpy()\n","        efs = torch.cat([t[2] for t in self.targets]).cpu().numpy()\n","        races = torch.cat([t[3] for t in self.targets]).cpu().numpy()\n","        metric = self._metric(efs, races, y, y_hat)\n","        cindex = concordance_index(y, y_hat, efs)\n","        return cindex, metric\n","\n","    def _metric(self, efs, races, y, y_hat):\n","        \"\"\"\n","        Calculate c-index accounting for each race_group\n","        \"\"\"\n","        metric_list = []\n","        for race in np.unique(races):\n","            y_ = y[races == race]\n","            y_hat_ = y_hat[races == race]\n","            efs_ = efs[races == race]\n","            metric_list.append(concordance_index(y_, y_hat_, efs_))\n","        metric = float(np.mean(metric_list) - np.sqrt(np.var(metric_list)))\n","        return metric\n","\n","    def test_step(self, batch, batch_idx):\n","        \"\"\"\n","        Same as training step but to log test data\n","        \"\"\"\n","        x_cat, x_cont, y, efs, y_aux = batch\n","        y_hat, emb = self(x_cat, x_cont)\n","        loss, race_loss = self.get_full_loss(efs, x_cat, y, y_hat)\n","        self.targets.append([y, y_hat.detach(), efs, x_cat[:, self.hparams.race_index]])\n","        self.log(\"test_loss\", loss)\n","        return loss\n","\n","    def on_test_epoch_end(self) -> None:\n","        \"\"\"\n","        At the end of the test epoch, calculates and logs the concordance index for the test set\n","        \"\"\"\n","        cindex, metric = self._calc_cindex()\n","        self.log(\"test_cindex\", metric, on_epoch=True, prog_bar=True, logger=True)\n","        self.log(\"test_cindex_simple\", cindex, on_epoch=True, prog_bar=True, logger=True)\n","        self.targets.clear()\n","\n","\n","    def configure_optimizers(self):\n","        \"\"\"\n","        configures the optimizer and learning rate scheduler:\n","        * Optimizer: Adam optimizer with weight decay (L2 regularization).\n","        * Scheduler: Cosine Annealing scheduler, which adjusts the learning rate according to a cosine curve.\n","        \"\"\"\n","        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.lr, weight_decay=self.hparams.weight_decay)\n","        scheduler_config = {\n","            \"scheduler\": torch.optim.lr_scheduler.CosineAnnealingLR(\n","                optimizer,\n","                T_max=45,\n","                eta_min=6e-3\n","            ),\n","            \"interval\": \"epoch\",\n","            \"frequency\": 1,\n","            \"strict\": False,\n","        }\n","\n","        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler_config}"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["def score(solution: pd.DataFrame, submission: pd.DataFrame, row_id_column_name: str) -> float:\n","    \"\"\"\n","    >>> import pandas as pd\n","    >>> row_id_column_name = \"id\"\n","    >>> y_pred = {'prediction': {0: 1.0, 1: 0.0, 2: 1.0}}\n","    >>> y_pred = pd.DataFrame(y_pred)\n","    >>> y_pred.insert(0, row_id_column_name, range(len(y_pred)))\n","    >>> y_true = { 'efs': {0: 1.0, 1: 0.0, 2: 0.0}, 'efs_time': {0: 25.1234,1: 250.1234,2: 2500.1234}, 'race_group': {0: 'race_group_1', 1: 'race_group_1', 2: 'race_group_1'}}\n","    >>> y_true = pd.DataFrame(y_true)\n","    >>> y_true.insert(0, row_id_column_name, range(len(y_true)))\n","    >>> score(y_true.copy(), y_pred.copy(), row_id_column_name)\n","    0.75\n","    \"\"\"\n","    \n","    del solution[row_id_column_name]\n","    del submission[row_id_column_name]\n","    \n","    event_label = 'efs'\n","    interval_label = 'efs_time'\n","    prediction_label = 'prediction'\n","    # Merging solution and submission dfs on ID\n","    merged_df = pd.concat([solution, submission], axis=1)\n","    merged_df.reset_index(inplace=True)\n","    merged_df_race_dict = dict(merged_df.groupby(['race_group']).groups)\n","    metric_list = []\n","    for race in merged_df_race_dict.keys():\n","        # Retrieving values from y_test based on index\n","        indices = sorted(merged_df_race_dict[race])\n","        merged_df_race = merged_df.iloc[indices]\n","        # Calculate the concordance index\n","        c_index_race = concordance_index(\n","                        merged_df_race[interval_label],\n","                        -merged_df_race[prediction_label],\n","                        merged_df_race[event_label])\n","        metric_list.append(c_index_race)\n","    return float(np.mean(metric_list)-np.sqrt(np.var(metric_list)))\n","\n","def scci_metric(y_test, y_pred, id_col_name : str = \"ID\",\n","        survived_col_name : str = \"efs\",\n","        survival_time_col_name : str = \"efs_time\",\n","        stratify_col_name : str = \"race_group\"):\n","    idcs = y_test.index\n","    og_train = pd.read_csv(train_csv_path)\n","    \n","    y_true = og_train.loc[idcs, [id_col_name, survived_col_name, survival_time_col_name, stratify_col_name]].copy()\n","    y_pred_df = og_train.loc[idcs, [id_col_name]].copy()\n","    y_pred_df[\"prediction\"] = y_pred\n","    scci = score(y_true.copy(), y_pred_df.copy(), id_col_name)\n","    return scci"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Test shape: (3, 59)\n","Train shape: (28800, 61)\n","White1.0                                        593\n","Black or African-American1.0                    557\n","More than one race0.0                           525\n","Asian1.0                                        513\n","American Indian or Alaska Native1.0             489\n","Native Hawaiian or other Pacific Islander1.0    481\n","American Indian or Alaska Native0.0             469\n","Native Hawaiian or other Pacific Islander0.0    460\n","Asian0.0                                        454\n","More than one race1.0                           444\n","Black or African-American0.0                    402\n","White0.0                                        373\n","Name: count, dtype: int64\n","White1.0                                        606\n","Asian1.0                                        548\n","More than one race0.0                           524\n","Black or African-American1.0                    524\n","Native Hawaiian or other Pacific Islander1.0    500\n","American Indian or Alaska Native0.0             482\n","American Indian or Alaska Native1.0             476\n","More than one race1.0                           445\n","Native Hawaiian or other Pacific Islander0.0    441\n","Black or African-American0.0                    435\n","Asian0.0                                        419\n","White0.0                                        360\n","Name: count, dtype: int64\n","White1.0                                        596\n","Black or African-American1.0                    539\n","Asian1.0                                        530\n","American Indian or Alaska Native1.0             492\n","More than one race0.0                           491\n","Native Hawaiian or other Pacific Islander1.0    480\n","More than one race1.0                           478\n","American Indian or Alaska Native0.0             466\n","Native Hawaiian or other Pacific Islander0.0    461\n","Asian0.0                                        436\n","Black or African-American0.0                    420\n","White0.0                                        371\n","Name: count, dtype: int64\n","White1.0                                        620\n","Black or African-American1.0                    531\n","Asian1.0                                        528\n","More than one race0.0                           527\n","Native Hawaiian or other Pacific Islander1.0    502\n","American Indian or Alaska Native1.0             501\n","American Indian or Alaska Native0.0             457\n","More than one race1.0                           442\n","Native Hawaiian or other Pacific Islander0.0    440\n","Asian0.0                                        438\n","Black or African-American0.0                    428\n","White0.0                                        346\n","Name: count, dtype: int64\n","White1.0                                        607\n","Asian1.0                                        546\n","Black or African-American1.0                    528\n","More than one race0.0                           521\n","Native Hawaiian or other Pacific Islander1.0    501\n","American Indian or Alaska Native1.0             487\n","American Indian or Alaska Native0.0             471\n","More than one race1.0                           448\n","Native Hawaiian or other Pacific Islander0.0    441\n","Black or African-American0.0                    431\n","Asian0.0                                        420\n","White0.0                                        359\n","Name: count, dtype: int64\n"]}],"source":["test, train_original = load_data()\n","\n","kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)\n","folds = kf.split(\n","                train_original, train_original.race_group.astype(str)\n","            )\n","for i, (train_index, test_index) in enumerate(folds):\n","    tt = train_original.copy()\n","    train = tt.iloc[train_index]\n","    X_val = tt.iloc[test_index]\n","\n","    race_efs = X_val['race_group'].astype(str) + X_val['efs'].astype(str)\n","\n","    print(race_efs.value_counts())"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2025-01-28T16:16:39.102505Z","iopub.status.busy":"2025-01-28T16:16:39.102128Z","iopub.status.idle":"2025-01-28T16:24:38.743132Z","shell.execute_reply":"2025-01-28T16:24:38.742414Z","shell.execute_reply.started":"2025-01-28T16:16:39.102475Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Seed set to 32\n"]},{"name":"stdout","output_type":"stream","text":["Test shape: (3, 59)\n","Train shape: (28800, 61)\n"]},{"name":"stderr","output_type":"stream","text":["Seed set to 32\n"]},{"name":"stdout","output_type":"stream","text":["There are 58 FEATURES: ['dri_score', 'psych_disturb', 'cyto_score', 'diabetes', 'hla_match_c_high', 'hla_high_res_8', 'tbi_status', 'arrhythmia', 'hla_low_res_6', 'graft_type', 'vent_hist', 'renal_issue', 'pulm_severe', 'prim_disease_hct', 'hla_high_res_6', 'cmv_status', 'hla_high_res_10', 'hla_match_dqb1_high', 'tce_imm_match', 'hla_nmdp_6', 'hla_match_c_low', 'rituximab', 'hla_match_drb1_low', 'hla_match_dqb1_low', 'prod_type', 'cyto_score_detail', 'conditioning_intensity', 'ethnicity', 'year_hct', 'obesity', 'mrd_hct', 'in_vivo_tcd', 'tce_match', 'hla_match_a_high', 'hepatic_severe', 'donor_age', 'prior_tumor', 'hla_match_b_low', 'peptic_ulcer', 'age_at_hct', 'hla_match_a_low', 'gvhd_proph', 'rheum_issue', 'sex_match', 'hla_match_b_high', 'race_group', 'comorbidity_score', 'karnofsky_score', 'hepatic_mild', 'tce_div_match', 'donor_related', 'melphalan_dose', 'hla_low_res_8', 'cardiac', 'hla_match_drb1_high', 'pulm_moderate', 'hla_low_res_10', 'is_cyto_score_same']\n"]},{"name":"stderr","output_type":"stream","text":["Seed set to 32\n","GPU available: True (mps), used: False\n","TPU available: False, using: 0 TPU cores\n","HPU available: False, using: 0 HPUs\n","/Users/yuwei-1/anaconda3/envs/ktools/lib/python3.12/site-packages/pytorch_lightning/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n","/Users/yuwei-1/anaconda3/envs/ktools/lib/python3.12/site-packages/pytorch_lightning/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n","\n","  | Name    | Type       | Params | Mode \n","-----------------------------------------------\n","0 | model   | NN         | 159 K  | train\n","1 | aux_cls | Sequential | 1.0 K  | train\n","-----------------------------------------------\n","159 K     Trainable params\n","769       Non-trainable params\n","160 K     Total params\n","0.640     Total estimated model params size (MB)\n","71        Modules in train mode\n","0         Modules in eval mode\n","/Users/yuwei-1/anaconda3/envs/ktools/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n","/Users/yuwei-1/anaconda3/envs/ktools/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (12) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0c7cd966660e4b45ac3c85699bccabb1","version_major":2,"version_minor":0},"text/plain":["Training: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/Users/yuwei-1/anaconda3/envs/ktools/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:384: `ModelCheckpoint(monitor='val_loss')` could not find the monitored key in the returned metrics: ['lr-Adam', 'train_loss', 'train_loss_step', 'grad_2.0_norm/embeddings.embeddings.0.weight', 'grad_2.0_norm/embeddings.embeddings.1.weight', 'grad_2.0_norm/embeddings.embeddings.2.weight', 'grad_2.0_norm/embeddings.embeddings.3.weight', 'grad_2.0_norm/embeddings.embeddings.4.weight', 'grad_2.0_norm/embeddings.embeddings.5.weight', 'grad_2.0_norm/embeddings.embeddings.6.weight', 'grad_2.0_norm/embeddings.embeddings.7.weight', 'grad_2.0_norm/embeddings.embeddings.8.weight', 'grad_2.0_norm/embeddings.embeddings.9.weight', 'grad_2.0_norm/embeddings.embeddings.10.weight', 'grad_2.0_norm/embeddings.embeddings.11.weight', 'grad_2.0_norm/embeddings.embeddings.12.weight', 'grad_2.0_norm/embeddings.embeddings.13.weight', 'grad_2.0_norm/embeddings.embeddings.14.weight', 'grad_2.0_norm/embeddings.embeddings.15.weight', 'grad_2.0_norm/embeddings.embeddings.16.weight', 'grad_2.0_norm/embeddings.embeddings.17.weight', 'grad_2.0_norm/embeddings.embeddings.18.weight', 'grad_2.0_norm/embeddings.embeddings.19.weight', 'grad_2.0_norm/embeddings.embeddings.20.weight', 'grad_2.0_norm/embeddings.embeddings.21.weight', 'grad_2.0_norm/embeddings.embeddings.22.weight', 'grad_2.0_norm/embeddings.embeddings.23.weight', 'grad_2.0_norm/embeddings.embeddings.24.weight', 'grad_2.0_norm/embeddings.embeddings.25.weight', 'grad_2.0_norm/embeddings.embeddings.26.weight', 'grad_2.0_norm/embeddings.embeddings.27.weight', 'grad_2.0_norm/embeddings.embeddings.28.weight', 'grad_2.0_norm/embeddings.embeddings.29.weight', 'grad_2.0_norm/embeddings.embeddings.30.weight', 'grad_2.0_norm/embeddings.embeddings.31.weight', 'grad_2.0_norm/embeddings.embeddings.32.weight', 'grad_2.0_norm/embeddings.embeddings.33.weight', 'grad_2.0_norm/embeddings.embeddings.34.weight', 'grad_2.0_norm/embeddings.embeddings.35.weight', 'grad_2.0_norm/embeddings.embeddings.36.weight', 'grad_2.0_norm/embeddings.embeddings.37.weight', 'grad_2.0_norm/embeddings.embeddings.38.weight', 'grad_2.0_norm/embeddings.embeddings.39.weight', 'grad_2.0_norm/embeddings.embeddings.40.weight', 'grad_2.0_norm/embeddings.embeddings.41.weight', 'grad_2.0_norm/embeddings.embeddings.42.weight', 'grad_2.0_norm/embeddings.embeddings.43.weight', 'grad_2.0_norm/embeddings.embeddings.44.weight', 'grad_2.0_norm/embeddings.embeddings.45.weight', 'grad_2.0_norm/embeddings.embeddings.46.weight', 'grad_2.0_norm/embeddings.embeddings.47.weight', 'grad_2.0_norm/embeddings.embeddings.48.weight', 'grad_2.0_norm/embeddings.embeddings.49.weight', 'grad_2.0_norm/embeddings.embeddings.50.weight', 'grad_2.0_norm/embeddings.embeddings.51.weight', 'grad_2.0_norm/embeddings.embeddings.52.weight', 'grad_2.0_norm/embeddings.embeddings.53.weight', 'grad_2.0_norm/embeddings.projection.0.weight', 'grad_2.0_norm/embeddings.projection.0.bias', 'grad_2.0_norm/embeddings.projection.2.weight', 'grad_2.0_norm/embeddings.projection.2.bias', 'grad_2.0_norm/mlp.0.response', 'grad_2.0_norm/mlp.0.feature_selection_logits', 'grad_2.0_norm/mlp.0.feature_thresholds', 'grad_2.0_norm/mlp.0.log_temperatures', 'grad_2.0_norm/mlp.1.weight', 'grad_2.0_norm/mlp.1.bias', 'grad_2.0_norm/out.weight', 'grad_2.0_norm/out.bias', 'grad_2.0_norm_total', 'train_loss_epoch', 'race_loss', 'aux_loss', 'epoch', 'step']. HINT: Did you call `log('val_loss', value)` in the `LightningModule`?\n","\n","Detected KeyboardInterrupt, attempting graceful shutdown ...\n"]},{"ename":"NameError","evalue":"name 'exit' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","File \u001b[0;32m~/anaconda3/envs/ktools/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py:47\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n","File \u001b[0;32m~/anaconda3/envs/ktools/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:574\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    568\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    570\u001b[0m     ckpt_path,\n\u001b[1;32m    571\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    572\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    573\u001b[0m )\n\u001b[0;32m--> 574\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n","File \u001b[0;32m~/anaconda3/envs/ktools/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:981\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    978\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    979\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 981\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n","File \u001b[0;32m~/anaconda3/envs/ktools/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:1025\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1025\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m~/anaconda3/envs/ktools/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:205\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start()\n\u001b[0;32m--> 205\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n","File \u001b[0;32m~/anaconda3/envs/ktools/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:363\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 363\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/anaconda3/envs/ktools/lib/python3.12/site-packages/pytorch_lightning/loops/training_epoch_loop.py:140\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 140\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end(data_fetcher)\n","File \u001b[0;32m~/anaconda3/envs/ktools/lib/python3.12/site-packages/pytorch_lightning/loops/training_epoch_loop.py:250\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.advance\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39mautomatic_optimization:\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;66;03m# in automatic optimization, there can only be one optimizer\u001b[39;00m\n\u001b[0;32m--> 250\u001b[0m     batch_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautomatic_optimization\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n","File \u001b[0;32m~/anaconda3/envs/ktools/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py:190\u001b[0m, in \u001b[0;36m_AutomaticOptimization.run\u001b[0;34m(self, optimizer, batch_idx, kwargs)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;66;03m# BACKWARD PASS\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;66;03m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    192\u001b[0m result \u001b[38;5;241m=\u001b[39m closure\u001b[38;5;241m.\u001b[39mconsume_result()\n","File \u001b[0;32m~/anaconda3/envs/ktools/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py:268\u001b[0m, in \u001b[0;36m_AutomaticOptimization._optimizer_step\u001b[0;34m(self, batch_idx, train_step_and_backward_closure)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;66;03m# model hook\u001b[39;00m\n\u001b[0;32m--> 268\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_lightning_module_hook\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moptimizer_step\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_step_and_backward_closure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m should_accumulate:\n","File \u001b[0;32m~/anaconda3/envs/ktools/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py:167\u001b[0m, in \u001b[0;36m_call_lightning_module_hook\u001b[0;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LightningModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpl_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 167\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n","File \u001b[0;32m~/anaconda3/envs/ktools/lib/python3.12/site-packages/pytorch_lightning/core/module.py:1306\u001b[0m, in \u001b[0;36mLightningModule.optimizer_step\u001b[0;34m(self, epoch, batch_idx, optimizer, optimizer_closure)\u001b[0m\n\u001b[1;32m   1282\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Override this method to adjust the default way the :class:`~pytorch_lightning.trainer.trainer.Trainer` calls\u001b[39;00m\n\u001b[1;32m   1283\u001b[0m \u001b[38;5;124;03mthe optimizer.\u001b[39;00m\n\u001b[1;32m   1284\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1304\u001b[0m \n\u001b[1;32m   1305\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1306\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer_closure\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/anaconda3/envs/ktools/lib/python3.12/site-packages/pytorch_lightning/core/optimizer.py:153\u001b[0m, in \u001b[0;36mLightningOptimizer.step\u001b[0;34m(self, closure, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_strategy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_strategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_on_after_step()\n","File \u001b[0;32m~/anaconda3/envs/ktools/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py:238\u001b[0m, in \u001b[0;36mStrategy.optimizer_step\u001b[0;34m(self, optimizer, closure, model, **kwargs)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, pl\u001b[38;5;241m.\u001b[39mLightningModule)\n\u001b[0;32m--> 238\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecision_plugin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/anaconda3/envs/ktools/lib/python3.12/site-packages/pytorch_lightning/plugins/precision/precision.py:122\u001b[0m, in \u001b[0;36mPrecision.optimizer_step\u001b[0;34m(self, optimizer, model, closure, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m closure \u001b[38;5;241m=\u001b[39m partial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrap_closure, model, optimizer, closure)\n\u001b[0;32m--> 122\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/anaconda3/envs/ktools/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:137\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.patch_track_step_called.<locals>.wrap_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    136\u001b[0m opt\u001b[38;5;241m.\u001b[39m_opt_called \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__get__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/anaconda3/envs/ktools/lib/python3.12/site-packages/torch/optim/optimizer.py:487\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    483\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    484\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    485\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n","File \u001b[0;32m~/anaconda3/envs/ktools/lib/python3.12/site-packages/torch/optim/optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n","File \u001b[0;32m~/anaconda3/envs/ktools/lib/python3.12/site-packages/torch/optim/adam.py:202\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39menable_grad():\n\u001b[0;32m--> 202\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_groups:\n","File \u001b[0;32m~/anaconda3/envs/ktools/lib/python3.12/site-packages/pytorch_lightning/plugins/precision/precision.py:108\u001b[0m, in \u001b[0;36mPrecision._wrap_closure\u001b[0;34m(self, model, optimizer, closure)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"This double-closure allows makes sure the ``closure`` is executed before the ``on_before_optimizer_step``\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;124;03mhook is called.\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    106\u001b[0m \n\u001b[1;32m    107\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 108\u001b[0m closure_result \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_after_closure(model, optimizer)\n","File \u001b[0;32m~/anaconda3/envs/ktools/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py:144\u001b[0m, in \u001b[0;36mClosure.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[Tensor]:\n\u001b[0;32m--> 144\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\u001b[38;5;241m.\u001b[39mloss\n","File \u001b[0;32m~/anaconda3/envs/ktools/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/anaconda3/envs/ktools/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py:138\u001b[0m, in \u001b[0;36mClosure.closure\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m step_output\u001b[38;5;241m.\u001b[39mclosure_loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 138\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_output\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclosure_loss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m step_output\n","File \u001b[0;32m~/anaconda3/envs/ktools/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py:239\u001b[0m, in \u001b[0;36m_AutomaticOptimization._make_backward_fn.<locals>.backward_fn\u001b[0;34m(loss)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mbackward_fn\u001b[39m(loss: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 239\u001b[0m     \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbackward\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/anaconda3/envs/ktools/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py:319\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 319\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n","File \u001b[0;32m~/anaconda3/envs/ktools/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py:212\u001b[0m, in \u001b[0;36mStrategy.backward\u001b[0;34m(self, closure_loss, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m closure_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecision_plugin\u001b[38;5;241m.\u001b[39mpre_backward(closure_loss, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module)\n\u001b[0;32m--> 212\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecision_plugin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlightning_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m closure_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecision_plugin\u001b[38;5;241m.\u001b[39mpost_backward(closure_loss, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module)\n","File \u001b[0;32m~/anaconda3/envs/ktools/lib/python3.12/site-packages/pytorch_lightning/plugins/precision/precision.py:72\u001b[0m, in \u001b[0;36mPrecision.backward\u001b[0;34m(self, tensor, model, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Performs the actual backpropagation.\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \n\u001b[1;32m     63\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/anaconda3/envs/ktools/lib/python3.12/site-packages/pytorch_lightning/core/module.py:1101\u001b[0m, in \u001b[0;36mLightningModule.backward\u001b[0;34m(self, loss, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1101\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/anaconda3/envs/ktools/lib/python3.12/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/anaconda3/envs/ktools/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/anaconda3/envs/ktools/lib/python3.12/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n","File \u001b[0;32m~/anaconda3/envs/ktools/lib/python3.12/site-packages/torch/autograd/function.py:292\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;124;03mThis class is used for internal autograd work. Do not use.\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 292\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mapply\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m    293\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;124;03m    Apply method used when executing this Node during the backward\u001b[39;00m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: ","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","File \u001b[0;32m<timed exec>:117\u001b[0m\n","File \u001b[0;32m<timed exec>:42\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(hparams)\u001b[0m\n","File \u001b[0;32m<timed exec>:109\u001b[0m, in \u001b[0;36mtrain_final\u001b[0;34m(X_num_train, dl_train, dl_val, categorical_cardinality, hparams, categorical_cols)\u001b[0m\n","File \u001b[0;32m~/anaconda3/envs/ktools/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:538\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 538\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/anaconda3/envs/ktools/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py:64\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(launcher, _SubprocessScriptLauncher):\n\u001b[1;32m     63\u001b[0m         launcher\u001b[38;5;241m.\u001b[39mkill(_get_sigkill_signal())\n\u001b[0;32m---> 64\u001b[0m     \u001b[43mexit\u001b[49m(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n\u001b[1;32m     67\u001b[0m     _interrupt(trainer, exception)\n","\u001b[0;31mNameError\u001b[0m: name 'exit' is not defined"]}],"source":["%%time\n","\n","def main(hparams):\n","    \"\"\"\n","    Main function to train the model.\n","    The steps are as following :\n","    * load data and fill efs and efs time for test data with 1\n","    * initialize pred array with 0\n","    * get categorical and numerical columns\n","    * split the train data on the stratified criterion : race_group * newborns yes/no\n","    * preprocess the fold data (create dataloaders)\n","    * train the model and create final submission output\n","    \"\"\"\n","    metrics_list = []\n","    test, train_original = load_data()\n","    test['efs_time'] = 1\n","    test['efs'] = 1\n","    test_pred = np.zeros(test.shape[0])\n","    \n","    # test_pred = np.zeros(test_df.shape[0])\n","    oof_preds = np.zeros(train_original.shape[0])\n","\n","    categorical_cols, numerical = get_feature_types(train_original)\n","    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)\n","    folds = kf.split(\n","                    train_original, train_original.race_group.astype(str)\n","                )\n","    # [next(folds) for _ in range(3)]\n","    for i, (train_index, test_index) in enumerate(folds):\n","        if ORIGINAL_DATA:\n","            tt = train_original.copy()\n","            train = tt.iloc[train_index]\n","            X_val = tt.iloc[test_index]\n","            X_cat_val, X_num_train, X_num_val, dl_train, dl_val, transformers = preprocess_data(train, X_val)\n","        else:\n","            X_train, y_train = X.iloc[train_index], y.iloc[train_index]\n","            X_val, y_val = X.iloc[test_index], y.iloc[test_index]\n","            dl_train, X_cat_train, X_num_train = init_ktools_dl(X_train, y_train, training=True)\n","            dl_val, X_cat_val, X_num_val = init_ktools_dl(X_val, y_val)\n","\n","        cat_names = settings.categorical_col_names\n","        cat_sizes = [int(x) for x in X[cat_names].nunique().values]\n","\n","        model = train_final(X_num_train, dl_train, dl_val, cat_sizes, categorical_cols=categorical_cols)\n","\n","        oof_pred, _ = model.eval()(\n","            torch.tensor(X_cat_val, dtype=torch.long),\n","            torch.tensor(X_num_val, dtype=torch.float32)\n","        )\n","\n","        metrics_list += [scci_metric(X_val, -oof_pred.detach().cpu().numpy())]\n","\n","        oof_preds[test_index] = oof_pred.detach().cpu().numpy()\n","\n","        # Create submission\n","        if ORIGINAL_DATA:\n","            train = tt.iloc[train_index]\n","            X_cat_val, X_num_train, X_num_val, dl_train, dl_val, transformers = preprocess_data(train, test)\n","        else:\n","            X_cat_val, X_num_val = test_df.select_dtypes('category').values, test_df.select_dtypes('number').values\n","            \n","        pred, _ = model.eval()(\n","            torch.tensor(X_cat_val, dtype=torch.long),\n","            torch.tensor(X_num_val, dtype=torch.float32)\n","        )\n","        test_pred += pred.detach().cpu().numpy()\n","    \n","    print(f\"metric across folds: \", [f\"{n:.3f}\" for n in metrics_list])\n","    print(\"oof scci metric score: \", scci_metric(train_original, -oof_preds))\n","    subm_data = pd.read_csv(sub_csv_path)\n","    subm_data['prediction'] = -test_pred\n","    subm_data.to_csv('submission.csv', index=False)\n","    \n","    display(subm_data.head())\n","    return \n","\n","\n","def train_final(X_num_train, dl_train, dl_val, categorical_cardinality, hparams=None, categorical_cols=None):\n","    \"\"\"\n","    Defines model hyperparameters and fit the model.\n","    \"\"\"\n","    if hparams is None:\n","        hparams = {\n","            \"embedding_dim\": 16,\n","            \"projection_dim\": 112,\n","            \"hidden_dim\": 56,\n","            \"lr\": 0.06464861983337984,\n","            \"dropout\": 0.05463240181423116,\n","            \"aux_weight\": 0.26545778308743806,\n","            \"margin\": 0.2588153271003354,\n","            \"weight_decay\": 0.0002773544957610778\n","        }\n","    model = LitNN(\n","        continuous_dim=X_num_train.shape[1],\n","        categorical_cardinality=categorical_cardinality,\n","        race_index=categorical_cols.index(\"race_group\"),\n","        **hparams\n","    )\n","    checkpoint_callback = pl.callbacks.ModelCheckpoint(monitor=\"val_loss\", save_top_k=1)\n","    trainer = pl.Trainer(\n","        accelerator='cpu',\n","        max_epochs=60,\n","        callbacks=[\n","            checkpoint_callback,\n","            LearningRateMonitor(logging_interval='epoch'),\n","            TQDMProgressBar(),\n","            StochasticWeightAveraging(swa_lrs=1e-5, swa_epoch_start=45, annealing_epochs=15)\n","        ],\n","        deterministic=True\n","    )\n","    trainer.fit(model, dl_train)\n","    # model = LitNN.load_from_checkpoint(checkpoint_callback.best_model_path)\n","    trainer.test(model, dl_val)\n","    return model.eval()\n","\n","\n","hparams = None\n","set_seed(RANDOM_SEED)\n","res = main(hparams)\n","print(\"done\")"]},{"cell_type":"markdown","metadata":{},"source":["---"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"databundleVersionId":10381525,"sourceId":70942,"sourceType":"competition"},{"sourceId":211322530,"sourceType":"kernelVersion"},{"sourceId":219607918,"sourceType":"kernelVersion"}],"dockerImageVersionId":30840,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.0"}},"nbformat":4,"nbformat_minor":4}
