{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "487aef93",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-31T18:09:07.759443Z",
     "iopub.status.busy": "2024-12-31T18:09:07.758903Z",
     "iopub.status.idle": "2024-12-31T18:09:07.817870Z",
     "shell.execute_reply": "2024-12-31T18:09:07.816630Z"
    },
    "papermill": {
     "duration": 0.070249,
     "end_time": "2024-12-31T18:09:07.820758",
     "exception": false,
     "start_time": "2024-12-31T18:09:07.750509",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['insurance-premium-prediction', 'autofe-feature-importance', 'insurance-dbs', 'playground-series-s4e12']\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "print(os.listdir('/kaggle/input/'))\n",
    "\n",
    "path = \"/kaggle/input/insurance-dbs\"\n",
    "dst_path = \"/kaggle/working\"\n",
    "\n",
    "for file in os.listdir(path):\n",
    "    database = os.path.join(path, file)\n",
    "    shutil.copyfile(database, os.path.join(dst_path, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac9c5835",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-31T18:09:07.837869Z",
     "iopub.status.busy": "2024-12-31T18:09:07.836547Z",
     "iopub.status.idle": "2024-12-31T18:09:08.845961Z",
     "shell.execute_reply": "2024-12-31T18:09:08.844437Z"
    },
    "papermill": {
     "duration": 1.021267,
     "end_time": "2024-12-31T18:09:08.849137",
     "exception": false,
     "start_time": "2024-12-31T18:09:07.827870",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/insurance-premium-prediction/README.md\n",
      "/kaggle/input/insurance-premium-prediction/Insurance Premium Prediction Dataset.csv\n",
      "/kaggle/input/autofe-feature-importance/robust_feature_importance.csv\n",
      "/kaggle/input/autofe-feature-importance/combined_df.csv\n",
      "/kaggle/input/insurance-dbs/cat_lg_with_original.db\n",
      "/kaggle/input/insurance-dbs/cat_lossguide.db\n",
      "/kaggle/input/insurance-dbs/cat_gpu.db\n",
      "/kaggle/input/insurance-dbs/cat_depthwise.db\n",
      "/kaggle/input/insurance-dbs/cat_symtree.db\n",
      "/kaggle/input/playground-series-s4e12/sample_submission.csv\n",
      "/kaggle/input/playground-series-s4e12/train.csv\n",
      "/kaggle/input/playground-series-s4e12/test.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import sys\n",
    "import pandas as pd\n",
    "from functools import reduce\n",
    "from lifelines.utils import concordance_index\n",
    "from ktools.metrics.stratified_concordance_index import stratified_concordance_index\n",
    "import numpy as np # linear algebra\n",
    "\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a41a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas.api.types\n",
    "import numpy as np\n",
    "from typing import Union\n",
    "from lifelines.utils import concordance_index\n",
    "\n",
    "\n",
    "def stratified_concordance_index(solution : pd.DataFrame, \n",
    "                                 predictions : Union[pd.Series, np.ndarray], \n",
    "                                 event_binary_col_name : str,\n",
    "                                 duration_col_name : str,\n",
    "                                 group_col_name : str) -> float:\n",
    "    \n",
    "    \"\"\"\n",
    "    Solution dataframe should contain all necessary columns\n",
    "    \"\"\"\n",
    "\n",
    "    solution['predictions'] = predictions\n",
    "    solution.reset_index(inplace=True)\n",
    "    solution_group_dict = dict(solution.groupby([group_col_name]).groups)\n",
    "    metric_list = []\n",
    "\n",
    "    for race in solution_group_dict.keys():\n",
    "\n",
    "        indices = sorted(solution_group_dict[race])\n",
    "        merged_df_race = solution.iloc[indices]\n",
    "\n",
    "        c_index_race = concordance_index(\n",
    "                        merged_df_race[duration_col_name],\n",
    "                        -merged_df_race['predictions'],\n",
    "                        merged_df_race[event_binary_col_name])\n",
    "        metric_list.append(c_index_race)\n",
    "    return float(np.mean(metric_list)-np.sqrt(np.var(metric_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bed813eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-31T18:09:08.922997Z",
     "iopub.status.busy": "2024-12-31T18:09:08.922614Z",
     "iopub.status.idle": "2024-12-31T18:09:12.728139Z",
     "shell.execute_reply": "2024-12-31T18:09:12.726772Z"
    },
    "papermill": {
     "duration": 3.81795,
     "end_time": "2024-12-31T18:09:12.730820",
     "exception": false,
     "start_time": "2024-12-31T18:09:08.912870",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from typing import Any, Dict, List, Tuple, Callable, Union\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "\n",
    "class CrossValidationExecutor:\n",
    "\n",
    "    def __init__(self,\n",
    "                 sklearn_model_instance,\n",
    "                 evaluation_metric : Callable,\n",
    "                 kfold_object,\n",
    "                 training_features : Union[List[str], None] = None,\n",
    "                 use_test_as_valid = True,\n",
    "                 num_classes = None,\n",
    "                 verbose=1) -> None:\n",
    "        \n",
    "        self.model = sklearn_model_instance\n",
    "        self._evaluation_metric = evaluation_metric\n",
    "        self._kf = kfold_object\n",
    "        self._num_splits = kfold_object.get_n_splits()\n",
    "        self._training_features = training_features\n",
    "        self._use_test_as_valid = use_test_as_valid\n",
    "        self._num_classes = num_classes\n",
    "        self._verbose = verbose\n",
    "\n",
    "    def run(self, X : pd.DataFrame, y : Union[pd.DataFrame, pd.Series], additional_data=None, local_transform_list=[lambda x : x], output_transform_list=[lambda x : x[-1]]) -> Tuple[Tuple[float], np.ndarray, List[Any]]:\n",
    "\n",
    "        training_features = X.columns.tolist() if self._training_features is None else self._training_features\n",
    "        X.reset_index(drop=True, inplace=True)\n",
    "        y.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        if additional_data is not None:\n",
    "            X_add, y_add = additional_data\n",
    "            pd.testing.assert_index_equal(X.columns, X_add.columns, check_exact=True)\n",
    "            pd.testing.assert_series_equal(X.dtypes, X_add.dtypes, check_exact=True)\n",
    "            pd.testing.assert_index_equal(y.columns, y_add.columns, check_exact=True)\n",
    "            pd.testing.assert_series_equal(y.dtypes, y_add.dtypes, check_exact=True)\n",
    "\n",
    "        cv_results = []\n",
    "        model_list = []\n",
    "        # oof_predictions = np.zeros(y.shape[0]) if self._num_classes is None else np.zeros((y.shape[0], self._num_classes))\n",
    "        oof_predictions = None\n",
    "        # metric_predictions = np.zeros(y.shape[0]) if self._num_classes is None else np.zeros((y.shape[0], self._num_classes))\n",
    "        metric_predictions = None\n",
    "\n",
    "        for i, (train_index, val_index) in enumerate(self._kf.split(X, y)):\n",
    "            \n",
    "            X_full_test = X.loc[val_index, :]\n",
    "            X_train, X_test = X.loc[train_index, training_features], X.loc[val_index, training_features]\n",
    "            y_train, y_test = y.loc[train_index], y.loc[val_index]\n",
    "\n",
    "            if additional_data is not None:\n",
    "                X_train = pd.concat([X_train, X_add], axis=0)\n",
    "                y_train = pd.concat([y_train, y_add], axis=0)\n",
    "\n",
    "            X_train, y_train = reduce(lambda acc, func: func(acc), local_transform_list, (X_train, y_train))\n",
    "            validation_set = None\n",
    "            if self._use_test_as_valid:\n",
    "                validation_set = [X_test, y_test]\n",
    "\n",
    "            model = deepcopy(self.model).fit(X_train, y_train, validation_set=validation_set)\n",
    "            model_list += [model]\n",
    "            y_pred = model.predict(X_test)\n",
    "            y_pred_processed = reduce(lambda acc, func: func(acc), output_transform_list, (X_full_test.copy(), y_pred))\n",
    "            \n",
    "            cv_results += [self._evaluation_metric(y_test, deepcopy(y_pred_processed))]\n",
    "\n",
    "            if oof_predictions is None:\n",
    "                oof_shape = (y.shape[0],) if len(y_pred.shape) == 1 else (y.shape[0], y_pred.shape[-1])\n",
    "                oof_predictions = np.zeros(oof_shape)\n",
    "            if metric_predictions is None:\n",
    "                y_hat_shape = (y.shape[0],) if len(y_pred_processed.shape) == 1 else (y.shape[0], y_pred_processed.shape[-1])\n",
    "                metric_predictions = np.zeros(y_hat_shape)\n",
    "\n",
    "            oof_predictions[val_index] = y_pred\n",
    "            metric_predictions[val_index] = y_pred_processed\n",
    "\n",
    "            if self._verbose > 1:\n",
    "                print(f\"The CV results of the current fold is {cv_results[-1]}\")\n",
    "\n",
    "        oof_score = self._evaluation_metric(y, metric_predictions)\n",
    "        mean_cv_score = np.mean(cv_results)\n",
    "        score_tuple = (oof_score, mean_cv_score)\n",
    "\n",
    "        if self._verbose > 0:\n",
    "            print(\"#\"*100)\n",
    "            print(\"OOF prediction score : \", oof_score)\n",
    "            print(f\"Mean {self._num_splits}-cv results : {mean_cv_score} +- {np.std(cv_results)}\")\n",
    "            print(\"#\"*100)\n",
    "\n",
    "        return score_tuple, oof_predictions, model_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c11181",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import sys\n",
    "# from lightgbm import early_stopping, log_evaluation\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "class LGBMModel():\n",
    "\n",
    "\n",
    "\n",
    "    def __init__(self,\n",
    "\n",
    "                 num_boost_round=100,\n",
    "\n",
    "                 early_stopping_rounds=20,\n",
    "\n",
    "                 random_state=129,\n",
    "\n",
    "                 verbose=-1,\n",
    "\n",
    "                 n_jobs=1,\n",
    "\n",
    "                 **lgb_param_grid,) -> None:\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self._num_boost_round = num_boost_round\n",
    "\n",
    "        self._lgb_param_grid = {\"verbose\" : verbose, \n",
    "\n",
    "                                \"early_stopping_rounds\" : early_stopping_rounds,\n",
    "\n",
    "                                \"random_state\" : random_state,\n",
    "\n",
    "                                \"n_jobs\" : n_jobs,\n",
    "\n",
    "                                **lgb_param_grid}\n",
    "\n",
    "        self._callbacks = [\n",
    "\n",
    "                            # log_evaluation(period=log_period), \n",
    "\n",
    "                            # early_stopping(stopping_rounds=stopping_rounds)\n",
    "\n",
    "                           ]\n",
    "\n",
    "        self._random_state = random_state\n",
    "\n",
    "        \n",
    "\n",
    "    def fit(self, X, y, validation_set = None, val_size=0.05):\n",
    "\n",
    "        if validation_set is None:\n",
    "\n",
    "            X_train, X_valid, y_train, y_valid = train_test_split(X, \n",
    "\n",
    "                                                                  y, \n",
    "\n",
    "                                                                  test_size=val_size, \n",
    "\n",
    "                                                                  random_state=self._random_state)\n",
    "\n",
    "        else:\n",
    "\n",
    "            X_train, y_train = X, y\n",
    "\n",
    "            X_valid, y_valid = validation_set\n",
    "\n",
    "\n",
    "\n",
    "        train_data = lgb.Dataset(X_train, label=y_train)\n",
    "\n",
    "        val_data = lgb.Dataset(X_valid, label=y_valid, reference=train_data)\n",
    "\n",
    "        self.model = lgb.train(self._lgb_param_grid,\n",
    "\n",
    "                                train_data,\n",
    "\n",
    "                                num_boost_round=self._num_boost_round,\n",
    "\n",
    "                                valid_sets=[train_data, val_data],\n",
    "\n",
    "                                valid_names=['train', 'valid'],\n",
    "\n",
    "                                callbacks=self._callbacks,\n",
    "                                )\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = self.model.predict(X)\n",
    "        return y_pred\n",
    "\n",
    "    \n",
    "import optuna\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from optuna.samplers import TPESampler\n",
    "from typing import *\n",
    "\n",
    "\n",
    "class OptunaHyperparameterOptimizer():\n",
    "\n",
    "    def __init__(self,\n",
    "                 X_train : pd.DataFrame,\n",
    "                 y_train : pd.DataFrame,\n",
    "                 model,\n",
    "                 param_grid_getter,\n",
    "                 kfold_object,\n",
    "                 metric : callable,\n",
    "                 training_features : Union[List[str], None] = None,\n",
    "                 direction : str = 'maximize',\n",
    "                 n_trials : int = 100,\n",
    "                 study_name : str = \"ml_experiment\",\n",
    "                 explore_fraction : float = 0.1,\n",
    "                 cross_validation_run_kwargs = {},\n",
    "                 verbose=False,\n",
    "                 random_state=42\n",
    "                 ) -> None:\n",
    "        \n",
    "        super().__init__()\n",
    "        self._X_train = X_train\n",
    "        self._y_train = y_train\n",
    "        self.model = model\n",
    "        self._metric = metric\n",
    "        self._training_features = training_features\n",
    "        self._param_grid_getter = param_grid_getter\n",
    "        self._kfold_object = kfold_object\n",
    "        self._direction = direction\n",
    "        self._n_trials = n_trials\n",
    "        self._study_name = study_name\n",
    "        self._explore_fraction = explore_fraction\n",
    "        self._cross_validation_run_kwargs = cross_validation_run_kwargs\n",
    "        self._verbose = verbose\n",
    "        self._random_state = random_state\n",
    "\n",
    "    def optimize(self, \n",
    "                 inital_parameters : Dict[str, float] = None,\n",
    "                 initial_distribution : Dict[str, Any] = None,\n",
    "                 timeout : int = 3600\n",
    "                 ):\n",
    "        if self._verbose:\n",
    "            print(\"#\"*100)\n",
    "            print(\"Starting Optuna Optimizer\")\n",
    "            print(\"#\"*100)\n",
    "\n",
    "        sampler = TPESampler(n_startup_trials=int(self._n_trials*self._explore_fraction),\n",
    "                             seed=self._random_state)\n",
    "\n",
    "        storage_name = \"sqlite:///{}.db\".format(self._study_name)\n",
    "        study = optuna.create_study(sampler=sampler,\n",
    "                                    study_name=self._study_name, \n",
    "                                    direction=self._direction,\n",
    "                                    storage=storage_name,\n",
    "                                    load_if_exists=True)\n",
    "        \n",
    "        if inital_parameters is not None:\n",
    "            fixed_trial = optuna.trial.FixedTrial(inital_parameters)\n",
    "            study.add_trial(optuna.create_trial(\n",
    "                            params=inital_parameters,\n",
    "                            distributions=initial_distribution,\n",
    "                            value=self._objective(fixed_trial)\n",
    "            ))\n",
    "        study.optimize(self._objective, n_trials=self._n_trials, timeout=timeout)\n",
    "        # joblib.dump(study, \"/kaggle/working/study.pkl\")\n",
    "        optimal_params = study.best_params\n",
    "        return optimal_params\n",
    "    \n",
    "    def _objective(self, trial : optuna.Trial):\n",
    "        parameters = self._param_grid_getter.get(trial)\n",
    "\n",
    "        cv_scores, oof, model_list = CrossValidationExecutor(self.model(**parameters),\n",
    "                                                             self._metric,\n",
    "                                                             self._kfold_object,\n",
    "                                                             training_features=self._training_features,\n",
    "                                                             use_test_as_valid=True\n",
    "                                                             ).run(self._X_train, \n",
    "                                                                   self._y_train, \n",
    "                                                                   **self._cross_validation_run_kwargs)\n",
    "\n",
    "        return cv_scores[0]\n",
    "\n",
    "    \n",
    "\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class XGBoostModel():\n",
    "\n",
    "\n",
    "\n",
    "    def __init__(self,\n",
    "\n",
    "                 eval_verbosity=False,\n",
    "\n",
    "                 num_boost_round=100,\n",
    "\n",
    "                 early_stopping_rounds=20,\n",
    "\n",
    "                 random_state=129,\n",
    "\n",
    "                 verbosity=0,\n",
    "\n",
    "                 n_jobs=-1,\n",
    "\n",
    "                 **xgb_param_grid) -> None:\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self._eval_verbosity = eval_verbosity\n",
    "\n",
    "        self._num_boost_round = num_boost_round\n",
    "\n",
    "        self._early_stopping_rounds = early_stopping_rounds\n",
    "\n",
    "        self._xgb_param_grid = {\"verbosity\" : verbosity,\n",
    "\n",
    "                                \"random_state\" : random_state,\n",
    "\n",
    "                                \"n_jobs\" : n_jobs,\n",
    "\n",
    "                                **xgb_param_grid}\n",
    "\n",
    "        self._random_state = random_state\n",
    "\n",
    "    \n",
    "\n",
    "    def fit(self, X, y, validation_set = None, val_size=0.05):\n",
    "\n",
    "        if validation_set is None:\n",
    "\n",
    "            X_train, X_valid, y_train, y_valid = train_test_split(X, \n",
    "\n",
    "                                                                  y, \n",
    "\n",
    "                                                                  test_size=val_size, \n",
    "\n",
    "                                                                  random_state=self._random_state)\n",
    "\n",
    "        else:\n",
    "\n",
    "            X_train, y_train = X, y\n",
    "\n",
    "            X_valid, y_valid = validation_set\n",
    "\n",
    "        train_data = xgb.DMatrix(X_train, label=y_train, enable_categorical=True)\n",
    "\n",
    "        valid_data = xgb.DMatrix(X_valid, label=y_valid, enable_categorical=True)\n",
    "\n",
    "        eval_data = [(train_data, 'train'), (valid_data, 'eval')]\n",
    "\n",
    "    \n",
    "\n",
    "        self.model = xgb.train(\n",
    "\n",
    "            self._xgb_param_grid, \n",
    "\n",
    "            train_data, \n",
    "\n",
    "            evals=eval_data,                       \n",
    "\n",
    "            early_stopping_rounds=self._early_stopping_rounds,   \n",
    "\n",
    "            num_boost_round=self._num_boost_round,        \n",
    "\n",
    "            verbose_eval=self._eval_verbosity                 \n",
    "\n",
    "        )\n",
    "\n",
    "        return self\n",
    "\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "\n",
    "        test_data = xgb.DMatrix(X, enable_categorical=True)\n",
    "\n",
    "        y_pred = self.model.predict(test_data)\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "import catboost as cat\n",
    "\n",
    "from typing import List\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from catboost import CatBoostRegressor, Pool, train\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class CatBoostModel():\n",
    "\n",
    "\n",
    "\n",
    "    def __init__(self,\n",
    "\n",
    "                 num_boost_round=100,\n",
    "\n",
    "                 early_stopping_rounds=20,\n",
    "\n",
    "                 random_state=129,\n",
    "\n",
    "                 verbose=False,\n",
    "\n",
    "                 **catboost_params) -> None:\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self._num_boost_round = num_boost_round\n",
    "\n",
    "        self._stopping_rounds = early_stopping_rounds\n",
    "\n",
    "        self._catboost_params = {\"random_seed\" : random_state,\n",
    "\n",
    "                                 \"verbose\" : verbose,\n",
    "\n",
    "                                 **catboost_params}\n",
    "\n",
    "        self._random_state = random_state\n",
    "\n",
    "\n",
    "\n",
    "    def fit(self, X, y, validation_set = None, val_size=0.05):\n",
    "\n",
    "        self.cat_col_names = [col_name for col_name in X.columns if X[col_name].dtype == 'category']\n",
    "\n",
    "\n",
    "\n",
    "        if validation_set is None:\n",
    "\n",
    "            X_train, X_valid, y_train, y_valid = train_test_split(X, \n",
    "\n",
    "                                                                  y, \n",
    "\n",
    "                                                                  test_size=val_size, \n",
    "\n",
    "                                                                  random_state=self._random_state)\n",
    "\n",
    "        else:\n",
    "\n",
    "            X_train, y_train = X, y\n",
    "\n",
    "            X_valid, y_valid = validation_set\n",
    "\n",
    "            \n",
    "\n",
    "        train_pool = Pool(data=X_train, label=y_train, cat_features=self.cat_col_names)\n",
    "\n",
    "        val_pool = Pool(data=X_valid, label=y_valid, cat_features=self.cat_col_names)\n",
    "\n",
    "        self.model = cat.train(\n",
    "\n",
    "                params=self._catboost_params,           \n",
    "\n",
    "                dtrain=train_pool,   \n",
    "\n",
    "                eval_set=val_pool,\n",
    "\n",
    "                num_boost_round=self._num_boost_round,   \n",
    "\n",
    "                early_stopping_rounds=self._stopping_rounds  \n",
    "\n",
    "                )\n",
    "\n",
    "        return self\n",
    "\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "\n",
    "        test_pool = Pool(data=X, cat_features=self.cat_col_names)\n",
    "\n",
    "        y_pred = self.model.predict(test_pool)\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "    \n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, TargetEncoder\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier, HistGradientBoostingRegressor\n",
    "\n",
    "    \n",
    "\n",
    "class HGBModel():\n",
    "\n",
    "    def __init__(self,\n",
    "                 smooth=\"auto\", \n",
    "                 target_type=\"continuous\",\n",
    "                 num_boost_round=100,\n",
    "                 early_stopping=True,\n",
    "                 validation_fraction=0.05,\n",
    "                 early_stopping_rounds=20,\n",
    "                 verbose=0,\n",
    "                 random_state=129,\n",
    "                 **hgb_params) -> None:\n",
    "        hgb_params = {\"max_iter\" : num_boost_round,\n",
    "                      \"early_stopping\" : early_stopping,\n",
    "                      \"validation_fraction\" : validation_fraction,\n",
    "                      \"n_iter_no_change\" : early_stopping_rounds,\n",
    "                      \"verbose\" : verbose,\n",
    "                      \"random_state\" : random_state,\n",
    "                      \"categorical_features\" : \"from_dtype\",\n",
    "                      **hgb_params}\n",
    "        \n",
    "        self._target_enc = TargetEncoder(target_type=target_type, \n",
    "                                         smooth=smooth, \n",
    "                                         random_state=random_state)\n",
    "        self._target_type = target_type\n",
    "        if target_type == \"continuous\":\n",
    "            self.model = HistGradientBoostingRegressor(**hgb_params)\n",
    "        else:\n",
    "            self.model = HistGradientBoostingClassifier(**hgb_params)\n",
    "\n",
    "    def fit(self, X, y, validation_set=None, **kwargs):\n",
    "        categorical_features = [col_name for col_name in X.columns if X[col_name].dtype == 'category']\n",
    "        target_enc_values = self._target_enc.fit_transform(X[categorical_features], y)\n",
    "        X = X.drop(columns=categorical_features)\n",
    "        X[categorical_features] = target_enc_values\n",
    "        self.model.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        categorical_features = [col_name for col_name in X.columns if X[col_name].dtype == 'category']\n",
    "        target_enc_values = self._target_enc.transform(X[categorical_features])\n",
    "        X = X.drop(columns=categorical_features)\n",
    "        X[categorical_features] = target_enc_values\n",
    "        if self._target_type == \"continuous\":\n",
    "            y_pred = self.model.predict(X)\n",
    "        else:\n",
    "            y_pred = self.model.predict_proba(X)\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "\n",
    "from ydf import GradientBoostedTreesLearner, Task\n",
    "from typing import List, Union\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "class YDFGBoostModel():\n",
    "\n",
    "    target_col = \"target\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 num_boost_round : int = 100,\n",
    "                 early_stopping_rounds : int = 20,\n",
    "                 task : str = \"REGRESSION\",\n",
    "                 categorical_algorithm : str = \"RANDOM\",\n",
    "                 loss : str = \"SQUARED_ERROR\",\n",
    "                 random_state : int = 42,\n",
    "                 verbose : bool = False,\n",
    "                 **model_kwargs\n",
    "                 ) -> None:\n",
    "        super().__init__()\n",
    "        self._random_state = random_state\n",
    "        self._verbose = verbose\n",
    "\n",
    "        task = Task.CLASSIFICATION if task.upper() == \"CLASSIFICATION\" else Task.REGRESSION\n",
    "        self.model = GradientBoostedTreesLearner(label = self.target_col,\n",
    "                                                 task = task,\n",
    "                                                 categorical_algorithm = categorical_algorithm,\n",
    "                                                 loss = loss,\n",
    "                                                 early_stopping_num_trees_look_ahead = early_stopping_rounds,\n",
    "                                                 num_trees = num_boost_round,\n",
    "                                                 **model_kwargs)\n",
    "\n",
    "    def _convert_back_to_dataset(self, X, y):\n",
    "        X[self.target_col] = y.values\n",
    "        return X\n",
    "    \n",
    "    def fit(self, X : pd.DataFrame, y : Union[pd.DataFrame, pd.Series, np.ndarray], \n",
    "            validation_set = None, val_size=0.05):\n",
    "        \n",
    "        if validation_set is None:\n",
    "            X_train, X_valid, y_train, y_valid = train_test_split(X, \n",
    "                                                                  y, \n",
    "                                                                  test_size=val_size, \n",
    "                                                                  random_state=self._random_state)\n",
    "        else:\n",
    "            X_train, y_train = X, y\n",
    "            X_valid, y_valid = validation_set\n",
    "        \n",
    "        train_df = self._convert_back_to_dataset(X_train, y_train)\n",
    "        valid_df = self._convert_back_to_dataset(X_valid, y_valid)\n",
    "\n",
    "        self.model = self.model.train(train_df, valid=valid_df, verbose=self._verbose)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = self.model.predict(X)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "003bc650",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-31T18:09:12.746707Z",
     "iopub.status.busy": "2024-12-31T18:09:12.746004Z",
     "iopub.status.idle": "2024-12-31T18:09:12.761880Z",
     "shell.execute_reply": "2024-12-31T18:09:12.760706Z"
    },
    "papermill": {
     "duration": 0.026737,
     "end_time": "2024-12-31T18:09:12.764487",
     "exception": false,
     "start_time": "2024-12-31T18:09:12.737750",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import *\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def func(x):\n",
    "    return x\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataSciencePipelineSettings:\n",
    "    train_csv_path : str\n",
    "    test_csv_path : str\n",
    "    target_col_name : str\n",
    "    original_csv_path : str = None\n",
    "    original_csv_processing : callable = func\n",
    "    sample_submission_path : str = None\n",
    "    training_col_names : List[str] = None\n",
    "    categorical_col_names : List[str] = None\n",
    "    training_data_percentage : float = 0.8\n",
    "    category_occurrence_threshold : int = 300\n",
    "    logged : bool = False\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.train_df, self.test_df = self._load_csv_paths()\n",
    "        self.training_col_names, self.categorical_col_names = self._get_column_info()\n",
    "        self.combined_df = self._combine_datasets()\n",
    "\n",
    "    def _load_csv_paths(self):\n",
    "        train_df = self._smart_drop_index(pd.read_csv(self.train_csv_path))\n",
    "        test_df = self._smart_drop_index(pd.read_csv(self.test_csv_path))\n",
    "        if self.original_csv_path is not None:\n",
    "            train_df = train_df.assign(source=0)\n",
    "            test_df = test_df.assign(source=0)\n",
    "            original_df = self._smart_drop_index(pd.read_csv(self.original_csv_path)).assign(source=1)\n",
    "            original_df = self.original_csv_processing(original_df)\n",
    "\n",
    "            pd.testing.assert_index_equal(train_df.columns.sort_values(), original_df.columns.sort_values(), check_exact=True)\n",
    "            pd.testing.assert_series_equal(train_df.dtypes.sort_index(), original_df.dtypes.sort_index(), check_exact=True)\n",
    "            train_df = pd.concat([train_df, original_df], axis=0).reset_index(drop=True)\n",
    "\n",
    "        return train_df, test_df\n",
    "    \n",
    "    def _get_column_info(self):\n",
    "        cat_col_names = [col_name for col_name in self.train_df.columns if self.train_df[col_name].dtype == 'object']\n",
    "        training_features = list(self.train_df.drop(columns=self.target_col_name).columns)\n",
    "        return training_features, cat_col_names\n",
    "    \n",
    "    def _combine_datasets(self):\n",
    "        combined_df = pd.concat([self.train_df, self.test_df], keys=['train', 'test'])\n",
    "        return combined_df\n",
    "    \n",
    "    def update(self):\n",
    "        self.train_df = self.combined_df.loc['train'].copy()\n",
    "        self.test_df = self.combined_df.loc['test'].copy()\n",
    "        return self.train_df, self.test_df        \n",
    "\n",
    "    @staticmethod\n",
    "    def _smart_drop_index(df):\n",
    "        try:\n",
    "            differences = df.iloc[:, 0].diff().dropna()\n",
    "            if differences.nunique() == 1:\n",
    "                df = df.drop(columns=df.columns[0])\n",
    "        except:\n",
    "            pass\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29438eef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-31T18:09:12.781934Z",
     "iopub.status.busy": "2024-12-31T18:09:12.781443Z",
     "iopub.status.idle": "2024-12-31T18:09:12.825552Z",
     "shell.execute_reply": "2024-12-31T18:09:12.824335Z"
    },
    "papermill": {
     "duration": 0.056628,
     "end_time": "2024-12-31T18:09:12.828160",
     "exception": false,
     "start_time": "2024-12-31T18:09:12.771532",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BaseLGBMParamGrid():\n",
    "    @staticmethod\n",
    "    def get(trial : optuna.Trial):\n",
    "        params = {\n",
    "            # \"boosting_type\" : \"gbdt\",\n",
    "            # \"early_stopping_rounds\" : trial.suggest_int(\"early_stopping_rounds\", 1, 200, log=True),\n",
    "            \"early_stopping_rounds\" : 20,\n",
    "            \"num_leaves\" : trial.suggest_int(\"num_leaves\", 2, 500),\n",
    "            \"max_depth\" : trial.suggest_int(\"max_depth\", 0, 50),\n",
    "            \"learning_rate\" : trial.suggest_float(\"learning_rate\", 1e-2, 1.0, log=True),\n",
    "            # \"num_boost_round\" : trial.suggest_int(\"num_boost_round\", 50, 3000),\n",
    "            \"num_boost_round\" : 10000,\n",
    "            \"subsample\" : trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "            \"colsample_bytree\" : trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "            \"reg_alpha\" : trial.suggest_float(\"reg_alpha\", 1e-6, 10, log=True),\n",
    "            \"reg_lambda\" : trial.suggest_float(\"reg_lambda\", 1e-6, 10, log=True),\n",
    "            \"min_data_in_leaf\" : trial.suggest_int(\"min_data_in_leaf\", 1, 100),\n",
    "            \"feature_fraction\" : trial.suggest_float(\"feature_fraction\", 0.5, 1.0),\n",
    "            # \"bagging_fraction\" : trial.suggest_float(\"bagging_fraction\", 0.5, 1.0),\n",
    "            # \"bagging_freq\" : trial.suggest_int(\"bagging_freq\", 1, 5),\n",
    "            \"max_bin\" : trial.suggest_int(\"max_bin\", 50, 5000, log=True),\n",
    "            # \"data_sample_strategy\" : \"bagging\",\n",
    "            'min_child_weight': trial.suggest_float('min_child_weight', 1e-4, 100, log=True),\n",
    "            # \"scale_pos_weight\" : trial.suggest_float(\"scale_pos_weight\", 1, 1000, log=True),\n",
    "            'cat_smooth': trial.suggest_float('cat_smooth', 1, 100, log=True),\n",
    "            'objective' : 'regression',\n",
    "            'metric' : 'rmse'}\n",
    "        return params\n",
    "\n",
    "\n",
    "class LGBMGBDTParamGrid(BaseLGBMParamGrid):\n",
    "    def get(self, trial : optuna.Trial):\n",
    "        base_params = super().get(trial)\n",
    "        params = {\n",
    "            \"boosting_type\" : \"gbdt\",\n",
    "            \"data_sample_strategy\" : \"bagging\"\n",
    "        }\n",
    "        params.update(base_params)\n",
    "        return params\n",
    "\n",
    "\n",
    "class LGBMGBDTGossParamGrid(BaseLGBMParamGrid):\n",
    "    def get(self, trial : optuna.Trial):\n",
    "        base_params = super().get(trial)\n",
    "        params = {\n",
    "            \"boosting_type\" : \"gbdt\",\n",
    "            \"data_sample_strategy\" : \"goss\"\n",
    "        }\n",
    "        params.update(base_params)\n",
    "        return params\n",
    "\n",
    "    \n",
    "class LGBMDARTParamGrid(BaseLGBMParamGrid):\n",
    "    def get(self, trial : optuna.Trial):\n",
    "        base_params = super().get(trial)\n",
    "        params = {\n",
    "            \"boosting_type\" : \"dart\",\n",
    "        }\n",
    "        params.update(base_params)\n",
    "        return params\n",
    "\n",
    "    \n",
    "class LGBMRFParamGrid(BaseLGBMParamGrid):\n",
    "    def get(self, trial : optuna.Trial):\n",
    "        base_params = super().get(trial)\n",
    "        params = {\n",
    "            \"boosting_type\" : \"rf\",\n",
    "            \"num_boost_round\" : 10000\n",
    "        }\n",
    "        params.update(base_params)\n",
    "        return params\n",
    "\n",
    "    \n",
    "\n",
    "class BaseXGBoostParamGrid():\n",
    "    @staticmethod\n",
    "    def get(trial : optuna.Trial):\n",
    "        params = {\n",
    "            # \"booster\": trial.suggest_categorical(\"booster\", [\"gbtree\", \"dart\"]),\n",
    "            # \"max_bin\" : trial.suggest_int(\"max_bin\", 50, 5000, log=True),\n",
    "            \"max_bin\" : 10000,\n",
    "            \"early_stopping_rounds\" : 20,\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-2, 0.3, log=True),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 50),\n",
    "            # \"num_boost_round\" : trial.suggest_int(\"num_boost_round\", 50, 3000),\n",
    "            \"num_boost_round\" : 10000,\n",
    "            \"gamma\" : trial.suggest_float(\"gamma\", 0, 10),\n",
    "            \"min_child_weight\" : trial.suggest_float(\"min_child_weight\", 0.1, 100, log=True),\n",
    "            \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "            \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "            \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.5, 1.0),\n",
    "            \"colsample_bynode\": trial.suggest_float(\"colsample_bynode\", 0.5, 1.0),\n",
    "            \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1e-6, 10.0, log=True),\n",
    "            \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-6, 10.0, log=True),\n",
    "            \"max_cat_threshold\" : trial.suggest_int(\"max_cat_threshold\", 1, 1000, log=True),\n",
    "            # \"scale_pos_weight\" : trial.suggest_float(\"scale_pos_weight\", 1, 1000, log=True),\n",
    "            \"sampling_method\" : \"uniform\",\n",
    "            # \"grow_policy\" : trial.suggest_categorical(\"grow_policy\", [\"lossguide\", \"depthwise\"]),\n",
    "            \"objective\": \"reg:squarederror\",\n",
    "            \"eval_metric\": \"rmse\",\n",
    "            \"tree_method\":\"gpu_hist\",\n",
    "            \"device\" : \"cuda\"\n",
    "        }\n",
    "        return params\n",
    "\n",
    "class XGBoostGBTreeLossguide(BaseXGBoostParamGrid):\n",
    "    def get(self, trial : optuna.Trial):\n",
    "        base_params = super().get(trial)\n",
    "        params = {\n",
    "            \"booster\" : \"gbtree\",\n",
    "            \"grow_policy\" : \"lossguide\"\n",
    "        }\n",
    "        params.update(base_params)\n",
    "        return params\n",
    "\n",
    "class XGBoostGBTreeDepthwise(BaseXGBoostParamGrid):\n",
    "    def get(self, trial : optuna.Trial):\n",
    "        base_params = super().get(trial)\n",
    "        params = {\n",
    "            \"booster\" : \"gbtree\",\n",
    "            \"grow_policy\" : \"depthwise\"\n",
    "        }\n",
    "        params.update(base_params)\n",
    "        return params\n",
    "\n",
    "\n",
    "class XGBoostGBTreeLinear(BaseXGBoostParamGrid):\n",
    "    def get(self, trial : optuna.Trial):\n",
    "        base_params = super().get(trial)\n",
    "        params = {\n",
    "            \"booster\" : \"gblinear\",\n",
    "        }\n",
    "        params.update(base_params)\n",
    "        return params\n",
    "\n",
    "    \n",
    "\n",
    "class XGBoostDART(BaseXGBoostParamGrid):\n",
    "    def get(self, trial : optuna.Trial):\n",
    "        base_params = super().get(trial)\n",
    "        params = {\n",
    "            \"booster\" : \"dart\",\n",
    "        }\n",
    "        params.update(base_params)\n",
    "        return params\n",
    "\n",
    "    \n",
    "\n",
    "class BaseCatBoostParamGrid():\n",
    "    @staticmethod\n",
    "    def get(trial : optuna.Trial):\n",
    "        params = {\n",
    "            # \"booster\": trial.suggest_categorical(\"booster\", [\"gbtree\", \"dart\"]),\n",
    "            \"early_stopping_rounds\" : 20, #: trial.suggest_int(\"early_stopping_rounds\", 1, 200, log=True),\n",
    "            \"max_bin\" : trial.suggest_int(\"max_bin\", 2, 5000, log=True),\n",
    "            # \"max_bin\" : 5000,\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.1, 0.3, log=True),\n",
    "            \"depth\": trial.suggest_int(\"depth\", 3, 16),\n",
    "            \"num_boost_round\" : 10000,\n",
    "#             trial.suggest_int(\"num_boost_round\", 50, 5000, log=True),\n",
    "            \"bagging_temperature\" : trial.suggest_float(\"bagging_temperature\", 0.1, 100, log=True),\n",
    "            \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "            \"colsample_bylevel\" : trial.suggest_float(\"colsample_bylevel\", 0.5, 1.0),\n",
    "            \"min_data_in_leaf\" : trial.suggest_float(\"min_data_in_leaf\", 1, 1000, log=True),\n",
    "            \"l2_leaf_reg\": trial.suggest_float(\"l2_leaf_reg\", 1e-6, 10.0, log=True),\n",
    "#             \"grow_policy\" : trial.suggest_categorical(\"grow_policy\", [\"Lossguide\", \"Depthwise\", \"SymmetricTree\"]),\n",
    "            \"leaf_estimation_iterations\" : trial.suggest_int(\"leaf_estimation_iterations\", 1, 5),\n",
    "            # \"scale_pos_weight\" : trial.suggest_float(\"scale_pos_weight\", 1, 1000, log=True),\n",
    "            \"random_strength\" : trial.suggest_float(\"random_strength\", 0.1, 10),\n",
    "            \"leaf_estimation_method\" : trial.suggest_categorical(\"leaf_estimation_method\", [\"Newton\", \"Gradient\"]),\n",
    "            \"loss_function\" : \"Cox\",\n",
    "            'eval_metric': 'Cox',\n",
    "            # 'task_type' : \"CPU\",\n",
    "        }\n",
    "        return params\n",
    "\n",
    "class GPUCatBoostParamGrid():\n",
    "    @staticmethod\n",
    "    def get(trial : optuna.Trial):\n",
    "        params = {\n",
    "            \"early_stopping_rounds\" : 20, #: trial.suggest_int(\"early_stopping_rounds\", 1, 200, log=True),\n",
    "            \"max_bin\" : trial.suggest_int(\"max_bin\", 2, 5000, log=True),\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-2, 0.3, log=True),\n",
    "            \"depth\": trial.suggest_int(\"depth\", 3, 16),\n",
    "            \"num_boost_round\" : 10000,\n",
    "            \"bagging_temperature\" : trial.suggest_float(\"bagging_temperature\", 0.1, 100, log=True),\n",
    "            # \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "            # \"colsample_bylevel\" : trial.suggest_float(\"colsample_bylevel\", 0.5, 1.0),\n",
    "            \"min_data_in_leaf\" : trial.suggest_float(\"min_data_in_leaf\", 1, 1000, log=True),\n",
    "            \"l2_leaf_reg\": trial.suggest_float(\"l2_leaf_reg\", 1e-6, 10.0, log=True),\n",
    "            \"leaf_estimation_iterations\" : trial.suggest_int(\"leaf_estimation_iterations\", 1, 5),\n",
    "            # \"scale_pos_weight\" : trial.suggest_float(\"scale_pos_weight\", 1, 1000, log=True),\n",
    "            # \"random_strength\" : trial.suggest_float(\"random_strength\", 0.1, 10),\n",
    "            \"leaf_estimation_method\" : trial.suggest_categorical(\"leaf_estimation_method\", [\"Newton\", \"Gradient\"]),\n",
    "            # 'loss_function' : \"Logloss\",\n",
    "            # 'eval_metric': 'AUC',\n",
    "            \"loss_function\" : \"RMSE\",\n",
    "            'eval_metric': 'RMSE',\n",
    "            'task_type' : \"GPU\",\n",
    "            'devices' : '0,1',\n",
    "            \"bootstrap_type\" : \"Bayesian\"\n",
    "        }\n",
    "        return params\n",
    "\n",
    "class CatBoostDepthWise(BaseCatBoostParamGrid):\n",
    "    def get(self, trial : optuna.Trial):\n",
    "        base_params = super().get(trial)\n",
    "        params = {\n",
    "            \"grow_policy\" : \"Depthwise\",\n",
    "        }\n",
    "        params.update(base_params)\n",
    "        return params\n",
    "\n",
    "    \n",
    "\n",
    "class CatBoostLossGuide(BaseCatBoostParamGrid):\n",
    "    def get(self, trial : optuna.Trial):\n",
    "        base_params = super().get(trial)\n",
    "        params = {\n",
    "            \"grow_policy\" : \"Lossguide\",\n",
    "        }\n",
    "        params.update(base_params)\n",
    "        return params\n",
    "\n",
    "    \n",
    "class CatBoostSymmetricTree(BaseCatBoostParamGrid):\n",
    "    def get(self, trial : optuna.Trial):\n",
    "        base_params = super().get(trial)\n",
    "        params = {\n",
    "            \"grow_policy\" : \"SymmetricTree\",\n",
    "        }\n",
    "        params.update(base_params)\n",
    "        return params\n",
    "\n",
    "\n",
    "\n",
    "class CatBoostRegion(BaseCatBoostParamGrid):\n",
    "    def get(self, trial : optuna.Trial):\n",
    "        base_params = super().get(trial)\n",
    "        params = {\n",
    "            \"grow_policy\" : \"Region\",\n",
    "            'task_type' : \"GPU\",\n",
    "        }\n",
    "        params.update(base_params)\n",
    "        return params\n",
    "\n",
    "    \n",
    "class HGBParamGrid():\n",
    "    @staticmethod\n",
    "    def get(trial : optuna.Trial):\n",
    "        params = {\n",
    "            \"max_bins\" : trial.suggest_int(\"max_bins\", 2, 255),\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-2, 0.3, log=True),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 50),\n",
    "            \"max_leaf_nodes\" : trial.suggest_int(\"max_leaf_nodes\", 2, 400, log=True),\n",
    "            \"min_samples_leaf\" : trial.suggest_int(\"min_samples_leaf\", 2, 500),\n",
    "            # \"num_boost_round\" : trial.suggest_int(\"num_boost_round\", 50, 3000),\n",
    "            \"num_boost_round\" : 3000,\n",
    "            \"early_stopping_rounds\" : 20,\n",
    "            \"validation_fraction\" : trial.suggest_float(\"validation_fraction\", 0.05, 0.2),\n",
    "            # \"early_stopping_rounds\" : trial.suggest_int(\"early_stopping_rounds\", 1, 200, log=True),\n",
    "            \"l2_regularization\": trial.suggest_float(\"l2_regularization\", 1e-6, 10.0, log=True),\n",
    "            \"max_features\": trial.suggest_float(\"max_features\", 0.5, 1.0),\n",
    "            \"interaction_cst\" : trial.suggest_categorical(\"interaction_cst\", [\"pairwise\", \"no_interactions\"]),\n",
    "            \"tol\": trial.suggest_float(\"tol\", 1e-7, 1e-2, log=True), \n",
    "            \"smooth\" : trial.suggest_float(\"smooth\", 1e2, 1e4, log=True),           \n",
    "        }\n",
    "        return params\n",
    "\n",
    "\n",
    "class YDFParamGrid():\n",
    "    @staticmethod\n",
    "    def get(trial : optuna.Trial):\n",
    "        params = {\n",
    "            \"categorical_algorithm\" : trial.suggest_categorical(\"categorical_algorithm\", [\"CART\", \"RANDOM\"]),\n",
    "            \"categorical_set_split_min_item_frequency\" : trial.suggest_int(\"categorical_set_split_min_item_frequency\", 1, 200),\n",
    "            # \"goss_alpha\" : trial.suggest_float(\"goss_alpha\", 0, 1),\n",
    "            # \"goss_beta\" : trial.suggest_float(\"goss_beta\", 0, 1),\n",
    "            # \"honest\" : trial.suggest_categorical(\"honest\", [True, False]),\n",
    "            \"l1_regularization\" : trial.suggest_float(\"l1_regularization\", 1e-4, 1e2, log=True),\n",
    "            \"l2_categorical_regularization\" : trial.suggest_float(\"l2_categorical_regularization\", 1e-4, 1e2, log=True),\n",
    "            \"l2_regularization\" : trial.suggest_float(\"l2_regularization\", 1e-4, 1e2, log=True),\n",
    "            \"max_depth\" : trial.suggest_int(\"max_depth\", -1, 300),\n",
    "            \"max_num_nodes\" : trial.suggest_int(\"max_num_nodes\", -1, 200),\n",
    "            \"min_examples\" : trial.suggest_int(\"min_examples\", 1, 1e3, log=True),\n",
    "            \"num_boost_round\" : 3000,\n",
    "            \"task\" : \"REGRESSION\",\n",
    "            \"loss\" : \"SQUARED_ERROR\"\n",
    "        }\n",
    "        return params\n",
    "    \n",
    "\n",
    "class YDFLocalParamGrid(YDFParamGrid):\n",
    "    def get(self, trial : optuna.Trial):\n",
    "        base_params = super().get(trial)\n",
    "        params = {\n",
    "            \"growing_strategy\" : \"LOCAL\",\n",
    "        }\n",
    "        params.update(base_params)\n",
    "        return params\n",
    "    \n",
    "class YDFBestGlobalParamGrid(YDFParamGrid):\n",
    "    def get(self, trial : optuna.Trial):\n",
    "        base_params = super().get(trial)\n",
    "        params = {\n",
    "            \"growing_strategy\" : \"BEST_FIRST_GLOBAL\",\n",
    "            \"subsample\" : trial.suggest_float(\"subsample\", 0.5, 1),\n",
    "        }\n",
    "        params.update(base_params)\n",
    "        return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2cd0284f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-31T18:09:12.846322Z",
     "iopub.status.busy": "2024-12-31T18:09:12.845888Z",
     "iopub.status.idle": "2024-12-31T18:09:20.135776Z",
     "shell.execute_reply": "2024-12-31T18:09:20.134373Z"
    },
    "papermill": {
     "duration": 7.303136,
     "end_time": "2024-12-31T18:09:20.138481",
     "exception": false,
     "start_time": "2024-12-31T18:09:12.835345",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/insurance-premium-prediction/README.md\n",
      "/kaggle/input/insurance-premium-prediction/Insurance Premium Prediction Dataset.csv\n",
      "/kaggle/input/autofe-feature-importance/robust_feature_importance.csv\n",
      "/kaggle/input/autofe-feature-importance/combined_df.csv\n",
      "/kaggle/input/insurance-dbs/cat_lg_with_original.db\n",
      "/kaggle/input/insurance-dbs/cat_lossguide.db\n",
      "/kaggle/input/insurance-dbs/cat_gpu.db\n",
      "/kaggle/input/insurance-dbs/cat_depthwise.db\n",
      "/kaggle/input/insurance-dbs/cat_symtree.db\n",
      "/kaggle/input/playground-series-s4e12/sample_submission.csv\n",
      "/kaggle/input/playground-series-s4e12/train.csv\n",
      "/kaggle/input/playground-series-s4e12/test.csv\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler, MinMaxScaler\n",
    "\n",
    "for root, dirs, files in os.walk(\"/kaggle/input/\"):\n",
    "    for file in files:\n",
    "        file_path = os.path.join(root, file)\n",
    "        print(file_path)\n",
    "        if \"train.csv\" in file_path:\n",
    "            train_csv_path = file_path\n",
    "        elif \"test.csv\" in file_path:\n",
    "            test_csv_path = file_path\n",
    "        elif \"sample\" in file_path:\n",
    "            sample_sub_csv_path = file_path\n",
    "target_col_name = pd.read_csv(train_csv_path).columns[-1]\n",
    "\n",
    "\n",
    "class FillNullValues():\n",
    "    @staticmethod\n",
    "    def transform(original_settings : DataSciencePipelineSettings, numeric_fill=-1, category_fill='missing'):\n",
    "        settings = deepcopy(original_settings)\n",
    "        for col_name in settings.training_col_names:\n",
    "            if pd.api.types.is_numeric_dtype(settings.combined_df[col_name]):\n",
    "                settings.combined_df[col_name] = settings.combined_df[col_name].fillna(numeric_fill)\n",
    "            else:\n",
    "                settings.combined_df[col_name] = settings.combined_df[col_name].fillna(category_fill)\n",
    "        return settings\n",
    "\n",
    "class ConvertAllToCategorical():\n",
    "    @staticmethod\n",
    "    def transform(original_settings : DataSciencePipelineSettings):\n",
    "        settings = deepcopy(original_settings)\n",
    "        cat_cols = settings.training_col_names\n",
    "        settings.combined_df[cat_cols] = settings.combined_df[cat_cols].astype(str).astype('category')\n",
    "        return settings\n",
    "\n",
    "class ConvertObjectToCategorical():\n",
    "    @staticmethod\n",
    "    def transform(original_settings : DataSciencePipelineSettings):\n",
    "        settings = deepcopy(original_settings)\n",
    "        cat_cols = settings.categorical_col_names\n",
    "        settings.combined_df[cat_cols] = settings.combined_df[cat_cols].astype('category')\n",
    "        return settings\n",
    "\n",
    "class ConvertObjectToStrCategorical():\n",
    "    @staticmethod\n",
    "    def transform(original_settings : DataSciencePipelineSettings):\n",
    "        settings = deepcopy(original_settings)\n",
    "        cat_cols = settings.categorical_col_names\n",
    "        settings.combined_df[cat_cols] = settings.combined_df[cat_cols].astype(str).astype('category')\n",
    "        return settings\n",
    "\n",
    "class LogTransformTarget():\n",
    "    @staticmethod\n",
    "    def transform(original_settings : DataSciencePipelineSettings):\n",
    "        settings = deepcopy(original_settings)\n",
    "        target = settings.target_col_name\n",
    "        settings.combined_df[target] = np.log1p(settings.combined_df[target])\n",
    "        return settings\n",
    "    \n",
    "class OrdinalEncode():\n",
    "    @staticmethod\n",
    "    def transform(original_settings : DataSciencePipelineSettings):\n",
    "        settings = deepcopy(original_settings)\n",
    "        train_df, test_df = settings.update()\n",
    "        ordinal_encoder = OrdinalEncoder(encoded_missing_value=-1, handle_unknown=\"use_encoded_value\", unknown_value=-1)\n",
    "        train_df[settings.categorical_col_names] = ordinal_encoder.fit_transform(train_df[settings.categorical_col_names])\n",
    "        test_df[settings.categorical_col_names] = ordinal_encoder.transform(test_df[settings.categorical_col_names])\n",
    "        settings.combined_df = pd.concat([train_df, test_df], keys=['train', 'test'])\n",
    "        settings.combined_df[settings.categorical_col_names] = settings.combined_df[settings.categorical_col_names].astype(int)\n",
    "        return settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e18691f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreateSurvivalTarget():\n",
    "    @staticmethod\n",
    "    def transform(original_settings : DataSciencePipelineSettings):\n",
    "        settings = deepcopy(original_settings)\n",
    "        settings.combined_df[\"survival_target\"] = np.where(settings.combined_df['efs'].astype(bool), \n",
    "                                                         settings.combined_df['efs_time'], \n",
    "                                                         -settings.combined_df['efs_time'])\n",
    "        return settings    \n",
    "\n",
    "settings = DataSciencePipelineSettings(train_csv_path,\n",
    "                                       test_csv_path,\n",
    "                                       target_col_name,\n",
    "                                       )\n",
    "transforms = [\n",
    "             FillNullValues.transform,\n",
    "             OrdinalEncode.transform,\n",
    "             ConvertObjectToStrCategorical.transform,\n",
    "             CreateSurvivalTarget.transform\n",
    "             ]\n",
    "\n",
    "settings = reduce(lambda acc, func: func(acc), transforms, settings)\n",
    "settings.update()\n",
    "\n",
    "train, test_df = settings.update()\n",
    "test_df.drop(columns=[target_col_name], inplace=True)\n",
    "X, y = train.drop(columns=[\"survival_target\"]), train[[\"survival_target\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bff3ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = [f for f in X.columns.tolist() if f not in [target_col_name, \"efs\"]]\n",
    "indices = np.array([X.columns.get_loc(col) for col in ['efs', 'efs_time', 'race_group']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0bd879",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_transform(input):\n",
    "    (X_test, y_pred) = input\n",
    "    X_test['predictions'] = y_pred\n",
    "    return X_test\n",
    "\n",
    "def sci_metric(y_test, y_processed):\n",
    "    if isinstance(y_processed, np.ndarray):\n",
    "        data = y_processed[:, indices]\n",
    "        solution = pd.DataFrame(columns=['efs', 'efs_time', 'race_group'], data=data)\n",
    "        predicted = y_processed[:, -1]\n",
    "    else:\n",
    "        solution = y_processed\n",
    "        predicted = y_processed['predictions']\n",
    "\n",
    "    metric_value = stratified_concordance_index(solution,\n",
    "                                                predicted,\n",
    "                                                'efs',\n",
    "                                                'efs_time',\n",
    "                                                'race_group')\n",
    "    return metric_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48b47ad3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-31T18:09:24.014002Z",
     "iopub.status.busy": "2024-12-31T18:09:24.013537Z",
     "iopub.status.idle": "2024-12-31T18:10:06.279242Z",
     "shell.execute_reply": "2024-12-31T18:10:06.277488Z"
    },
    "papermill": {
     "duration": 42.277894,
     "end_time": "2024-12-31T18:10:06.282084",
     "exception": false,
     "start_time": "2024-12-31T18:09:24.004190",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial memory usage: 298.08 MB\n",
      "Reduced memory usage: 230.46 MB\n",
      "Memory reduced by: 22.7%\n",
      "Initial memory usage: 214.50 MB\n",
      "Reduced memory usage: 180.93 MB\n",
      "Memory reduced by: 15.7%\n"
     ]
    }
   ],
   "source": [
    "model = CatBoostModel\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "optimizer = OptunaHyperparameterOptimizer(X,\n",
    "                                          y,\n",
    "                                          model,\n",
    "                                          CatBoostDepthWise(),\n",
    "                                          kf,\n",
    "                                          sci_metric,\n",
    "                                          training_features=train_features,\n",
    "                                          direction = 'maximize',\n",
    "                                          n_trials=100,\n",
    "                                          cross_validation_run_kwargs={'output_transform_list': [output_transform]}\n",
    "                                          study_name = 'cat_dw',\n",
    "                                          random_state=42)\n",
    "\n",
    "best_params = optimizer.optimize(timeout=3600*10)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 10305135,
     "sourceId": 84896,
     "sourceType": "competition"
    },
    {
     "datasetId": 5547076,
     "sourceId": 9178166,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6332367,
     "sourceId": 10243674,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6368943,
     "sourceId": 10290990,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30804,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1374.592377,
   "end_time": "2024-12-31T18:31:42.059087",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-12-31T18:08:47.466710",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
