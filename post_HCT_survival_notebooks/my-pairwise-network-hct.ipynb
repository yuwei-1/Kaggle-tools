{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2025-03-05T00:04:47.632019Z","iopub.status.busy":"2025-03-05T00:04:47.631753Z","iopub.status.idle":"2025-03-05T00:05:05.024018Z","shell.execute_reply":"2025-03-05T00:05:05.023322Z","shell.execute_reply.started":"2025-03-05T00:04:47.631995Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import torch\n","from sklearn.impute import SimpleImputer\n","from sklearn.preprocessing import LabelEncoder, StandardScaler\n","from torch.utils.data import TensorDataset\n","import json\n","import pytorch_lightning as pl\n","import numpy as np, pandas as pd\n","import matplotlib.pyplot as plt\n","import torch\n","import torch.nn as nn\n","from pytorch_lightning.callbacks import LearningRateMonitor, TQDMProgressBar, EarlyStopping, StochasticWeightAveraging\n","from sklearn.model_selection import StratifiedKFold\n","import random\n","import sys\n","# sys.path.append(\"/kaggle/input/\")\n","\n","from functools import reduce\n","from ktools_utils import *\n","from functools import reduce\n","from copy import deepcopy"]},{"cell_type":"markdown","metadata":{},"source":["## Prepare data\n","\n","Below are a few utility functions to load and prepare the data for training with pytorch."]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2025-03-05T00:05:05.025966Z","iopub.status.busy":"2025-03-05T00:05:05.025419Z","iopub.status.idle":"2025-03-05T00:05:05.029555Z","shell.execute_reply":"2025-03-05T00:05:05.028945Z","shell.execute_reply.started":"2025-03-05T00:05:05.025940Z"},"trusted":true},"outputs":[],"source":["RANDOM_SEED = 32\n","ORIGINAL_DATA=False\n","\n","train_csv_path = \"../data/post_hct_survival/train.csv\"\n","test_csv_path = \"../data/post_hct_survival/test.csv\"\n","sub_csv_path = \"../data/post_hct_survival/sample_submission.csv\"\n","\n","# train_csv_path = \"/kaggle/input/equity-post-HCT-survival-predictions/train.csv\"\n","# test_csv_path = \"/kaggle/input/equity-post-HCT-survival-predictions/test.csv\"\n","# sub_csv_path = \"/kaggle/input/equity-post-HCT-survival-predictions/sample_submission.csv\"\n","\n","target_col_name = ['efs', 'efs_time']"]},{"cell_type":"code","execution_count":45,"metadata":{"execution":{"iopub.execute_input":"2025-03-05T00:05:05.030838Z","iopub.status.busy":"2025-03-05T00:05:05.030489Z","iopub.status.idle":"2025-03-05T00:05:05.051776Z","shell.execute_reply":"2025-03-05T00:05:05.051198Z","shell.execute_reply.started":"2025-03-05T00:05:05.030803Z"},"trusted":true},"outputs":[],"source":["def get_X_cat(df, cat_cols, transformers=None):\n","    \"\"\"\n","    Apply a specific categorical data transformer or a LabelEncoder if None.\n","    \"\"\"\n","    if transformers is None:\n","        transformers = [LabelEncoder().fit(df[col]) for col in cat_cols]\n","    return transformers, np.array(\n","        [transformer.transform(df[col]) for col, transformer in zip(cat_cols, transformers)]\n","    ).T\n","\n","\n","def preprocess_data(train, val, numericals=None, categoricals=None):\n","    \"\"\"\n","    Standardize numerical variables and transform (Label-encode) categoricals.\n","    Fill NA values with mean for numerical.\n","    Create torch dataloaders to prepare data for training and evaluation.\n","    \"\"\"\n","    train = add_features(train)\n","    val = add_features(val)\n","    X_cat_train, X_cat_val, numerical, categorical_cols, transformers = get_categoricals(train, val)\n","    numerical = numerical if numericals is None else numericals\n","    scaler = StandardScaler()\n","    imp = SimpleImputer(missing_values=np.nan, strategy='mean', add_indicator=True)\n","    X_num_train = imp.fit_transform(train[numerical])\n","    X_num_train = scaler.fit_transform(X_num_train)\n","    X_num_val = imp.transform(val[numerical])\n","    X_num_val = scaler.transform(X_num_val)\n","    dl_train = init_dl(X_cat_train, X_num_train, train, training=True)\n","    dl_val = init_dl(X_cat_val, X_num_val, val)\n","\n","    # train[categorical_cols] = X_cat_train\n","    # train[numerical] = X_num_train\n","    # val[categorical_cols] = X_cat_val\n","    # val[numerical] = X_num_val\n","\n","    return X_cat_train, X_cat_val, X_num_train, X_num_val, dl_train, dl_val, transformers\n","\n","\n","def get_categoricals(train, val):\n","    \"\"\"\n","    Remove constant categorical columns and transform them using LabelEncoder.\n","    Return the label-transformers for each categorical column, categorical dataframes and numerical columns.\n","    \"\"\"\n","    categorical_cols, numerical = get_feature_types(train)\n","    remove = []\n","    for col in categorical_cols:\n","        if train[col].nunique() == 1:\n","            remove.append(col)\n","        ind = ~val[col].isin(train[col])\n","        if ind.any():\n","            val.loc[ind, col] = np.nan\n","    categorical_cols = [col for col in categorical_cols if col not in remove]\n","    transformers, X_cat_train = get_X_cat(train, categorical_cols)\n","    _, X_cat_val = get_X_cat(val, categorical_cols, transformers)\n","    return X_cat_train, X_cat_val, numerical, categorical_cols, transformers\n","\n","\n","def init_dl(X_cat, X_num, df, training=False):\n","    \"\"\"\n","    Initialize data loaders with 4 dimensions : categorical dataframe, numerical dataframe and target values (efs and efs_time).\n","    Notice that efs_time is log-transformed.\n","    Fix batch size to 2048 and return dataloader for training or validation depending on training value.\n","    \"\"\"\n","    ds_train = TensorDataset(\n","        torch.tensor(X_cat, dtype=torch.long),\n","        torch.tensor(X_num, dtype=torch.float32),\n","        torch.tensor(df.efs_time.values, dtype=torch.float32).log(),\n","        torch.tensor(df.efs.values, dtype=torch.long)\n","    )\n","    bs = 2048\n","    dl_train = torch.utils.data.DataLoader(ds_train, batch_size=bs, pin_memory=True, shuffle=training)\n","    return dl_train\n","\n","\n","def get_feature_types(train):\n","    \"\"\"\n","    Utility function to return categorical and numerical column names.\n","    \"\"\"\n","    categorical_cols = [col for i, col in enumerate(train.columns) if ((train[col].dtype == \"object\") | (2 < train[col].nunique() < 25))]\n","    RMV = [\"ID\", \"efs\", \"efs_time\", \"y\"]\n","    FEATURES = [c for c in train.columns if not c in RMV]\n","    print(f\"There are {len(FEATURES)} FEATURES: {FEATURES}\")\n","    numerical = [i for i in FEATURES if i not in categorical_cols]\n","    return categorical_cols, numerical\n","\n","\n","def add_features(df):\n","    \"\"\"\n","    Create some new features to help the model focus on specific patterns.\n","    \"\"\"\n","    df['is_cyto_score_same'] = (df['cyto_score'] == df['cyto_score_detail']).astype(int)\n","    df['year_hct'] -= 2000\n","    \n","    return df\n","\n","\n","def load_data():\n","    \"\"\"\n","    Load data and add features.\n","    \"\"\"\n","    test = pd.read_csv(test_csv_path)\n","    test = add_features(test)\n","    print(\"Test shape:\", test.shape)\n","    train = pd.read_csv(train_csv_path)\n","    train = add_features(train)\n","    print(\"Train shape:\", train.shape)\n","    return test, train"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2025-03-05T00:05:05.052666Z","iopub.status.busy":"2025-03-05T00:05:05.052481Z","iopub.status.idle":"2025-03-05T00:05:05.075982Z","shell.execute_reply":"2025-03-05T00:05:05.075389Z","shell.execute_reply.started":"2025-03-05T00:05:05.052650Z"},"trusted":true},"outputs":[],"source":["from typing import List\n","\n","\n","def init_ktools_dl(X : pd.DataFrame, y : pd.DataFrame, categorical_idcs : List, numerical_idcs : List, training=False):\n","    \"\"\"\n","    Initialize data loaders with 4 dimensions : categorical dataframe, numerical dataframe and target values (efs and efs_time).\n","    Notice that efs_time is log-transformed.\n","    Fix batch size to 2048 and return dataloader for training or validation depending on training value.\n","    \"\"\"\n","\n","    X_cat = X.iloc[:, categorical_idcs].values\n","    X_num = X.iloc[:, numerical_idcs].values\n","    ds_train = TensorDataset(\n","        torch.tensor(X_cat, dtype=torch.long),\n","        torch.tensor(X_num, dtype=torch.float32),\n","        torch.tensor(y.efs_time.values, dtype=torch.float32).log(),\n","        torch.tensor(y.efs.values, dtype=torch.long)\n","    )\n","    bs = 2048\n","    dl_train = torch.utils.data.DataLoader(ds_train, batch_size=bs, pin_memory=True, shuffle=training)\n","    return dl_train, X_cat, X_num"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2025-03-05T00:05:05.076928Z","iopub.status.busy":"2025-03-05T00:05:05.076695Z","iopub.status.idle":"2025-03-05T00:05:05.099952Z","shell.execute_reply":"2025-03-05T00:05:05.099195Z","shell.execute_reply.started":"2025-03-05T00:05:05.076908Z"},"trusted":true},"outputs":[],"source":["def get_cats():\n","    df = pd.read_csv(train_csv_path)\n","    cats = [col for col in df.columns if (2 < df[col].nunique() < 25) | (df[col].dtype == 'object')]\n","    return cats\n","# categoricals = get_cats()"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2025-03-05T00:05:05.100902Z","iopub.status.busy":"2025-03-05T00:05:05.100643Z","iopub.status.idle":"2025-03-05T00:05:05.117417Z","shell.execute_reply":"2025-03-05T00:05:05.116836Z","shell.execute_reply.started":"2025-03-05T00:05:05.100881Z"},"trusted":true},"outputs":[],"source":["from lifelines.utils import concordance_index\n","\n","def score(solution: pd.DataFrame, submission: pd.DataFrame, row_id_column_name: str) -> float:\n","    \"\"\"\n","    >>> import pandas as pd\n","    >>> row_id_column_name = \"id\"\n","    >>> y_pred = {'prediction': {0: 1.0, 1: 0.0, 2: 1.0}}\n","    >>> y_pred = pd.DataFrame(y_pred)\n","    >>> y_pred.insert(0, row_id_column_name, range(len(y_pred)))\n","    >>> y_true = { 'efs': {0: 1.0, 1: 0.0, 2: 0.0}, 'efs_time': {0: 25.1234,1: 250.1234,2: 2500.1234}, 'race_group': {0: 'race_group_1', 1: 'race_group_1', 2: 'race_group_1'}}\n","    >>> y_true = pd.DataFrame(y_true)\n","    >>> y_true.insert(0, row_id_column_name, range(len(y_true)))\n","    >>> score(y_true.copy(), y_pred.copy(), row_id_column_name)\n","    0.75\n","    \"\"\"\n","    \n","    del solution[row_id_column_name]\n","    del submission[row_id_column_name]\n","    \n","    event_label = 'efs'\n","    interval_label = 'efs_time'\n","    prediction_label = 'prediction'\n","    # Merging solution and submission dfs on ID\n","    merged_df = pd.concat([solution, submission], axis=1)\n","    merged_df.reset_index(inplace=True)\n","    merged_df_race_dict = dict(merged_df.groupby(['race_group']).groups)\n","    metric_list = []\n","    for race in merged_df_race_dict.keys():\n","        # Retrieving values from y_test based on index\n","        indices = sorted(merged_df_race_dict[race])\n","        merged_df_race = merged_df.iloc[indices]\n","        # Calculate the concordance index\n","        c_index_race = concordance_index(\n","                        merged_df_race[interval_label],\n","                        -merged_df_race[prediction_label],\n","                        merged_df_race[event_label])\n","        metric_list.append(c_index_race)\n","    return float(np.mean(metric_list)-np.sqrt(np.var(metric_list)))\n","\n","\n","def scci_metric(y_test, y_pred, id_col_name : str = \"ID\",\n","               survived_col_name : str = \"efs\",\n","               survival_time_col_name : str = \"efs_time\",\n","               stratify_col_name : str = \"race_group\"):\n","    idcs = y_test.index\n","    og_train = pd.read_csv(train_csv_path)\n","    \n","    y_true = og_train.loc[idcs, [id_col_name, survived_col_name, survival_time_col_name, stratify_col_name]].copy()\n","    y_pred_df = og_train.loc[idcs, [id_col_name]].copy()\n","    y_pred_df[\"prediction\"] = y_pred\n","    scci = score(y_true.copy(), y_pred_df.copy(), id_col_name)\n","    return scci"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2025-03-05T00:05:05.119538Z","iopub.status.busy":"2025-03-05T00:05:05.119351Z","iopub.status.idle":"2025-03-05T00:05:05.136022Z","shell.execute_reply":"2025-03-05T00:05:05.135466Z","shell.execute_reply.started":"2025-03-05T00:05:05.119522Z"},"trusted":true},"outputs":[],"source":["# # from sklearn.metrics import accuracy_score\n","# # from ktools.fitting.cross_validation_executor import CrossValidationExecutor\n","# # from ktools.modelling.ktools_models.xgb_model import XGBoostModel\n","# # from ktools.modelling.model_transform_wrappers.survival_model_wrapper import transform_quantile\n","\n","\n","# settings = DataSciencePipelineSettings(train_csv_path,\n","#                                         test_csv_path,\n","#                                         target_col_name,\n","#                                         categorical_col_names=categoricals\n","#                                         )\n","\n","class AddHCTFeatures():\n","    @staticmethod\n","    def transform(original_settings : DataSciencePipelineSettings):\n","        settings = deepcopy(original_settings)\n","        settings.combined_df['is_cyto_score_same'] = (settings.combined_df['cyto_score'] == settings.combined_df['cyto_score_detail']).astype(int)\n","        settings.combined_df['year_hct'] -= 2000\n","        settings.training_col_names += ['is_cyto_score_same']\n","        settings.numerical_col_names += ['is_cyto_score_same']\n","        return settings\n","\n","class ImputeNumericalAddIndicator():\n","    @staticmethod\n","    def transform(original_settings : DataSciencePipelineSettings, imputation_strategy='mean', add_indicator=True):\n","        settings = deepcopy(original_settings)\n","        train_df, test_df = settings.update()\n","        added_cols = []\n","        for col_name in settings.numerical_col_names:\n","            imputer = SimpleImputer(strategy=imputation_strategy, add_indicator=add_indicator)\n","            train_transformed = imputer.fit_transform(train_df[[col_name]])\n","            test_transformed = imputer.transform(test_df[[col_name]])\n","            if train_transformed.shape[1] > 1:\n","                nan_col_name = col_name + \"_nan\"\n","                settings.combined_df.loc['train', [col_name, nan_col_name]] = train_transformed\n","                settings.combined_df.loc['test', [col_name, nan_col_name]] = test_transformed\n","                settings.training_col_names += [nan_col_name]\n","                added_cols += [nan_col_name]\n","            else:\n","                settings.combined_df.loc['train', col_name] = train_transformed\n","                settings.combined_df.loc['test', col_name] = test_transformed\n","        settings.numerical_col_names.extend(added_cols)\n","        return settings\n","\n","\n","class NanUnknownCategoricals():\n","    @staticmethod\n","    def transform(original_settings : DataSciencePipelineSettings):\n","        settings = deepcopy(original_settings)\n","        train_df, test_df = settings.update()\n","        for col in settings.categorical_col_names:\n","            mask = ~test_df[col].isin(train_df[col].unique())\n","            test_df.loc[mask, col] = np.nan\n","        settings.combined_df = pd.concat([train_df, test_df], keys=['train', 'test'])\n","        return settings\n","    \n","transforms = [\n","            AddHCTFeatures.transform,\n","            ImputeNumericalAddIndicator.transform,\n","            StandardScaleNumerical.transform,\n","            NanUnknownCategoricals.transform,\n","            FillNullValues.transform,\n","            OrdinalEncode.transform,\n","            ConvertObjectToCategorical.transform,\n","            # AddOOFFeatures.transform\n","            ]\n","\n","# settings = reduce(lambda acc, func: func(acc), transforms, settings)\n","# train, test_df = settings.update()\n","\n","# test_df.drop(columns=target_col_name, inplace=True)\n","# X, y = train.drop(columns=settings.target_col_name), train[settings.target_col_name]"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2025-03-05T00:05:05.137595Z","iopub.status.busy":"2025-03-05T00:05:05.137377Z","iopub.status.idle":"2025-03-05T00:05:05.160805Z","shell.execute_reply":"2025-03-05T00:05:05.160203Z","shell.execute_reply.started":"2025-03-05T00:05:05.137572Z"},"trusted":true},"outputs":[],"source":["# cat_names = settings.categorical_col_names\n","# cat_sizes = [int(x) for x in X[cat_names].nunique().values]\n","# emb_sizes = [16] * len(cat_sizes)\n","\n","# # emb_sizes = [min(3*n, 16) for n in cat_sizes]\n","# categorical_idcs = [X.columns.get_loc(col) for col in cat_names]\n","# numerical_idcs = list(set(range(X.shape[1])).difference(set(categorical_idcs)))\n","# race_idx = X[cat_names].columns.get_loc('race_group')"]},{"cell_type":"markdown","metadata":{},"source":["## Define models with pairwise ranking loss\n","\n","The model is defined in 3 steps :\n","* Embedding class for categorical data\n","* MLP for numerical and categorical data\n","* Final model trained with pairwise ranking loss with selection of valid pairs"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2025-03-05T00:05:05.161787Z","iopub.status.busy":"2025-03-05T00:05:05.161503Z","iopub.status.idle":"2025-03-05T00:05:05.179995Z","shell.execute_reply":"2025-03-05T00:05:05.179408Z","shell.execute_reply.started":"2025-03-05T00:05:05.161741Z"},"trusted":true},"outputs":[],"source":["import functools\n","import torch.nn.functional as F\n","\n","@functools.lru_cache\n","def combinations(N):\n","    with torch.no_grad():\n","        ind = torch.arange(N)\n","        comb = torch.combinations(ind, r=2)\n","    return comb\n","\n","def pairwise_loss(event :torch.Tensor, event_time:torch.Tensor, risk:torch.Tensor, margin=0.2):\n","    n = event.shape[0]\n","    pairwise_combinations = combinations(n)\n","\n","    # Find mask\n","    pairwise_combinations = pairwise_combinations.clone().detach()\n","    first_of_pair, second_of_pair = pairwise_combinations.T\n","    valid_mask = False\n","    valid_mask |= ((event[first_of_pair] == 1) & (event[second_of_pair] == 1))\n","    valid_mask |= ((event[first_of_pair] == 1) & (event_time[first_of_pair] < event_time[second_of_pair]))\n","    valid_mask |= ((event[second_of_pair] == 1) & (event_time[second_of_pair] < event_time[first_of_pair]))\n","\n","    direction = 2*(event_time[first_of_pair] > event_time[second_of_pair]).int() - 1\n","    margin_loss = F.relu(-direction*(risk[first_of_pair] - risk[second_of_pair]) + margin)\n","    return (margin_loss.double()*valid_mask.double()).sum()/valid_mask.sum()\n","\n","\n","def race_equality_loss(race, event, event_time, risk, margin=0.2):\n","    unq_races, _ = torch.unique(race, return_counts=True)\n","    race_specific_loss = torch.zeros(len(unq_races), dtype=torch.double).to(race.device)\n","    for i, r in enumerate(unq_races):\n","        idcs = race == r\n","        race_specific_loss[i] = pairwise_loss(event[idcs], event_time[idcs], risk[idcs], margin=margin)\n","    return torch.std(race_specific_loss)"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2025-03-05T00:05:05.181089Z","iopub.status.busy":"2025-03-05T00:05:05.180816Z","iopub.status.idle":"2025-03-05T00:05:05.204966Z","shell.execute_reply":"2025-03-05T00:05:05.204357Z","shell.execute_reply.started":"2025-03-05T00:05:05.181061Z"},"trusted":true},"outputs":[],"source":["from collections import defaultdict\n","from torch.nn.modules import Module\n","from lifelines.utils import concordance_index\n","import pytorch_lightning as pl\n","import torch.nn as nn\n","import torch\n","from typing import *\n","from abc import ABC, abstractmethod\n","from collections import defaultdict\n","\n","\n","class KtoolsBaseLightningmodel(pl.LightningModule):\n","    \"\"\"\n","    Main Model creation and losses definition to fully train the model.\n","    \"\"\"\n","    def __init__(\n","            self,\n","            model : nn.Module,\n","            learning_rate : float,\n","            weight_decay : float,\n","    ):\n","        super(KtoolsBaseLightningmodel, self).__init__()\n","        self.model = model\n","        self._learning_rate = learning_rate\n","        self._weight_decay = weight_decay\n","        self.global_metrics = defaultdict(list)\n","\n","    def forward(self, x_cat, x_cont):\n","        return self.model(x_cat, x_cont)\n","    \n","    @abstractmethod\n","    def get_loss(self, batch, mode : str):\n","        assert mode in {'train', 'valid', 'test'}\n","        pass\n","\n","    @abstractmethod\n","    def get_global_metrics(self):\n","        pass\n","\n","    def training_step(self, batch, batch_idx):\n","        total_loss, loss_dict, batch_metrics = self.get_loss(batch, mode='train')\n","        for (k, v) in loss_dict.items():\n","            self.log(k, v, on_epoch=True, prog_bar=True, logger=True)\n","        return total_loss\n","\n","    def validation_step(self, batch, batch_idx):\n","        total_loss, loss_dict, batch_metrics = self.get_loss(batch, mode='valid')\n","        if batch_idx == 0:\n","            self.global_metrics.update(batch_metrics)\n","        else:\n","            for (k, v) in batch_metrics.items():\n","                self.global_metrics[k] += v\n","        for (k, v) in loss_dict.items():\n","            self.log(k, v, on_epoch=True, prog_bar=True, logger=True)\n","        return total_loss\n","    \n","    def test_step(self, batch, batch_idx):\n","        total_loss, loss_dict, batch_metrics = self.get_loss(batch, mode='test')\n","        if batch_idx == 0:\n","            self.global_metrics.update(batch_metrics)\n","        else:\n","            for (k, v) in batch_metrics.items():\n","                self.global_metrics[k] += v\n","        for (k, v) in loss_dict.items():\n","            self.log(k, v, on_epoch=True, prog_bar=True, logger=True)\n","        return total_loss\n","    \n","    def on_validation_epoch_end(self) -> None:\n","        metric_dict = self.get_global_metrics()\n","        for (k, v) in metric_dict.items():\n","            self.log(k, v, on_epoch=True, prog_bar=True, logger=True)\n","\n","    def on_test_epoch_end(self) -> None:\n","        metric_dict = self.get_global_metrics()\n","        for (k, v) in metric_dict.items():\n","            self.log(k, v, on_epoch=True, prog_bar=True, logger=True)\n","\n","    def configure_optimizers(self):\n","\n","        optimizer = torch.optim.Adam(self.parameters(), lr=self._learning_rate, weight_decay=self._weight_decay)\n","        scheduler_config = {\n","            \"scheduler\": torch.optim.lr_scheduler.CosineAnnealingLR(\n","                optimizer,\n","                T_max=1,\n","                eta_min=6e-3\n","            ),\n","            \"interval\": \"epoch\",\n","            \"frequency\": 1,\n","            \"strict\": False,\n","        }\n","\n","        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler_config}\n","\n","class NonLinearFeedForwardModule(nn.Module):\n","\n","    def __init__(self,\n","                 input_dim : int,\n","                 hidden_dim : int,\n","                 output_dim : int,\n","                 activation : str = 'gelu'):\n","        \n","        super(NonLinearFeedForwardModule, self).__init__()\n","        self.ffm = nn.Sequential(\n","            nn.Linear(input_dim, hidden_dim),\n","            get_activation(activation),\n","            nn.Linear(hidden_dim, output_dim)\n","        )\n","\n","    def forward(self, x : torch.Tensor):\n","        return self.ffm(x)\n","\n","class PostHCTModel(KtoolsBaseLightningmodel):\n","\n","    def __init__(self, \n","                 model: Module,\n","                 race_index : int, \n","                 learning_rate: float = 0.06464861983337984, \n","                 weight_decay: float = 0.0002773544957610778,\n","                 margin : float = 0.2588153271003354):\n","        \n","        super(PostHCTModel, self).__init__(model, learning_rate, weight_decay)\n","        self._race_index = race_index\n","        self._margin = margin\n","\n","        hidden_size = 56\n","        self.aux_predictor = NonLinearFeedForwardModule(hidden_size,\n","                                                        hidden_size//3,\n","                                                        1\n","                                                        )\n","\n","    def get_loss(self, batch, mode = None):\n","        x_cat, x_num, efs_time, efs = batch\n","        risk, emb = self(x_cat, x_num)\n","        aux_pred = self.aux_predictor(emb)\n","\n","        risk = risk.squeeze()\n","        aux_pred = aux_pred.squeeze()\n","\n","        pwloss = pairwise_loss(efs, efs_time, risk, margin=self._margin)\n","        race_loss = 0.1*race_equality_loss(x_cat[:, self._race_index], efs, efs_time, risk, margin=self._margin)\n","\n","        aux_mask = efs == 1\n","        # print(aux_pred, efs_time)\n","        aux_loss = F.mse_loss(aux_pred, efs_time, reduction='none')\n","        efs_time_loss = 0.26545778308743806*(aux_loss.double() * aux_mask.double()).sum()/aux_mask.sum()\n","\n","\n","        loss_dict = {f'{mode}_pairwise_loss' : pwloss,\n","                     f'{mode}_efs_time_loss' : efs_time_loss,\n","                     f'{mode}_race_std_loss' : race_loss}\n","        \n","        batch_dict = {'efs_time' : [efs_time],\n","                      'efs' : [efs],\n","                      'risk_score' : [risk.squeeze()],\n","                      'races' : [x_cat[:, self._race_index]]}\n","        \n","        return pwloss + efs_time_loss + race_loss, loss_dict, batch_dict\n","    \n","    def get_global_metrics(self):\n","        efs = torch.cat(self.global_metrics['efs']).cpu().numpy()\n","        y_hat = torch.cat(self.global_metrics['risk_score']).cpu().numpy()\n","        efs_time = torch.cat(self.global_metrics['efs_time']).cpu().numpy()\n","        races = torch.cat(self.global_metrics['races']).cpu().numpy()\n","        self.eval_preds = y_hat.copy()\n","        self.global_metrics.clear()\n","\n","        try:\n","            metric = self._metric(efs, races, efs_time, y_hat)\n","            cindex = concordance_index(efs_time, y_hat, efs)\n","        except:\n","            metric = 0.5\n","            cindex = 0.5\n","        return {'stratified concordance index' : metric, 'basic_concordance_index' : cindex}\n","    \n","    def _metric(self, efs, races, y, y_hat):\n","        metric_list = []\n","        for race in np.unique(races):\n","            y_ = y[races == race]\n","            y_hat_ = y_hat[races == race]\n","            efs_ = efs[races == race]\n","            metric_list.append(concordance_index(y_, y_hat_, efs_))\n","        metric = float(np.mean(metric_list) - np.sqrt(np.var(metric_list)))\n","        return metric\n","    \n","    def configure_optimizers(self):\n","\n","        optimizer = torch.optim.Adam(self.parameters(), lr=self._learning_rate, weight_decay=self._weight_decay)\n","        scheduler_config = {\n","            \"scheduler\": torch.optim.lr_scheduler.CosineAnnealingLR(\n","                optimizer,\n","                T_max=45,\n","                eta_min=6e-3\n","            ),\n","            \"interval\": \"epoch\",\n","            \"frequency\": 1,\n","            \"strict\": False,\n","        }\n","\n","        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler_config}\n","    \n","    def predict_step(self, batch, batch_idx):\n","        \"\"\"Unpacks the batch correctly for inference.\"\"\"\n","        x_cat, x_cont = batch\n","        risk, _ = self(x_cat, x_cont)\n","        print(risk)\n","        return risk\n","    \n","    def on_test_epoch_end(self) -> None:\n","        metric_dict = self.get_global_metrics()\n","        for (k, v) in metric_dict.items():\n","            self.log(k, v, on_epoch=True, prog_bar=True, logger=True)"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2025-03-05T00:05:05.205923Z","iopub.status.busy":"2025-03-05T00:05:05.205641Z","iopub.status.idle":"2025-03-05T00:05:05.228181Z","shell.execute_reply":"2025-03-05T00:05:05.227590Z","shell.execute_reply.started":"2025-03-05T00:05:05.205895Z"},"trusted":true},"outputs":[],"source":["def ktools_preprocess_data(train, test):\n","    categoricals = get_cats()\n","    settings = DataSciencePipelineSettings(train_csv_path,\n","                                            test_csv_path,\n","                                            target_col_name,\n","                                            categorical_col_names=categoricals,\n","                                            train_data=train,\n","                                            test_data=test\n","                                            )\n","    settings = reduce(lambda acc, func: func(acc), transforms, settings)\n","    return settings.get_data(), settings"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2025-03-05T00:05:05.229240Z","iopub.status.busy":"2025-03-05T00:05:05.228960Z","iopub.status.idle":"2025-03-05T00:05:05.251043Z","shell.execute_reply":"2025-03-05T00:05:05.250457Z","shell.execute_reply.started":"2025-03-05T00:05:05.229219Z"},"trusted":true},"outputs":[],"source":["def eval_dl(X_cat, X_num):\n","    ds_train = TensorDataset(\n","        torch.tensor(X_cat, dtype=torch.long),\n","        torch.tensor(X_num, dtype=torch.float32),\n","        torch.randn(X_cat.shape[0], dtype=torch.float32),\n","        torch.ones(X_cat.shape[0], dtype=torch.long)\n","    )\n","    bs = 2048\n","    dl = torch.utils.data.DataLoader(ds_train, batch_size=bs, pin_memory=True)\n","    return dl"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2025-03-05T00:05:05.251994Z","iopub.status.busy":"2025-03-05T00:05:05.251656Z","iopub.status.idle":"2025-03-05T00:05:08.043727Z","shell.execute_reply":"2025-03-05T00:05:08.043130Z","shell.execute_reply.started":"2025-03-05T00:05:05.251972Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from typing import *\n","from pytorch_tabular.models.common.layers import ODST\n","\n","\n","class EmbeddingCategoricalModule(nn.Module):\n","    \"\"\"\n","    Embed categorical feature\n","    \"\"\"\n","\n","    def __init__(self,\n","                 category_cardinalities : List[str],\n","                 embedding_sizes : List[str],\n","                 projection_dim : Union[None, int] = None) -> None:\n","        \n","        super(EmbeddingCategoricalModule, self).__init__()\n","        self._category_cardinalities = category_cardinalities\n","        self._embedding_sizes = embedding_sizes\n","        self._embedding_layers = self._build_embedding_layers()\n","\n","        if projection_dim is not None:\n","            self.mlp = nn.Linear(self.concatenated_len, projection_dim)\n","        else:\n","            self.mlp = nn.Identity()\n","\n","    def forward(self, x_cat : torch.Tensor) -> torch.Tensor:\n","        x = [embedder(x_cat[:, i]) for i, embedder in enumerate(self._embedding_layers)]\n","        x = torch.cat(x, dim=1)\n","        x = self.mlp(x)\n","        return x\n","\n","    @property\n","    def num_features(self):\n","        return len(self._embedding_sizes)\n","    \n","    @property\n","    def concatenated_len(self):\n","        return sum(self._embedding_sizes)\n","\n","    def _build_embedding_layers(self):\n","        embedding_layers = nn.ModuleList([\n","            nn.Embedding(self._category_cardinalities[i], self._embedding_sizes[i]) for i in range(self.num_features)\n","            ])\n","        return embedding_layers\n","\n","class IskanderPairwiseNetwork(nn.Module):\n","\n","    def __init__(self,\n","                category_cardinalities : List[str],\n","                numerical_size : int,\n","                embedding_sizes : List[str],\n","                embedding_projected_dim : int = 112,\n","                hidden_size : int = 56,\n","                output_size : int = 1,\n","                dropout : float = 0.05463240181423116\n","                ):\n","        \n","        super(IskanderPairwiseNetwork, self).__init__()\n","        self.embedding_module = EmbeddingCategoricalModule(category_cardinalities,\n","                                                           embedding_sizes)\n","        cat_dim = self.embedding_module.concatenated_len\n","        \n","        self.project_embeddings = NonLinearFeedForwardModule(cat_dim, \n","                                                            embedding_projected_dim,\n","                                                            embedding_projected_dim)\n","        \n","        # self.aux_predictor = NonLinearFeedForwardModule(hidden_size,\n","        #                                                 hidden_size//3,\n","        #                                                 output_size\n","        #                                                 )\n","\n","        self.odst = nn.Sequential(\n","            nn.Dropout(dropout),\n","            ODST(embedding_projected_dim + numerical_size, hidden_size),\n","            nn.BatchNorm1d(hidden_size),\n","            nn.Dropout(dropout)\n","        )\n","        self.risk_out = nn.Linear(hidden_size, output_size)\n","\n","        for m in self.modules():\n","            if isinstance(m, nn.Linear):\n","                nn.init.xavier_normal_(m.weight)\n","                nn.init.zeros_(m.bias)\n","    \n","    def forward(self, x_cat : torch.Tensor, x_num : torch.Tensor):\n","        emb = self.embedding_module(x_cat)\n","        emb = self.project_embeddings(emb)\n","        x = torch.cat([emb, x_num], dim=1)\n","        x = self.odst(x)\n","        risk = self.risk_out(x)\n","        # efs_time_pred = self.aux_predictor(x)\n","        return risk, x\n","    \n","    def data_aware_init(self, dataloader):\n","        \n","        cats,  nums = [], []\n","        for batch in dataloader:\n","            x_cat, x_num, *other = batch\n","            cats += [x_cat]\n","            nums += [x_num]\n","        all_cat = torch.cat(cats)\n","        all_num = torch.cat(nums)\n","\n","        with torch.no_grad():\n","            self(all_cat, all_num)"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2025-03-05T00:05:08.045039Z","iopub.status.busy":"2025-03-05T00:05:08.044642Z","iopub.status.idle":"2025-03-05T00:05:08.057512Z","shell.execute_reply":"2025-03-05T00:05:08.056860Z","shell.execute_reply.started":"2025-03-05T00:05:08.045009Z"},"trusted":true},"outputs":[],"source":["from dataclasses import dataclass\n","from typing import *\n","import pandas as pd\n","\n","\n","@dataclass\n","class DataSciencePipelineSettings(object):\n","    train_csv_path : str\n","    test_csv_path : str\n","    target_col_name : List[str]\n","    original_csv_path : str = None\n","\n","    train_data : Union[None, pd.DataFrame] = None\n","    test_data : Union[None, pd.DataFrame] = None\n","    original_data : Union[None, pd.DataFrame] = None\n","\n","    original_csv_processing : callable = lambda x : x\n","    sample_submission_path : str = None\n","    training_col_names : List[str] = None\n","    categorical_col_names : List[str] = None\n","    training_data_percentage : float = 0.8\n","    category_occurrence_threshold : int = 300\n","    logged : bool = False\n","\n","    def __post_init__(self):\n","        self.train_df, self.test_df = self._gather_data()\n","        self.training_col_names, self.categorical_col_names, self.numerical_col_names = self._get_column_info()\n","        self.combined_df = self._combine_datasets()\n","\n","    def _gather_data(self):\n","        train_df = self._smart_drop_index(pd.read_csv(self.train_csv_path) if self.train_data is None else self.train_data)\n","        test_df = self._smart_drop_index(pd.read_csv(self.test_csv_path) if self.test_data is None else self.test_data)\n","\n","        if self.original_csv_path is not None:\n","            train_df = train_df.assign(source=0)\n","            test_df = test_df.assign(source=0)\n","            original_df = self._smart_drop_index(pd.read_csv(self.original_csv_path) if self.original_data is None else self.original_data).assign(source=1)\n","            original_df = self.original_csv_processing(original_df)\n","\n","            pd.testing.assert_index_equal(train_df.columns.sort_values(), original_df.columns.sort_values(), check_exact=True)\n","            pd.testing.assert_series_equal(train_df.dtypes.sort_index(), original_df.dtypes.sort_index(), check_exact=True)\n","            train_df = pd.concat([train_df, original_df], axis=0).reset_index(drop=True)\n","\n","        return train_df, test_df\n","    \n","    def _get_column_info(self):\n","        cat_col_names = [col_name for col_name in self.train_df.columns if self.train_df[col_name].dtype == 'object']\n","        training_features = list(self.train_df.drop(columns=self.target_col_name).columns)\n","        cat_col_names = cat_col_names if self.categorical_col_names is None else self.categorical_col_names\n","        num_col_names = [f for f in training_features if f not in cat_col_names]\n","        return training_features, cat_col_names, num_col_names\n","    \n","    def _combine_datasets(self):\n","        combined_df = pd.concat([self.train_df, self.test_df], keys=['train', 'test'])\n","        return combined_df\n","    \n","    def update(self):\n","        self.train_df = self.combined_df.loc['train'].copy()\n","        self.test_df = self.combined_df.loc['test'].copy()\n","        return self.train_df, self.test_df\n","    \n","    def get_data(self):\n","        self.update()\n","        X_test, y_test = self.test_df.drop(columns=self.target_col_name), self.test_df[self.target_col_name]\n","        X, y = self.train_df.drop(columns=self.target_col_name), self.train_df[self.target_col_name]\n","        return X, y, X_test, y_test\n","\n","    @staticmethod\n","    def _smart_drop_index(df):\n","        try:\n","            differences = df.iloc[:, 0].diff().dropna()\n","            if differences.nunique() == 1:\n","                df = df.drop(columns=df.columns[0])\n","        except:\n","            pass\n","        return df\n","    \n","    @property\n","    def target_col(self):\n","        \"\"\"target column name property.\"\"\"\n","        return self.target_col_name\n","\n","    @target_col.setter\n","    def target_col(self, value):\n","        self.target_col_name = value"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2025-03-05T00:05:08.058621Z","iopub.status.busy":"2025-03-05T00:05:08.058308Z","iopub.status.idle":"2025-03-05T00:05:08.130645Z","shell.execute_reply":"2025-03-05T00:05:08.130085Z","shell.execute_reply.started":"2025-03-05T00:05:08.058590Z"},"trusted":true},"outputs":[],"source":["def encode_in_order(array):\n","    d = {}\n","    idx = 0\n","    for i, n in enumerate(array):\n","        if n not in d:\n","            d[n] = idx\n","            idx += 1\n","        array[i] = d[array[i]]\n","    return array\n","\n","def get_activation(activation):\n","    if activation == 'relu':\n","        return nn.ReLU()\n","    elif activation == 'gelu':\n","        return nn.GELU()\n","    elif activation == 'sigmoid':\n","        return nn.Sigmoid()\n","    elif activation == 'none':\n","        return nn.Identity()"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2025-03-05T00:05:08.131919Z","iopub.status.busy":"2025-03-05T00:05:08.131624Z","iopub.status.idle":"2025-03-05T00:05:08.149786Z","shell.execute_reply":"2025-03-05T00:05:08.149226Z","shell.execute_reply.started":"2025-03-05T00:05:08.131890Z"},"trusted":true},"outputs":[],"source":["torch.use_deterministic_algorithms(False)"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2025-03-05T00:05:08.150844Z","iopub.status.busy":"2025-03-05T00:05:08.150546Z","iopub.status.idle":"2025-03-05T00:05:08.177605Z","shell.execute_reply":"2025-03-05T00:05:08.176803Z","shell.execute_reply.started":"2025-03-05T00:05:08.150816Z"},"trusted":true},"outputs":[],"source":["# pl.seed_everything(42)\n","\n","# kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)\n","# train_dataframe = pd.read_csv(train_csv_path, index_col=0)\n","# test_dataframe = pd.read_csv(test_csv_path, index_col=0)\n","\n","# # print(train_dataframe)\n","\n","# folds = kf.split(\n","#                 train_dataframe, train_dataframe.race_group.astype(str)\n","#             )\n","\n","# metrics_list = []\n","# oof_preds = np.zeros(train_dataframe.shape[0])\n","# test_pred = np.zeros(test_dataframe.shape[0])\n","\n","# _, train_original = load_data()\n","\n","# for i, (train_index, test_index) in enumerate(folds):\n","\n","#     train_df = train_dataframe.iloc[train_index]\n","#     test_df = train_dataframe.iloc[test_index]\n","\n","#     tt = train_original.copy()\n","#     train = tt.iloc[train_index]\n","#     val = tt.iloc[test_index]\n","\n","#     (X_train, y_train, X_val, y_val), settings = ktools_preprocess_data(train_df, test_df)\n","\n","#     cat_names = settings.categorical_col_names\n","#     cat_sizes = [int(x) for x in X_train[cat_names].nunique().values]\n","#     emb_sizes = [16] * len(cat_sizes)\n","#     categorical_idcs = [X_train.columns.get_loc(col) for col in cat_names]\n","#     numerical_idcs = list(set(range(X_train.shape[1])).difference(set(categorical_idcs)))\n","#     race_idx = X_train[cat_names].columns.get_loc('race_group')\n","    \n","#     dl_train, X_cat_train, X_num_train = init_ktools_dl(X_train, y_train, categorical_idcs, numerical_idcs, training=True)\n","#     dl_val, X_cat_val, X_num_val = init_ktools_dl(X_val, y_val, categorical_idcs, numerical_idcs)\n","\n","#     num_cols = [x for x in X_val.iloc[:, numerical_idcs].columns.tolist() if 'nan' not in x]\n","#     X_cat_train_exp, X_cat_val_exp, X_num_train_exp, X_num_val_exp, dl_train, dl_val, transformers = preprocess_data(train, val, numericals=num_cols)\n","    \n","#     for i in range(X_cat_train_exp.shape[1]):\n","#         assert np.allclose(encode_in_order(X_cat_train_exp[:, i]), encode_in_order(X_cat_train[:, i]))\n","#         assert np.allclose(encode_in_order(X_cat_val_exp[:, i]), encode_in_order(X_cat_val[:, i]))\n","\n","#     X_num_val_exp[:, [-2, -1]] = X_num_val_exp[:, [-1, -2]]\n","#     X_num_train_exp[:, [-2, -1]] = X_num_train_exp[:, [-1, -2]]\n","#     assert np.allclose(X_num_val, X_num_val_exp)\n","#     assert np.allclose(X_num_train, X_num_train_exp)\n","\n","#     base_model = IskanderPairwiseNetwork(cat_sizes, len(numerical_idcs), emb_sizes)\n","#     model = PostHCTModel(base_model, race_index=race_idx)\n","    \n","#     checkpoint_callback = pl.callbacks.ModelCheckpoint(monitor=None,\n","#                                                        dirpath=\"checkpoints/\",\n","#                                                        filename=\"last\",\n","#                                                        save_top_k=1, \n","#                                                        save_last=True)\n","#     trainer = pl.Trainer(\n","#         accelerator='cuda',\n","#         max_epochs=60,\n","#         callbacks=[\n","#             checkpoint_callback,\n","#             LearningRateMonitor(logging_interval='epoch'),\n","#             TQDMProgressBar(),\n","#             StochasticWeightAveraging(swa_lrs=1e-5, swa_epoch_start=45, annealing_epochs=15)\n","#         ],\n","#         # deterministic=True\n","#     )\n","#     trainer.fit(model, dl_train)\n","#     trainer.test(model, dl_val)\n","#     model.eval()\n","\n","#     predictions = model.eval_preds\n","#     print(scci_metric(test_df, -predictions.squeeze()))\n","\n","#     metrics_list += [scci_metric(test_df, -predictions.squeeze())]\n","#     print(metrics_list)\n","    \n","#     oof_preds[test_index] = predictions.squeeze()\n","\n","\n","#     (_, _, X_test, y_test), settings = ktools_preprocess_data(train_df, test_dataframe)\n","#     X_cat_val, X_num_val = X_test.iloc[:, categorical_idcs].values, X_test.iloc[:, numerical_idcs].values\n","    \n","#     # test_dl = eval_dl(X_cat_val, X_num_val)\n","#     # trainer.test(model, test_dl)\n","#     # test_pred += model.eval_preds.squeeze()\n","\n","#     pred, _ = model.cuda().eval()(\n","#         torch.tensor(X_cat_val, dtype=torch.long).cuda(),\n","#         torch.tensor(X_num_val, dtype=torch.float32).cuda()\n","#     )\n","#     test_predictions_this_fold = pred.squeeze().detach().cpu().numpy()\n","\n","#     test_predictions_this_fold = StandardScaler().fit_transform(test_predictions_this_fold[:, None])\n","#     test_pred += test_predictions_this_fold.squeeze()\n","\n","\n","# print(f\"metric across folds: \", [f\"{n:.3f}\" for n in metrics_list])\n","# print(\"oof scci metric score: \", scci_metric(train_dataframe, -oof_preds))\n"]},{"cell_type":"code","execution_count":56,"metadata":{"execution":{"iopub.execute_input":"2025-03-05T00:05:08.178716Z","iopub.status.busy":"2025-03-05T00:05:08.178455Z","iopub.status.idle":"2025-03-05T00:05:08.538473Z","shell.execute_reply":"2025-03-05T00:05:08.537779Z","shell.execute_reply.started":"2025-03-05T00:05:08.178685Z"},"trusted":true},"outputs":[],"source":["train_dataframe = pd.read_csv(train_csv_path, index_col=0)\n","test_dataframe = pd.read_csv(test_csv_path, index_col=0)"]},{"cell_type":"code","execution_count":57,"metadata":{"execution":{"iopub.execute_input":"2025-03-05T00:05:08.539588Z","iopub.status.busy":"2025-03-05T00:05:08.539368Z","iopub.status.idle":"2025-03-05T00:05:08.544037Z","shell.execute_reply":"2025-03-05T00:05:08.543279Z","shell.execute_reply.started":"2025-03-05T00:05:08.539569Z"},"trusted":true},"outputs":[],"source":["test_dataframe['efs'] = 1\n","test_dataframe['efs_time'] = 1"]},{"cell_type":"code","execution_count":62,"metadata":{"execution":{"iopub.execute_input":"2025-03-05T00:05:22.885590Z","iopub.status.busy":"2025-03-05T00:05:22.885192Z","iopub.status.idle":"2025-03-05T00:05:45.306812Z","shell.execute_reply":"2025-03-05T00:05:45.305132Z","shell.execute_reply.started":"2025-03-05T00:05:22.885554Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["There are 58 FEATURES: ['dri_score', 'psych_disturb', 'cyto_score', 'diabetes', 'hla_match_c_high', 'hla_high_res_8', 'tbi_status', 'arrhythmia', 'hla_low_res_6', 'graft_type', 'vent_hist', 'renal_issue', 'pulm_severe', 'prim_disease_hct', 'hla_high_res_6', 'cmv_status', 'hla_high_res_10', 'hla_match_dqb1_high', 'tce_imm_match', 'hla_nmdp_6', 'hla_match_c_low', 'rituximab', 'hla_match_drb1_low', 'hla_match_dqb1_low', 'prod_type', 'cyto_score_detail', 'conditioning_intensity', 'ethnicity', 'year_hct', 'obesity', 'mrd_hct', 'in_vivo_tcd', 'tce_match', 'hla_match_a_high', 'hepatic_severe', 'donor_age', 'prior_tumor', 'hla_match_b_low', 'peptic_ulcer', 'age_at_hct', 'hla_match_a_low', 'gvhd_proph', 'rheum_issue', 'sex_match', 'hla_match_b_high', 'race_group', 'comorbidity_score', 'karnofsky_score', 'hepatic_mild', 'tce_div_match', 'donor_related', 'melphalan_dose', 'hla_low_res_8', 'cardiac', 'hla_match_drb1_high', 'pulm_moderate', 'hla_low_res_10', 'is_cyto_score_same']\n","2\n","0\n","1\n","0\n","2\n","6\n","0\n","0\n","4\n","0\n","0\n","0\n","0\n","2\n","5\n","0\n","7\n","2\n","7\n","4\n","2\n","0\n","2\n","0\n","1\n","0\n","2\n","0\n","0\n","1\n","0\n","3\n","2\n","0\n","0\n","2\n","0\n","2\n","7\n","0\n","0\n","2\n","1\n","0\n","5\n","0\n","3\n","1\n","1\n","6\n","0\n","2\n"]},{"name":"stderr","output_type":"stream","text":["GPU available: True (mps), used: False\n","TPU available: False, using: 0 TPU cores\n","HPU available: False, using: 0 HPUs\n","/Users/yuwei-1/anaconda3/envs/ktools/lib/python3.12/site-packages/pytorch_lightning/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n","/Users/yuwei-1/anaconda3/envs/ktools/lib/python3.12/site-packages/pytorch_lightning/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n","/Users/yuwei-1/anaconda3/envs/ktools/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /Users/yuwei-1/Documents/projects/Kaggle-tools/post_HCT_survival_notebooks/checkpoints exists and is not empty.\n","\n","  | Name          | Type                       | Params | Mode \n","---------------------------------------------------------------------\n","0 | model         | IskanderPairwiseNetwork    | 159 K  | train\n","1 | aux_predictor | NonLinearFeedForwardModule | 1.0 K  | train\n","---------------------------------------------------------------------\n","159 K     Trainable params\n","769       Non-trainable params\n","160 K     Total params\n","0.642     Total estimated model params size (MB)\n","74        Modules in train mode\n","0         Modules in eval mode\n"]},{"name":"stdout","output_type":"stream","text":["0\n","6\n"]},{"name":"stderr","output_type":"stream","text":["/Users/yuwei-1/anaconda3/envs/ktools/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n","/Users/yuwei-1/anaconda3/envs/ktools/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (15) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5ac934acfe5a4945b2af4597bdcc4dee","version_major":2,"version_minor":0},"text/plain":["Training: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\n","Detected KeyboardInterrupt, attempting graceful shutdown ...\n"]},{"ename":"NameError","evalue":"name 'exit' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","File \u001b[0;32m~/anaconda3/envs/ktools/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py:47\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n","File \u001b[0;32m~/anaconda3/envs/ktools/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:574\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    568\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    570\u001b[0m     ckpt_path,\n\u001b[1;32m    571\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    572\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    573\u001b[0m )\n\u001b[0;32m--> 574\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n","File \u001b[0;32m~/anaconda3/envs/ktools/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:981\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    978\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    979\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 981\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n","File \u001b[0;32m~/anaconda3/envs/ktools/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:1025\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1025\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m~/anaconda3/envs/ktools/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:205\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start()\n\u001b[0;32m--> 205\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n","File \u001b[0;32m~/anaconda3/envs/ktools/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:363\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 363\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/anaconda3/envs/ktools/lib/python3.12/site-packages/pytorch_lightning/loops/training_epoch_loop.py:140\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 140\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end(data_fetcher)\n","File \u001b[0;32m~/anaconda3/envs/ktools/lib/python3.12/site-packages/pytorch_lightning/loops/training_epoch_loop.py:250\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.advance\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39mautomatic_optimization:\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;66;03m# in automatic optimization, there can only be one optimizer\u001b[39;00m\n\u001b[0;32m--> 250\u001b[0m     batch_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautomatic_optimization\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n","File \u001b[0;32m~/anaconda3/envs/ktools/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py:190\u001b[0m, in \u001b[0;36m_AutomaticOptimization.run\u001b[0;34m(self, optimizer, batch_idx, kwargs)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;66;03m# BACKWARD PASS\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;66;03m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    192\u001b[0m result \u001b[38;5;241m=\u001b[39m closure\u001b[38;5;241m.\u001b[39mconsume_result()\n","File \u001b[0;32m~/anaconda3/envs/ktools/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py:268\u001b[0m, in \u001b[0;36m_AutomaticOptimization._optimizer_step\u001b[0;34m(self, batch_idx, train_step_and_backward_closure)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;66;03m# model hook\u001b[39;00m\n\u001b[0;32m--> 268\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_lightning_module_hook\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moptimizer_step\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_step_and_backward_closure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m should_accumulate:\n","File \u001b[0;32m~/anaconda3/envs/ktools/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py:167\u001b[0m, in \u001b[0;36m_call_lightning_module_hook\u001b[0;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LightningModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpl_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 167\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n","File \u001b[0;32m~/anaconda3/envs/ktools/lib/python3.12/site-packages/pytorch_lightning/core/module.py:1306\u001b[0m, in \u001b[0;36mLightningModule.optimizer_step\u001b[0;34m(self, epoch, batch_idx, optimizer, optimizer_closure)\u001b[0m\n\u001b[1;32m   1282\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Override this method to adjust the default way the :class:`~pytorch_lightning.trainer.trainer.Trainer` calls\u001b[39;00m\n\u001b[1;32m   1283\u001b[0m \u001b[38;5;124;03mthe optimizer.\u001b[39;00m\n\u001b[1;32m   1284\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1304\u001b[0m \n\u001b[1;32m   1305\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1306\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer_closure\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/anaconda3/envs/ktools/lib/python3.12/site-packages/pytorch_lightning/core/optimizer.py:153\u001b[0m, in \u001b[0;36mLightningOptimizer.step\u001b[0;34m(self, closure, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_strategy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_strategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_on_after_step()\n","File \u001b[0;32m~/anaconda3/envs/ktools/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py:238\u001b[0m, in \u001b[0;36mStrategy.optimizer_step\u001b[0;34m(self, optimizer, closure, model, **kwargs)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, pl\u001b[38;5;241m.\u001b[39mLightningModule)\n\u001b[0;32m--> 238\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecision_plugin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/anaconda3/envs/ktools/lib/python3.12/site-packages/pytorch_lightning/plugins/precision/precision.py:122\u001b[0m, in \u001b[0;36mPrecision.optimizer_step\u001b[0;34m(self, optimizer, model, closure, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m closure \u001b[38;5;241m=\u001b[39m partial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrap_closure, model, optimizer, closure)\n\u001b[0;32m--> 122\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/anaconda3/envs/ktools/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:137\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.patch_track_step_called.<locals>.wrap_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    136\u001b[0m opt\u001b[38;5;241m.\u001b[39m_opt_called \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__get__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/anaconda3/envs/ktools/lib/python3.12/site-packages/torch/optim/optimizer.py:487\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    483\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    484\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    485\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n","File \u001b[0;32m~/anaconda3/envs/ktools/lib/python3.12/site-packages/torch/optim/optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n","File \u001b[0;32m~/anaconda3/envs/ktools/lib/python3.12/site-packages/torch/optim/adam.py:202\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39menable_grad():\n\u001b[0;32m--> 202\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_groups:\n","File \u001b[0;32m~/anaconda3/envs/ktools/lib/python3.12/site-packages/pytorch_lightning/plugins/precision/precision.py:108\u001b[0m, in \u001b[0;36mPrecision._wrap_closure\u001b[0;34m(self, model, optimizer, closure)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"This double-closure allows makes sure the ``closure`` is executed before the ``on_before_optimizer_step``\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;124;03mhook is called.\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    106\u001b[0m \n\u001b[1;32m    107\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 108\u001b[0m closure_result \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_after_closure(model, optimizer)\n","File \u001b[0;32m~/anaconda3/envs/ktools/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py:144\u001b[0m, in \u001b[0;36mClosure.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[Tensor]:\n\u001b[0;32m--> 144\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\u001b[38;5;241m.\u001b[39mloss\n","File \u001b[0;32m~/anaconda3/envs/ktools/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/anaconda3/envs/ktools/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py:138\u001b[0m, in \u001b[0;36mClosure.closure\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m step_output\u001b[38;5;241m.\u001b[39mclosure_loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 138\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_output\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclosure_loss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m step_output\n","File \u001b[0;32m~/anaconda3/envs/ktools/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py:239\u001b[0m, in \u001b[0;36m_AutomaticOptimization._make_backward_fn.<locals>.backward_fn\u001b[0;34m(loss)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mbackward_fn\u001b[39m(loss: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 239\u001b[0m     \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbackward\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/anaconda3/envs/ktools/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py:319\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 319\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n","File \u001b[0;32m~/anaconda3/envs/ktools/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py:212\u001b[0m, in \u001b[0;36mStrategy.backward\u001b[0;34m(self, closure_loss, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m closure_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecision_plugin\u001b[38;5;241m.\u001b[39mpre_backward(closure_loss, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module)\n\u001b[0;32m--> 212\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecision_plugin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlightning_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m closure_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecision_plugin\u001b[38;5;241m.\u001b[39mpost_backward(closure_loss, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module)\n","File \u001b[0;32m~/anaconda3/envs/ktools/lib/python3.12/site-packages/pytorch_lightning/plugins/precision/precision.py:72\u001b[0m, in \u001b[0;36mPrecision.backward\u001b[0;34m(self, tensor, model, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Performs the actual backpropagation.\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \n\u001b[1;32m     63\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/anaconda3/envs/ktools/lib/python3.12/site-packages/pytorch_lightning/core/module.py:1101\u001b[0m, in \u001b[0;36mLightningModule.backward\u001b[0;34m(self, loss, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1101\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/anaconda3/envs/ktools/lib/python3.12/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/anaconda3/envs/ktools/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/anaconda3/envs/ktools/lib/python3.12/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n","File \u001b[0;32m~/anaconda3/envs/ktools/lib/python3.12/site-packages/torch/autograd/function.py:292\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;124;03mThis class is used for internal autograd work. Do not use.\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 292\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mapply\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m    293\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;124;03m    Apply method used when executing this Node during the backward\u001b[39;00m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: ","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[62], line 46\u001b[0m\n\u001b[1;32m     30\u001b[0m checkpoint_callback \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mModelCheckpoint(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     31\u001b[0m                                                    dirpath\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpoints/\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     32\u001b[0m                                                    filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlast\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     33\u001b[0m                                                    save_top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, \n\u001b[1;32m     34\u001b[0m                                                    save_last\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     35\u001b[0m trainer \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mTrainer(\n\u001b[1;32m     36\u001b[0m     accelerator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     37\u001b[0m     max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m60\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;66;03m# deterministic=True\u001b[39;00m\n\u001b[1;32m     45\u001b[0m )\n\u001b[0;32m---> 46\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdl_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtest(model, dl_val)\n\u001b[1;32m     49\u001b[0m test_preds \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39meval_preds\u001b[38;5;241m.\u001b[39msqueeze()\n","File \u001b[0;32m~/anaconda3/envs/ktools/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:538\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 538\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/anaconda3/envs/ktools/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py:64\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(launcher, _SubprocessScriptLauncher):\n\u001b[1;32m     63\u001b[0m         launcher\u001b[38;5;241m.\u001b[39mkill(_get_sigkill_signal())\n\u001b[0;32m---> 64\u001b[0m     \u001b[43mexit\u001b[49m(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n\u001b[1;32m     67\u001b[0m     _interrupt(trainer, exception)\n","\u001b[0;31mNameError\u001b[0m: name 'exit' is not defined"]}],"source":["(X_train, y_train, X_val, y_val), settings = ktools_preprocess_data(train_dataframe, test_dataframe)\n","\n","cat_names = settings.categorical_col_names\n","cat_sizes = [int(x) for x in X_train[cat_names].nunique().values]\n","emb_sizes = [16] * len(cat_sizes)\n","categorical_idcs = [X_train.columns.get_loc(col) for col in cat_names]\n","numerical_idcs = list(set(range(X_train.shape[1])).difference(set(categorical_idcs)))\n","race_idx = X_train[cat_names].columns.get_loc('race_group')\n","\n","dl_train, X_cat_train, X_num_train = init_ktools_dl(X_train, y_train, categorical_idcs, numerical_idcs, training=True)\n","dl_val, X_cat_val, X_num_val = init_ktools_dl(X_val, y_val, categorical_idcs, numerical_idcs)\n","num_cols = [x for x in X_val.iloc[:, numerical_idcs].columns.tolist() if 'nan' not in x]\n","X_cat_train_exp, X_cat_val_exp, X_num_train_exp, X_num_val_exp, dl_train, dl_val, transformers = preprocess_data(train_dataframe, test_dataframe, numericals=num_cols)\n","\n","for i in range(X_cat_train_exp.shape[1]):\n","    print(X_cat_val_exp[:, i].min())\n","    assert np.allclose(encode_in_order(X_cat_train_exp[:, i]), encode_in_order(X_cat_train[:, i]))\n","    assert np.allclose(encode_in_order(X_cat_val_exp[:, i]), encode_in_order(X_cat_val[:, i]))\n","\n","X_num_val_exp[:, [-2, -1]] = X_num_val_exp[:, [-1, -2]]\n","X_num_train_exp[:, [-2, -1]] = X_num_train_exp[:, [-1, -2]]\n","assert np.allclose(X_num_val, X_num_val_exp)\n","assert np.allclose(X_num_train, X_num_train_exp)\n","\n","# display(X_val)\n","\n","base_model = IskanderPairwiseNetwork(cat_sizes, len(numerical_idcs), emb_sizes)\n","model = PostHCTModel(base_model, race_index=race_idx)\n","\n","checkpoint_callback = pl.callbacks.ModelCheckpoint(monitor=None,\n","                                                   dirpath=\"checkpoints/\",\n","                                                   filename=\"last\",\n","                                                   save_top_k=1, \n","                                                   save_last=True)\n","trainer = pl.Trainer(\n","    accelerator='cpu',\n","    max_epochs=60,\n","    callbacks=[\n","        checkpoint_callback,\n","        LearningRateMonitor(logging_interval='epoch'),\n","        TQDMProgressBar(),\n","        StochasticWeightAveraging(swa_lrs=1e-5, swa_epoch_start=45, annealing_epochs=15)\n","    ],\n","    # deterministic=True\n",")\n","trainer.fit(model, dl_train)\n","trainer.test(model, dl_val)\n","\n","test_preds = model.eval_preds.squeeze()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["subm_data = pd.read_csv(sub_csv_path)\n","subm_data['prediction'] = -test_preds\n","subm_data.to_csv('submission.csv', index=False)"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"databundleVersionId":10381525,"sourceId":70942,"sourceType":"competition"},{"datasetId":6504548,"sourceId":10901676,"sourceType":"datasetVersion"},{"sourceId":211322530,"sourceType":"kernelVersion"},{"sourceId":219607918,"sourceType":"kernelVersion"}],"dockerImageVersionId":30918,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.0"}},"nbformat":4,"nbformat_minor":4}
