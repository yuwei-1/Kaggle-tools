{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import pandas as pd\n",
    "from functools import reduce\n",
    "from lifelines.utils import concordance_index\n",
    "from ktools.metrics.stratified_concordance_index import stratified_concordance_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import *\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def func(x):\n",
    "    return x\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataSciencePipelineSettings:\n",
    "    train_csv_path : str\n",
    "    test_csv_path : str\n",
    "    target_col_name : str\n",
    "    original_csv_path : str = None\n",
    "    original_csv_processing : callable = func\n",
    "    sample_submission_path : str = None\n",
    "    training_col_names : List[str] = None\n",
    "    categorical_col_names : List[str] = None\n",
    "    training_data_percentage : float = 0.8\n",
    "    category_occurrence_threshold : int = 300\n",
    "    logged : bool = False\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.train_df, self.test_df = self._load_csv_paths()\n",
    "        self.training_col_names, self.categorical_col_names = self._get_column_info()\n",
    "        self.combined_df = self._combine_datasets()\n",
    "\n",
    "    def _load_csv_paths(self):\n",
    "        train_df = self._smart_drop_index(pd.read_csv(self.train_csv_path))\n",
    "        test_df = self._smart_drop_index(pd.read_csv(self.test_csv_path))\n",
    "        if self.original_csv_path is not None:\n",
    "            train_df = train_df.assign(source=0)\n",
    "            test_df = test_df.assign(source=0)\n",
    "            original_df = self._smart_drop_index(pd.read_csv(self.original_csv_path)).assign(source=1)\n",
    "            original_df = self.original_csv_processing(original_df)\n",
    "\n",
    "            pd.testing.assert_index_equal(train_df.columns.sort_values(), original_df.columns.sort_values(), check_exact=True)\n",
    "            pd.testing.assert_series_equal(train_df.dtypes.sort_index(), original_df.dtypes.sort_index(), check_exact=True)\n",
    "            train_df = pd.concat([train_df, original_df], axis=0).reset_index(drop=True)\n",
    "\n",
    "        return train_df, test_df\n",
    "    \n",
    "    def _get_column_info(self):\n",
    "        cat_col_names = [col_name for col_name in self.train_df.columns if self.train_df[col_name].dtype == 'object']\n",
    "        training_features = list(self.train_df.drop(columns=self.target_col_name).columns)\n",
    "        return training_features, cat_col_names\n",
    "    \n",
    "    def _combine_datasets(self):\n",
    "        combined_df = pd.concat([self.train_df, self.test_df], keys=['train', 'test'])\n",
    "        return combined_df\n",
    "    \n",
    "    def update(self):\n",
    "        self.train_df = self.combined_df.loc['train'].copy()\n",
    "        self.test_df = self.combined_df.loc['test'].copy()\n",
    "        return self.train_df, self.test_df        \n",
    "\n",
    "    @staticmethod\n",
    "    def _smart_drop_index(df):\n",
    "        try:\n",
    "            differences = df.iloc[:, 0].diff().dropna()\n",
    "            if differences.nunique() == 1:\n",
    "                df = df.drop(columns=df.columns[0])\n",
    "        except:\n",
    "            pass\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler, MinMaxScaler\n",
    "\n",
    "\n",
    "\n",
    "class ConvertToLower():\n",
    "    @staticmethod\n",
    "    def transform(original_settings : DataSciencePipelineSettings):\n",
    "        settings = deepcopy(original_settings)\n",
    "        for col_name in settings.categorical_col_names:\n",
    "            settings.combined_df[col_name] = settings.combined_df[col_name].str.lower()\n",
    "        return settings\n",
    "    \n",
    "\n",
    "class FillNullValues():\n",
    "    @staticmethod\n",
    "    def transform(original_settings : DataSciencePipelineSettings, numeric_fill=-1, category_fill='missing'):\n",
    "        settings = deepcopy(original_settings)\n",
    "        for col_name in settings.training_col_names:\n",
    "            if pd.api.types.is_numeric_dtype(settings.combined_df[col_name]):\n",
    "                settings.combined_df[col_name] = settings.combined_df[col_name].fillna(numeric_fill)\n",
    "            else:\n",
    "                settings.combined_df[col_name] = settings.combined_df[col_name].fillna(category_fill)\n",
    "        return settings\n",
    "    \n",
    "\n",
    "class ConvertObjectToCategorical():\n",
    "    @staticmethod\n",
    "    def transform(original_settings : DataSciencePipelineSettings):\n",
    "        settings = deepcopy(original_settings)\n",
    "        cat_cols = settings.categorical_col_names\n",
    "        settings.combined_df[cat_cols] = settings.combined_df[cat_cols].astype('category')\n",
    "        return settings\n",
    "    \n",
    "\n",
    "class ConvertObjectToStrCategorical():\n",
    "    @staticmethod\n",
    "    def transform(original_settings : DataSciencePipelineSettings):\n",
    "        settings = deepcopy(original_settings)\n",
    "        cat_cols = settings.categorical_col_names\n",
    "        settings.combined_df[cat_cols] = settings.combined_df[cat_cols].astype(str).astype('category')\n",
    "        return settings\n",
    "\n",
    "\n",
    "class ConvertAllToCategorical():\n",
    "    @staticmethod\n",
    "    def transform(original_settings : DataSciencePipelineSettings):\n",
    "        settings = deepcopy(original_settings)\n",
    "        all_cols = settings.training_col_names\n",
    "        settings.combined_df[all_cols] = settings.combined_df[all_cols].astype(str).astype('category')\n",
    "        return settings\n",
    "    \n",
    "\n",
    "class LogTransformTarget():\n",
    "    @staticmethod\n",
    "    def transform(original_settings : DataSciencePipelineSettings):\n",
    "        settings = deepcopy(original_settings)\n",
    "        target = settings.target_col_name\n",
    "        settings.combined_df[target] = np.log1p(settings.combined_df[target])\n",
    "        return settings\n",
    "    \n",
    "\n",
    "class OrdinalEncode():\n",
    "    @staticmethod\n",
    "    def transform(original_settings : DataSciencePipelineSettings):\n",
    "        settings = deepcopy(original_settings)\n",
    "        train_df, test_df = settings.update()\n",
    "        ordinal_encoder = OrdinalEncoder(encoded_missing_value=-1, handle_unknown=\"use_encoded_value\", unknown_value=-1)\n",
    "        train_df[settings.categorical_col_names] = ordinal_encoder.fit_transform(train_df[settings.categorical_col_names])\n",
    "        test_df[settings.categorical_col_names] = ordinal_encoder.transform(test_df[settings.categorical_col_names])\n",
    "        settings.combined_df = pd.concat([train_df, test_df], keys=['train', 'test'])\n",
    "        settings.combined_df[settings.categorical_col_names] = settings.combined_df[settings.categorical_col_names].astype(int)\n",
    "        return settings\n",
    "\n",
    "class StandardScaleNumerical():\n",
    "    @staticmethod\n",
    "    def transform(original_settings : DataSciencePipelineSettings):\n",
    "        settings = deepcopy(original_settings)\n",
    "        scaler = StandardScaler()\n",
    "        train_df, test_df = settings.update()\n",
    "        num_cols = settings.combined_df.select_dtypes(include=['number']).columns\n",
    "        train_df[num_cols] = scaler.fit_transform(train_df[num_cols])\n",
    "        test_df[num_cols] = scaler.transform(test_df[num_cols])\n",
    "        settings.combined_df = pd.concat([train_df, test_df], keys=['train', 'test'])\n",
    "        return settings\n",
    "\n",
    "class MinMaxScalerNumerical():\n",
    "    @staticmethod\n",
    "    def transform(original_settings : DataSciencePipelineSettings):\n",
    "        settings = deepcopy(original_settings)\n",
    "        scaler = MinMaxScaler()\n",
    "        train_df, test_df = settings.update()\n",
    "        num_cols = settings.combined_df.select_dtypes(include=['number']).columns\n",
    "        train_df[num_cols] = scaler.fit_transform(train_df[num_cols])\n",
    "        test_df[num_cols] = scaler.transform(test_df[num_cols])\n",
    "        settings.combined_df = pd.concat([train_df, test_df], keys=['train', 'test'])\n",
    "        return settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas.api.types\n",
    "import numpy as np\n",
    "from typing import Union\n",
    "from lifelines.utils import concordance_index\n",
    "\n",
    "\n",
    "def stratified_concordance_index(solution : pd.DataFrame, \n",
    "                                 predictions : Union[pd.Series, np.ndarray], \n",
    "                                 event_binary_col_name : str,\n",
    "                                 duration_col_name : str,\n",
    "                                 group_col_name : str) -> float:\n",
    "    \n",
    "    \"\"\"\n",
    "    Solution dataframe should contain all necessary columns\n",
    "    \"\"\"\n",
    "\n",
    "    solution['predictions'] = predictions\n",
    "    solution.reset_index(inplace=True)\n",
    "    solution_group_dict = dict(solution.groupby([group_col_name]).groups)\n",
    "    metric_list = []\n",
    "\n",
    "    for race in solution_group_dict.keys():\n",
    "\n",
    "        indices = sorted(solution_group_dict[race])\n",
    "        merged_df_race = solution.iloc[indices]\n",
    "\n",
    "        c_index_race = concordance_index(\n",
    "                        merged_df_race[duration_col_name],\n",
    "                        -merged_df_race['predictions'],\n",
    "                        merged_df_race[event_binary_col_name])\n",
    "        metric_list.append(c_index_race)\n",
    "    return float(np.mean(metric_list)-np.sqrt(np.var(metric_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv_path = \"../data/post_hct_survival/train.csv\"\n",
    "test_csv_path = \"../data/post_hct_survival/test.csv\"\n",
    "target_col_name = \"efs_time\"\n",
    "\n",
    "class CreateSurvivalTarget():\n",
    "    @staticmethod\n",
    "    def transform(original_settings : DataSciencePipelineSettings):\n",
    "        settings = deepcopy(original_settings)\n",
    "        settings.combined_df[\"survival_target\"] = np.where(settings.combined_df['efs'].astype(bool), \n",
    "                                                         settings.combined_df['efs_time'], \n",
    "                                                         -settings.combined_df['efs_time'])\n",
    "        return settings    \n",
    "\n",
    "settings = DataSciencePipelineSettings(train_csv_path,\n",
    "                                       test_csv_path,\n",
    "                                       target_col_name,\n",
    "                                       )\n",
    "transforms = [\n",
    "             FillNullValues.transform,\n",
    "             OrdinalEncode.transform,\n",
    "             ConvertObjectToStrCategorical.transform,\n",
    "             CreateSurvivalTarget.transform\n",
    "             ]\n",
    "\n",
    "settings = reduce(lambda acc, func: func(acc), transforms, settings)\n",
    "settings.update()\n",
    "\n",
    "train, test_df = settings.update()\n",
    "test_df.drop(columns=[target_col_name], inplace=True)\n",
    "X, y = train.drop(columns=[\"survival_target\"]), train[[\"survival_target\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dri_score</th>\n",
       "      <th>psych_disturb</th>\n",
       "      <th>cyto_score</th>\n",
       "      <th>diabetes</th>\n",
       "      <th>hla_match_c_high</th>\n",
       "      <th>hla_high_res_8</th>\n",
       "      <th>tbi_status</th>\n",
       "      <th>arrhythmia</th>\n",
       "      <th>hla_low_res_6</th>\n",
       "      <th>graft_type</th>\n",
       "      <th>...</th>\n",
       "      <th>tce_div_match</th>\n",
       "      <th>donor_related</th>\n",
       "      <th>melphalan_dose</th>\n",
       "      <th>hla_low_res_8</th>\n",
       "      <th>cardiac</th>\n",
       "      <th>hla_match_drb1_high</th>\n",
       "      <th>pulm_moderate</th>\n",
       "      <th>hla_low_res_10</th>\n",
       "      <th>efs</th>\n",
       "      <th>efs_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42.356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>102.349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28795</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28796</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28797</th>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28798</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>52.351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28799</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.158</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28800 rows × 59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      dri_score psych_disturb cyto_score diabetes  hla_match_c_high  \\\n",
       "0             7             0          7        0              -1.0   \n",
       "1             2             0          1        0               2.0   \n",
       "2             7             0          7        0               2.0   \n",
       "3             0             0          1        0               2.0   \n",
       "4             0             0          7        0               2.0   \n",
       "...         ...           ...        ...      ...               ...   \n",
       "28795         3             3          0        0               2.0   \n",
       "28796         0             0          5        2               1.0   \n",
       "28797         9             3          5        3               2.0   \n",
       "28798         7             0          5        0               1.0   \n",
       "28799         8             0          7        0               2.0   \n",
       "\n",
       "       hla_high_res_8 tbi_status arrhythmia  hla_low_res_6 graft_type  ...  \\\n",
       "0                -1.0          0          0            6.0          0  ...   \n",
       "1                 8.0          6          0            6.0          1  ...   \n",
       "2                 8.0          0          0            6.0          0  ...   \n",
       "3                 8.0          0          0            6.0          0  ...   \n",
       "4                 8.0          0          0            6.0          1  ...   \n",
       "...               ...        ...        ...            ...        ...  ...   \n",
       "28795             8.0          0          0            6.0          1  ...   \n",
       "28796             4.0          0          0            5.0          1  ...   \n",
       "28797             8.0          0          3            6.0          1  ...   \n",
       "28798             4.0          0          0            3.0          1  ...   \n",
       "28799             8.0          0          0            6.0          0  ...   \n",
       "\n",
       "      tce_div_match donor_related melphalan_dose hla_low_res_8  cardiac  \\\n",
       "0                 4             2              1           8.0        0   \n",
       "1                 3             1              1           8.0        0   \n",
       "2                 3             1              1           8.0        0   \n",
       "3                 3             2              1           8.0        0   \n",
       "4                 3             1              0           8.0        0   \n",
       "...             ...           ...            ...           ...      ...   \n",
       "28795             0             3              1           8.0        3   \n",
       "28796             1             1              1           6.0        2   \n",
       "28797             1             2              1           8.0        3   \n",
       "28798             3             1              0           4.0        0   \n",
       "28799             3             1              0           8.0        0   \n",
       "\n",
       "      hla_match_drb1_high  pulm_moderate  hla_low_res_10  efs  efs_time  \n",
       "0                     2.0              0            10.0  0.0    42.356  \n",
       "1                     2.0              2            10.0  1.0     4.672  \n",
       "2                     2.0              0            10.0  0.0    19.793  \n",
       "3                     2.0              0            10.0  0.0   102.349  \n",
       "4                     2.0              0            10.0  0.0    16.223  \n",
       "...                   ...            ...             ...  ...       ...  \n",
       "28795                 2.0              0            10.0  0.0    18.633  \n",
       "28796                 1.0              2             8.0  1.0     4.892  \n",
       "28797                 2.0              0            10.0  0.0    23.157  \n",
       "28798                 1.0              0             5.0  0.0    52.351  \n",
       "28799                 2.0              2            10.0  0.0    25.158  \n",
       "\n",
       "[28800 rows x 59 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.drop(columns='efs_time')\n",
    "y = X.pop('efs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuwei-1/anaconda3/envs/ktools/lib/python3.12/site-packages/sklearn/model_selection/_split.py:91: UserWarning: The groups parameter is ignored by KFold\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The CV results of the current fold is 0.6880208333333333\n",
      "The CV results of the current fold is 0.6899305555555556\n",
      "The CV results of the current fold is 0.6940972222222223\n",
      "The CV results of the current fold is 0.6885416666666667\n",
      "The CV results of the current fold is 0.6784722222222223\n",
      "####################################################################################################\n",
      "OOF prediction score :  0.6878125\n",
      "Mean 5-cv results : 0.6878124999999999 +- 0.005134194552576257\n",
      "####################################################################################################\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "from ktools.fitting.cross_validation_executor import CrossValidationExecutor\n",
    "from ktools.modelling.ktools_models.lgbm_model import LGBMModel\n",
    "\n",
    "\n",
    "lgbm = LGBMModel(**{'objective': 'binary', 'metric': 'binary_logloss'})\n",
    "\n",
    "kf = KFold(5, shuffle=True, random_state=42)\n",
    "score_tuple, oof_predictions, model_list, _ = CrossValidationExecutor(lgbm,\n",
    "                              accuracy_score,\n",
    "                              kf,\n",
    "                              verbose=2).run(X, y, output_transform_list=[lambda x : (x[1] > 0.5).astype(int)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = [f for f in X.columns.tolist() if f not in [target_col_name, \"efs\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ktools.modelling.ktools_models.catboost_model import CatBoostModel\n",
    "\n",
    "\n",
    "cb_model = CatBoostModel(predict_type=\"else\", loss_function='Cox', eval_metric='Cox', grow_policy='Lossguide',\n",
    "        use_best_model=False, num_boost_round=100, learning_rate=0.1, early_stopping_rounds=10, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.array([X.columns.get_loc(col) for col in ['efs', 'efs_time', 'race_group']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_transform(input):\n",
    "    (X_test, y_pred) = input\n",
    "    X_test['predictions'] = y_pred\n",
    "    return X_test\n",
    "\n",
    "def sci_metric(y_test, y_processed):\n",
    "    if isinstance(y_processed, np.ndarray):\n",
    "        data = y_processed[:, indices]\n",
    "        solution = pd.DataFrame(columns=['efs', 'efs_time', 'race_group'], data=data)\n",
    "        predicted = y_processed[:, -1]\n",
    "    else:\n",
    "        solution = y_processed\n",
    "        predicted = y_processed['predictions']\n",
    "\n",
    "    metric_value = stratified_concordance_index(solution,\n",
    "                                                predicted,\n",
    "                                                'efs',\n",
    "                                                'efs_time',\n",
    "                                                'race_group')\n",
    "    return metric_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuwei-1/Documents/projects/Kaggle-tools/post_HCT_survival_notebooks/../ktools/metrics/stratified_concordance_index.py:20: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  solution_group_dict = dict(solution.groupby([group_col_name]).groups)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The CV results of the current fold is 0.6588076178652762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuwei-1/Documents/projects/Kaggle-tools/post_HCT_survival_notebooks/../ktools/metrics/stratified_concordance_index.py:20: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  solution_group_dict = dict(solution.groupby([group_col_name]).groups)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The CV results of the current fold is 0.6613100704558396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuwei-1/Documents/projects/Kaggle-tools/post_HCT_survival_notebooks/../ktools/metrics/stratified_concordance_index.py:20: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  solution_group_dict = dict(solution.groupby([group_col_name]).groups)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The CV results of the current fold is 0.6620737003779054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuwei-1/Documents/projects/Kaggle-tools/post_HCT_survival_notebooks/../ktools/metrics/stratified_concordance_index.py:20: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  solution_group_dict = dict(solution.groupby([group_col_name]).groups)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The CV results of the current fold is 0.6555574046534938\n",
      "The CV results of the current fold is 0.6560811964346313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuwei-1/Documents/projects/Kaggle-tools/post_HCT_survival_notebooks/../ktools/metrics/stratified_concordance_index.py:20: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  solution_group_dict = dict(solution.groupby([group_col_name]).groups)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################################################################################################\n",
      "OOF prediction score :  0.6601325635219006\n",
      "Mean 5-cv results : 0.6587659979574292 +- 0.0026426563379750474\n",
      "####################################################################################################\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from ktools.fitting.cross_validation_executor import CrossValidationExecutor\n",
    "\n",
    "\n",
    "kf = KFold(5, shuffle=True, random_state=42)\n",
    "score_tuple, oof_predictions, model_list = CrossValidationExecutor(cb_model,\n",
    "                              sci_metric,\n",
    "                              kf,\n",
    "                              train_features,\n",
    "                              verbose=2).run(X, y, output_transform_list=[output_transform])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = np.zeros(test_df.shape[0])\n",
    "for model in model_list:\n",
    "    test_predictions += model.predict(test_df)/kf.get_n_splits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_string = \"naive_cat\"\n",
    "\n",
    "sample_sub = pd.read_csv(sample_submission_file)\n",
    "sample_sub.iloc[:, 1] =  test_predictions\n",
    "sample_sub.to_csv(f\"{model_string}_submission.csv\", index=False)\n",
    "sample_sub.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ktools_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
