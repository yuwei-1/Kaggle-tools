{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "from typing import *\n",
    "from ktools.modelling.ktools_models.lgbm_model import LGBMModel\n",
    "from ktools.modelling.ktools_models.xgb_model import XGBoostModel\n",
    "from ktools.utils.reduce_dataframe_usage import reduce_dataframe_size\n",
    "from typing import Literal\n",
    "from matplotlib.pylab import RandomState\n",
    "from numpy import *\n",
    "from sklearn.model_selection import TimeSeriesSplit, KFold\n",
    "from ktools.fitting.safe_cross_validation_executor import SafeCrossValidationExecutor\n",
    "# from sklearn.metrics import\n",
    "from torch.utils.data import TensorDataset\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.linear_model import LinearRegression, SGDRegressor\n",
    "from ktools.modelling.model_transform_wrappers.log_model_wrapper import LogModelWrapper\n",
    "from ktools.preprocessing.basic_feature_transformers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv_path = \"../data/DRW_crypto_price_prediction/train.parquet\"\n",
    "test_csv_path = \"../data/DRW_crypto_price_prediction/test.parquet\"\n",
    "\n",
    "train_df = pd.read_parquet(train_csv_path, engine='pyarrow')\n",
    "test_df = pd.read_parquet(test_csv_path, engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bid_qty</th>\n",
       "      <th>ask_qty</th>\n",
       "      <th>buy_qty</th>\n",
       "      <th>sell_qty</th>\n",
       "      <th>volume</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>...</th>\n",
       "      <th>X882</th>\n",
       "      <th>X883</th>\n",
       "      <th>X884</th>\n",
       "      <th>X885</th>\n",
       "      <th>X886</th>\n",
       "      <th>X887</th>\n",
       "      <th>X888</th>\n",
       "      <th>X889</th>\n",
       "      <th>X890</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-03-01 00:00:00</th>\n",
       "      <td>15.283</td>\n",
       "      <td>8.425</td>\n",
       "      <td>176.405</td>\n",
       "      <td>44.984</td>\n",
       "      <td>221.389</td>\n",
       "      <td>0.121263</td>\n",
       "      <td>-0.417690</td>\n",
       "      <td>0.005399</td>\n",
       "      <td>0.125948</td>\n",
       "      <td>0.058359</td>\n",
       "      <td>...</td>\n",
       "      <td>1.925423</td>\n",
       "      <td>1.847943</td>\n",
       "      <td>0.005676</td>\n",
       "      <td>0.190791</td>\n",
       "      <td>0.369691</td>\n",
       "      <td>0.377630</td>\n",
       "      <td>0.210153</td>\n",
       "      <td>0.159183</td>\n",
       "      <td>0.530636</td>\n",
       "      <td>0.562539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-03-01 00:01:00</th>\n",
       "      <td>38.590</td>\n",
       "      <td>2.336</td>\n",
       "      <td>525.846</td>\n",
       "      <td>321.950</td>\n",
       "      <td>847.796</td>\n",
       "      <td>0.302841</td>\n",
       "      <td>-0.049576</td>\n",
       "      <td>0.356667</td>\n",
       "      <td>0.481087</td>\n",
       "      <td>0.237954</td>\n",
       "      <td>...</td>\n",
       "      <td>1.928569</td>\n",
       "      <td>1.849468</td>\n",
       "      <td>0.005227</td>\n",
       "      <td>0.184660</td>\n",
       "      <td>0.363642</td>\n",
       "      <td>0.374515</td>\n",
       "      <td>0.209573</td>\n",
       "      <td>0.158963</td>\n",
       "      <td>0.530269</td>\n",
       "      <td>0.533686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-03-01 00:02:00</th>\n",
       "      <td>0.442</td>\n",
       "      <td>60.250</td>\n",
       "      <td>159.227</td>\n",
       "      <td>136.369</td>\n",
       "      <td>295.596</td>\n",
       "      <td>0.167462</td>\n",
       "      <td>-0.291212</td>\n",
       "      <td>0.083138</td>\n",
       "      <td>0.206881</td>\n",
       "      <td>0.101727</td>\n",
       "      <td>...</td>\n",
       "      <td>1.928047</td>\n",
       "      <td>1.849282</td>\n",
       "      <td>0.004796</td>\n",
       "      <td>0.178719</td>\n",
       "      <td>0.357689</td>\n",
       "      <td>0.371424</td>\n",
       "      <td>0.208993</td>\n",
       "      <td>0.158744</td>\n",
       "      <td>0.529901</td>\n",
       "      <td>0.546505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-03-01 00:03:00</th>\n",
       "      <td>4.865</td>\n",
       "      <td>21.016</td>\n",
       "      <td>335.742</td>\n",
       "      <td>124.963</td>\n",
       "      <td>460.705</td>\n",
       "      <td>0.072944</td>\n",
       "      <td>-0.436590</td>\n",
       "      <td>-0.102483</td>\n",
       "      <td>0.017551</td>\n",
       "      <td>0.007149</td>\n",
       "      <td>...</td>\n",
       "      <td>1.928621</td>\n",
       "      <td>1.849608</td>\n",
       "      <td>0.004398</td>\n",
       "      <td>0.172967</td>\n",
       "      <td>0.351832</td>\n",
       "      <td>0.368358</td>\n",
       "      <td>0.208416</td>\n",
       "      <td>0.158524</td>\n",
       "      <td>0.529534</td>\n",
       "      <td>0.357703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-03-01 00:04:00</th>\n",
       "      <td>27.158</td>\n",
       "      <td>3.451</td>\n",
       "      <td>98.411</td>\n",
       "      <td>44.407</td>\n",
       "      <td>142.818</td>\n",
       "      <td>0.173820</td>\n",
       "      <td>-0.213489</td>\n",
       "      <td>0.096067</td>\n",
       "      <td>0.215709</td>\n",
       "      <td>0.107133</td>\n",
       "      <td>...</td>\n",
       "      <td>1.927084</td>\n",
       "      <td>1.848950</td>\n",
       "      <td>0.004008</td>\n",
       "      <td>0.167391</td>\n",
       "      <td>0.346066</td>\n",
       "      <td>0.365314</td>\n",
       "      <td>0.207839</td>\n",
       "      <td>0.158304</td>\n",
       "      <td>0.529167</td>\n",
       "      <td>0.362452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-29 23:55:00</th>\n",
       "      <td>4.163</td>\n",
       "      <td>6.805</td>\n",
       "      <td>39.037</td>\n",
       "      <td>55.351</td>\n",
       "      <td>94.388</td>\n",
       "      <td>0.020155</td>\n",
       "      <td>0.076565</td>\n",
       "      <td>0.228994</td>\n",
       "      <td>0.288856</td>\n",
       "      <td>0.151634</td>\n",
       "      <td>...</td>\n",
       "      <td>3.219345</td>\n",
       "      <td>3.340686</td>\n",
       "      <td>0.008679</td>\n",
       "      <td>0.224656</td>\n",
       "      <td>0.401595</td>\n",
       "      <td>0.393726</td>\n",
       "      <td>0.212651</td>\n",
       "      <td>0.136494</td>\n",
       "      <td>0.243172</td>\n",
       "      <td>0.396289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-29 23:56:00</th>\n",
       "      <td>2.290</td>\n",
       "      <td>4.058</td>\n",
       "      <td>110.201</td>\n",
       "      <td>67.171</td>\n",
       "      <td>177.372</td>\n",
       "      <td>0.016262</td>\n",
       "      <td>0.062527</td>\n",
       "      <td>0.214072</td>\n",
       "      <td>0.276463</td>\n",
       "      <td>0.146521</td>\n",
       "      <td>...</td>\n",
       "      <td>3.216719</td>\n",
       "      <td>3.339353</td>\n",
       "      <td>0.007928</td>\n",
       "      <td>0.217422</td>\n",
       "      <td>0.395019</td>\n",
       "      <td>0.390476</td>\n",
       "      <td>0.212063</td>\n",
       "      <td>0.136305</td>\n",
       "      <td>0.243004</td>\n",
       "      <td>0.328993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-29 23:57:00</th>\n",
       "      <td>5.237</td>\n",
       "      <td>3.640</td>\n",
       "      <td>70.499</td>\n",
       "      <td>30.753</td>\n",
       "      <td>101.252</td>\n",
       "      <td>0.045407</td>\n",
       "      <td>0.109834</td>\n",
       "      <td>0.263577</td>\n",
       "      <td>0.329266</td>\n",
       "      <td>0.174214</td>\n",
       "      <td>...</td>\n",
       "      <td>3.213444</td>\n",
       "      <td>3.337740</td>\n",
       "      <td>0.007243</td>\n",
       "      <td>0.210421</td>\n",
       "      <td>0.388549</td>\n",
       "      <td>0.387252</td>\n",
       "      <td>0.211477</td>\n",
       "      <td>0.136117</td>\n",
       "      <td>0.242836</td>\n",
       "      <td>0.189909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-29 23:58:00</th>\n",
       "      <td>5.731</td>\n",
       "      <td>4.901</td>\n",
       "      <td>22.365</td>\n",
       "      <td>52.195</td>\n",
       "      <td>74.560</td>\n",
       "      <td>0.124783</td>\n",
       "      <td>0.244168</td>\n",
       "      <td>0.408704</td>\n",
       "      <td>0.480016</td>\n",
       "      <td>0.251493</td>\n",
       "      <td>...</td>\n",
       "      <td>3.209945</td>\n",
       "      <td>3.336030</td>\n",
       "      <td>0.006608</td>\n",
       "      <td>0.203642</td>\n",
       "      <td>0.382184</td>\n",
       "      <td>0.384054</td>\n",
       "      <td>0.210892</td>\n",
       "      <td>0.135928</td>\n",
       "      <td>0.242668</td>\n",
       "      <td>0.410831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-29 23:59:00</th>\n",
       "      <td>3.925</td>\n",
       "      <td>3.865</td>\n",
       "      <td>86.585</td>\n",
       "      <td>217.137</td>\n",
       "      <td>303.722</td>\n",
       "      <td>0.368659</td>\n",
       "      <td>0.665382</td>\n",
       "      <td>0.867538</td>\n",
       "      <td>0.951903</td>\n",
       "      <td>0.491276</td>\n",
       "      <td>...</td>\n",
       "      <td>3.208415</td>\n",
       "      <td>3.335166</td>\n",
       "      <td>0.006072</td>\n",
       "      <td>0.197096</td>\n",
       "      <td>0.375931</td>\n",
       "      <td>0.380886</td>\n",
       "      <td>0.210310</td>\n",
       "      <td>0.135741</td>\n",
       "      <td>0.242501</td>\n",
       "      <td>0.731542</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>525887 rows Ã— 896 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     bid_qty  ask_qty  buy_qty  sell_qty   volume        X1  \\\n",
       "timestamp                                                                     \n",
       "2023-03-01 00:00:00   15.283    8.425  176.405    44.984  221.389  0.121263   \n",
       "2023-03-01 00:01:00   38.590    2.336  525.846   321.950  847.796  0.302841   \n",
       "2023-03-01 00:02:00    0.442   60.250  159.227   136.369  295.596  0.167462   \n",
       "2023-03-01 00:03:00    4.865   21.016  335.742   124.963  460.705  0.072944   \n",
       "2023-03-01 00:04:00   27.158    3.451   98.411    44.407  142.818  0.173820   \n",
       "...                      ...      ...      ...       ...      ...       ...   \n",
       "2024-02-29 23:55:00    4.163    6.805   39.037    55.351   94.388  0.020155   \n",
       "2024-02-29 23:56:00    2.290    4.058  110.201    67.171  177.372  0.016262   \n",
       "2024-02-29 23:57:00    5.237    3.640   70.499    30.753  101.252  0.045407   \n",
       "2024-02-29 23:58:00    5.731    4.901   22.365    52.195   74.560  0.124783   \n",
       "2024-02-29 23:59:00    3.925    3.865   86.585   217.137  303.722  0.368659   \n",
       "\n",
       "                           X2        X3        X4        X5  ...      X882  \\\n",
       "timestamp                                                    ...             \n",
       "2023-03-01 00:00:00 -0.417690  0.005399  0.125948  0.058359  ...  1.925423   \n",
       "2023-03-01 00:01:00 -0.049576  0.356667  0.481087  0.237954  ...  1.928569   \n",
       "2023-03-01 00:02:00 -0.291212  0.083138  0.206881  0.101727  ...  1.928047   \n",
       "2023-03-01 00:03:00 -0.436590 -0.102483  0.017551  0.007149  ...  1.928621   \n",
       "2023-03-01 00:04:00 -0.213489  0.096067  0.215709  0.107133  ...  1.927084   \n",
       "...                       ...       ...       ...       ...  ...       ...   \n",
       "2024-02-29 23:55:00  0.076565  0.228994  0.288856  0.151634  ...  3.219345   \n",
       "2024-02-29 23:56:00  0.062527  0.214072  0.276463  0.146521  ...  3.216719   \n",
       "2024-02-29 23:57:00  0.109834  0.263577  0.329266  0.174214  ...  3.213444   \n",
       "2024-02-29 23:58:00  0.244168  0.408704  0.480016  0.251493  ...  3.209945   \n",
       "2024-02-29 23:59:00  0.665382  0.867538  0.951903  0.491276  ...  3.208415   \n",
       "\n",
       "                         X883      X884      X885      X886      X887  \\\n",
       "timestamp                                                               \n",
       "2023-03-01 00:00:00  1.847943  0.005676  0.190791  0.369691  0.377630   \n",
       "2023-03-01 00:01:00  1.849468  0.005227  0.184660  0.363642  0.374515   \n",
       "2023-03-01 00:02:00  1.849282  0.004796  0.178719  0.357689  0.371424   \n",
       "2023-03-01 00:03:00  1.849608  0.004398  0.172967  0.351832  0.368358   \n",
       "2023-03-01 00:04:00  1.848950  0.004008  0.167391  0.346066  0.365314   \n",
       "...                       ...       ...       ...       ...       ...   \n",
       "2024-02-29 23:55:00  3.340686  0.008679  0.224656  0.401595  0.393726   \n",
       "2024-02-29 23:56:00  3.339353  0.007928  0.217422  0.395019  0.390476   \n",
       "2024-02-29 23:57:00  3.337740  0.007243  0.210421  0.388549  0.387252   \n",
       "2024-02-29 23:58:00  3.336030  0.006608  0.203642  0.382184  0.384054   \n",
       "2024-02-29 23:59:00  3.335166  0.006072  0.197096  0.375931  0.380886   \n",
       "\n",
       "                         X888      X889      X890     label  \n",
       "timestamp                                                    \n",
       "2023-03-01 00:00:00  0.210153  0.159183  0.530636  0.562539  \n",
       "2023-03-01 00:01:00  0.209573  0.158963  0.530269  0.533686  \n",
       "2023-03-01 00:02:00  0.208993  0.158744  0.529901  0.546505  \n",
       "2023-03-01 00:03:00  0.208416  0.158524  0.529534  0.357703  \n",
       "2023-03-01 00:04:00  0.207839  0.158304  0.529167  0.362452  \n",
       "...                       ...       ...       ...       ...  \n",
       "2024-02-29 23:55:00  0.212651  0.136494  0.243172  0.396289  \n",
       "2024-02-29 23:56:00  0.212063  0.136305  0.243004  0.328993  \n",
       "2024-02-29 23:57:00  0.211477  0.136117  0.242836  0.189909  \n",
       "2024-02-29 23:58:00  0.210892  0.135928  0.242668  0.410831  \n",
       "2024-02-29 23:59:00  0.210310  0.135741  0.242501  0.731542  \n",
       "\n",
       "[525887 rows x 896 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2**6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = train_df.columns.drop('label').tolist() + ['X890']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "896"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_df = reduce_dataframe_size(train_df)\n",
    "new_test_df = reduce_dataframe_size(test_df)\n",
    "\n",
    "del train_df\n",
    "del test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from ktools.modelling.ktools_models.pytorch_nns.nonlinear_ff_module import NonLinearFeedForwardModule\n",
    "\n",
    "\n",
    "class DeepAutoencoder(nn.Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 num_input_features : int,\n",
    "                 levels_of_compression : int):\n",
    "        \n",
    "        super(DeepAutoencoder, self).__init__()\n",
    "\n",
    "        self._num_input_features = num_input_features\n",
    "        self._levels_of_compression = levels_of_compression\n",
    "        \n",
    "        compressions = np.cumprod([1] + [2] * levels_of_compression)\n",
    "\n",
    "        self._encoder = nn.Sequential(*[\n",
    "            NonLinearFeedForwardModule(num_input_features // compressions[i],\n",
    "                                       num_input_features // compressions[i+1],\n",
    "                                       num_input_features // compressions[i+1])\n",
    "            for i in range(levels_of_compression - 1)\n",
    "        ])\n",
    "\n",
    "        self._decoder = nn.Sequential(*[\n",
    "            NonLinearFeedForwardModule(num_input_features // compressions[i],\n",
    "                                       num_input_features // compressions[i+1],\n",
    "                                       num_input_features // compressions[i+1])\n",
    "            for i in range(levels_of_compression - 1)[::-1]\n",
    "        ])\n",
    "    \n",
    "    def encode(self, x):\n",
    "        return self._encoder(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encode(x)\n",
    "        x = self._decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "\n",
    "\n",
    "settings = DataSciencePipelineSettings(train_csv_path=train_csv_path,\n",
    "                                    test_csv_path=test_csv_path,\n",
    "                                    target_col_name=\"label\",\n",
    "                                    train_data=new_train_df,\n",
    "                                    test_data=new_test_df)\n",
    "\n",
    "transform_list = [FillInfValues.transform,\n",
    "                    FillNullValues.transform,\n",
    "                    StandardScaleNumerical.transform]\n",
    "\n",
    "new_settings = reduce(lambda acc, func: func(acc), transform_list, settings)\n",
    "processed_train, processed_test = new_settings.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def custom_torch_dataloader(train_data : pd.DataFrame, batch_size : int = 64, shuffle : bool = True):\n",
    "    \n",
    "    num_datapoints = train_data.shape[0]\n",
    "    batches = round(num_datapoints / batch_size)\n",
    "    fold_obj = KFold(n_splits=batches, shuffle=shuffle, random_state=42)\n",
    "\n",
    "    for _, batch_idcs in fold_obj.split(train_data):\n",
    "        batch_data = train_data.iloc[batch_idcs]\n",
    "        batch_tensor = torch.tensor(batch_data.values, dtype=torch.float32)\n",
    "\n",
    "        \n",
    "        yield TensorDataset(batch_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from ktools.modelling.ktools_models.pytorch_nns.pytorch_lightning_model import KtoolsBaseLightningmodel\n",
    "\n",
    "\n",
    "class AutoEncoderModel(KtoolsBaseLightningmodel):\n",
    "\n",
    "    def __init__(self, \n",
    "                 model: torch.nn.Module,\n",
    "                 learning_rate: float = 0.06464861983337984, \n",
    "                 weight_decay: float = 0.0002773544957610778):\n",
    "        \n",
    "        super(AutoEncoderModel, self).__init__(model, learning_rate, weight_decay)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def get_loss(self, batch, mode = None):\n",
    "        reproduce_batch = self(batch)\n",
    "        loss = F.mse_loss(reproduce_batch, batch, reduction='mean')\n",
    "        loss_dict = {f'{mode}_reproduction_loss' : loss}\n",
    "        batch_dict = {}\n",
    "        return loss, loss_dict, batch_dict\n",
    "    \n",
    "    def get_global_metrics(self):\n",
    "        return {}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self._learning_rate, weight_decay=self._weight_decay)\n",
    "        scheduler_config = {\n",
    "            \"scheduler\": torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "                optimizer,\n",
    "                T_max=45,\n",
    "                eta_min=6e-3\n",
    "            ),\n",
    "            \"interval\": \"epoch\",\n",
    "            \"frequency\": 1,\n",
    "            \"strict\": False,\n",
    "        }\n",
    "\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler_config}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, X):\n",
    "        self.X = X\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x_row = self.X[idx].reshape(1, -1)\n",
    "        return torch.as_tensor(x_row, dtype=torch.float16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_ktools_dl(X : pd.DataFrame, target_col_name : str, training=False):\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    training_cols = [f for f in X.columns if f != target_col_name]\n",
    "\n",
    "    for col_name in training_cols:\n",
    "        X.loc[X[col_name] == np.inf, col_name] = -1\n",
    "        X.loc[X[col_name] == -np.inf, col_name] = -2\n",
    "    \n",
    "    X_train = scaler.fit_transform(X[training_cols])\n",
    "\n",
    "    ds_train = MyDataset(\n",
    "        X_train\n",
    "    )\n",
    "    bs = 64\n",
    "    dl_train = torch.utils.data.DataLoader(ds_train, batch_size=bs, pin_memory=True, shuffle=training)\n",
    "    return dl_train, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = DeepAutoencoder(train_df.shape[1] - 1, 4)\n",
    "model = AutoEncoderModel(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_train = init_ktools_dl(train_df, 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/yuwei-1/anaconda3/envs/ktools/lib/python3.12/site-packages/pytorch_lightning/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "/Users/yuwei-1/anaconda3/envs/ktools/lib/python3.12/site-packages/pytorch_lightning/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "checkpoint_callback = pl.callbacks.ModelCheckpoint()\n",
    "trainer = pl.Trainer(\n",
    "    accelerator='cpu',\n",
    "    max_epochs=60,\n",
    "    callbacks=[\n",
    "        checkpoint_callback,\n",
    "        LearningRateMonitor(logging_interval='epoch'),\n",
    "        TQDMProgressBar(),\n",
    "        StochasticWeightAveraging(swa_lrs=1e-5, swa_epoch_start=45, annealing_epochs=15)\n",
    "    ],\n",
    "    # deterministic=True\n",
    ")\n",
    "trainer.fit(model, dl_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RemoveEventsFromTimePeriod(IFeatureTransformer):\n",
    "\n",
    "    def __init__(self,\n",
    "                 time_intervals_to_remove : List[Tuple[pd.Timestamp]]) -> None:\n",
    "        self._time_intervals_to_remove = time_intervals_to_remove\n",
    "\n",
    "    def transform(self, original_settings : DataSciencePipelineSettings):\n",
    "        settings = deepcopy(original_settings)\n",
    "        train_df, test_df = settings.update()\n",
    "        for (start, end) in self._time_intervals_to_remove:\n",
    "            train_df = train_df[~((train_df.index >= start) & (train_df.index <= end))]\n",
    "        settings.combined_df = pd.concat([train_df, test_df], keys=['train', 'test'])\n",
    "        return settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_major_events = [\n",
    "    (pd.Timestamp(\"2023-03-10\"), pd.Timestamp(\"2023-03-19\")), # bank collapse\n",
    "    (pd.Timestamp(\"2023-06-16\"), pd.Timestamp(\"2023-06-23\")),\n",
    "    (pd.Timestamp(\"2023-08-14\"), pd.Timestamp(\"2023-08-18\")),\n",
    "    (pd.Timestamp(\"2023-10-16\"), pd.Timestamp(\"2023-10-25\")), # blackrock etf\n",
    "    (pd.Timestamp(\"2024-02-25\"), pd.Timestamp(\"2024-03-13\")),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_time_series_data_overlay_major_dates(df : pd.DataFrame,\n",
    "                                              target_col_name : str):\n",
    "    df = df.sort_index()\n",
    "    x = df.index\n",
    "    y = df[target_col_name]\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.bar(x, y, width=(x[-1] - x[0])/len(x))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_time_series_data_overlay_major_dates(train_df, 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mahdi_params = {\n",
    "    \"colsample_bylevel\": 0.4778015829774066,\n",
    "    \"colsample_bynode\": 0.362764358742407,\n",
    "    \"colsample_bytree\": 0.7107423488010493,\n",
    "    \"gamma\": 1.7094857725240398,\n",
    "    \"learning_rate\": 0.02213323588455387,\n",
    "    \"max_depth\": 20,\n",
    "    \"max_leaves\": 12,\n",
    "    \"min_child_weight\": 16,\n",
    "    \"n_estimators\": 1667,\n",
    "    \"n_jobs\": -1,\n",
    "    \"random_state\": 42,\n",
    "    \"reg_alpha\": 39.352415706891264,\n",
    "    \"reg_lambda\": 75.44843704068275,\n",
    "    \"subsample\": 0.06566669853471274,\n",
    "    \"verbosity\": 0\n",
    "}\n",
    "\n",
    "classifier_params = {\n",
    "    'objective':'binary:logistic',\n",
    "    'eval_metric':'auc',  \n",
    "    \"colsample_bylevel\": 0.4778015829774066,\n",
    "    \"colsample_bynode\": 0.362764358742407,\n",
    "    \"colsample_bytree\": 0.7107423488010493,\n",
    "    \"gamma\": 1.7094857725240398,\n",
    "    \"learning_rate\": 0.02213323588455387,\n",
    "    \"max_depth\": 20,\n",
    "    \"max_leaves\": 12,\n",
    "    \"min_child_weight\": 16,\n",
    "    \"n_estimators\": 1667,\n",
    "    \"n_jobs\": -1,\n",
    "    \"random_state\": 42,\n",
    "    \"reg_alpha\": 39.352415706891264,\n",
    "    \"reg_lambda\": 75.44843704068275,\n",
    "    \"subsample\": 0.06566669853471274,\n",
    "    \"verbosity\": 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binary_train = train_df.copy(deep=True)\n",
    "# binary_train['label'] = np.tanh(binary_train[])\n",
    "\n",
    "#1/(1 + np.exp(-binary_train['label'].values))\n",
    "# binary_train['label'] = (binary_train['label'] > 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuwei-1/anaconda3/envs/ktools/lib/python3.12/site-packages/sklearn/model_selection/_split.py:1213: UserWarning: The groups parameter is ignored by TimeSeriesSplit\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]INFO:cross_validation_log:The CV results of the current fold is 0.08138730376958847\n",
      "1it [00:02,  2.13s/it]INFO:cross_validation_log:The CV results of the current fold is 0.03161057457327843\n",
      "2it [00:05,  2.58s/it]INFO:cross_validation_log:The CV results of the current fold is 0.09128579497337341\n",
      "3it [00:09,  3.35s/it]INFO:cross_validation_log:The CV results of the current fold is 0.1347571164369583\n",
      "4it [00:14,  3.95s/it]INFO:cross_validation_log:The CV results of the current fold is 0.019442427903413773\n",
      "5it [00:19,  3.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################################################################################################\n",
      "OOF prediction score :  0.07104451111536203\n",
      "Mean 5-cv results : 0.07169664651155472 +- 0.04193247854709625\n",
      "####################################################################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "class TanhTransform(IFeatureTransformer):\n",
    "    def transform(original_settings : DataSciencePipelineSettings):\n",
    "        settings = deepcopy(original_settings)\n",
    "        train_df, test_df = settings.update()\n",
    "        train_df[settings.target_col_name] = np.tanh(train_df[settings.target_col_name])\n",
    "        settings.combined_df = pd.concat([train_df, test_df], keys=['train', 'test'])\n",
    "        return settings\n",
    "\n",
    "scve = SafeCrossValidationExecutor(\n",
    "    sklearn_model_instance=XGBoostModel(**mahdi_params), #SklearnWrapper(alpha=1),\n",
    "    evaluation_metric=lambda x, y : pearsonr(x, y).statistic,#lambda x, y : accuracy_score(x, np.where(y > 0.5, 1, 0)),\n",
    "    kfold_object=TimeSeriesSplit(5),#KFold(5, random_state=42, shuffle=True),\n",
    "    train_csv_path=train_csv_path,\n",
    "    test_csv_path=test_csv_path,\n",
    "    target_col_name='label',\n",
    "    training_features=[\"X863\", \"X856\", \"X344\", \"X598\", \"X862\", \"X385\", \"X852\", \"X603\", \"X860\", \"X674\",\n",
    "    \"X415\", \"X345\", \"X137\", \"X855\", \"X174\", \"X302\", \"X178\", \"X532\", \"X168\", \"X612\",\n",
    "    \"bid_qty\", \"ask_qty\", \"buy_qty\", \"sell_qty\", \"volume\"],\n",
    "    pipeline_transforms=[\n",
    "        # ReduceMemoryUsage.transform,\n",
    "        # TanhTransform.transform,\n",
    "        RemoveEventsFromTimePeriod(list_of_major_events).transform,\n",
    "        FillInfValues.transform,\n",
    "        # lambda x : RemoveOutliers.transform(x, lq=0.2, uq=0.8),\n",
    "        FillNullValues.transform,\n",
    "        StandardScaleNumerical.transform\n",
    "                        ]\n",
    ")\n",
    "(score_tuple,\n",
    " sign_oof_predictions,\n",
    " model_list,\n",
    " sign_test_predictions) = scve.run(train_data=train_df, test_data=test_df, weights=[0.8] * int(train_df.shape[0]/2) + [1.3] * (train_df.shape[0] - int(train_df.shape[0]/2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['label'] = np.abs(train_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuwei-1/anaconda3/envs/ktools/lib/python3.12/site-packages/sklearn/model_selection/_split.py:1213: UserWarning: The groups parameter is ignored by TimeSeriesSplit\n",
      "  warnings.warn(\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:cross_validation_log:The CV results of the current fold is 0.7370205521583557\n",
      "1it [00:01,  2.00s/it]INFO:cross_validation_log:The CV results of the current fold is 0.7975037097930908\n",
      "2it [00:04,  2.49s/it]INFO:cross_validation_log:The CV results of the current fold is 0.7684149742126465\n",
      "3it [00:08,  2.99s/it]INFO:cross_validation_log:The CV results of the current fold is 0.6767498850822449\n",
      "4it [00:12,  3.36s/it]INFO:cross_validation_log:The CV results of the current fold is 0.7725720405578613\n",
      "5it [00:17,  3.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################################################################################################\n",
      "OOF prediction score :  0.8035643742475335\n",
      "Mean 5-cv results : 0.7504522204399109 +- 0.04156497120857239\n",
      "####################################################################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "\n",
    "scve = SafeCrossValidationExecutor(\n",
    "    sklearn_model_instance=XGBoostModel(**mahdi_params), #SklearnWrapper(alpha=1),\n",
    "    evaluation_metric=root_mean_squared_error,\n",
    "    kfold_object=TimeSeriesSplit(5),#KFold(5, random_state=42, shuffle=True),\n",
    "    train_csv_path=train_csv_path,\n",
    "    test_csv_path=test_csv_path,\n",
    "    target_col_name='label',\n",
    "    training_features=[\"X863\", \"X856\", \"X344\", \"X598\", \"X862\", \"X385\", \"X852\", \"X603\", \"X860\", \"X674\",\n",
    "    \"X415\", \"X345\", \"X137\", \"X855\", \"X174\", \"X302\", \"X178\", \"X532\", \"X168\", \"X612\",\n",
    "    \"bid_qty\", \"ask_qty\", \"buy_qty\", \"sell_qty\", \"volume\"],\n",
    "    pipeline_transforms=[\n",
    "        # ReduceMemoryUsage.transform,\n",
    "        # RemoveEventsFromTimePeriod(list_of_major_events).transform,\n",
    "        FillInfValues.transform,\n",
    "        # lambda x : RemoveOutliers.transform(x, lq=0.2, uq=0.8),\n",
    "        FillNullValues.transform,\n",
    "        StandardScaleNumerical.transform\n",
    "                        ]\n",
    ")\n",
    "\n",
    "(score_tuple,\n",
    " mag_oof_predictions,\n",
    " model_list,\n",
    " mag_test_predictions) = scve.run(train_data=train_df, test_data=test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.049725249693240915"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearsonr(mag_oof_predictions * (sign_oof_predictions - 0.5), train_df['label']).statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SklearnWrapper(SGDRegressor):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        loss=\"squared_error\",\n",
    "        *,\n",
    "        penalty=\"l2\",\n",
    "        alpha=0.0001,\n",
    "        l1_ratio=0.15,\n",
    "        fit_intercept=True,\n",
    "        max_iter=1000,\n",
    "        tol=1e-3,\n",
    "        shuffle=True,\n",
    "        verbose=0,\n",
    "        epsilon=0.1,\n",
    "        random_state=129,\n",
    "        learning_rate=\"invscaling\",\n",
    "        eta0=0.01,\n",
    "        power_t=0.25,\n",
    "        early_stopping=False,\n",
    "        validation_fraction=0.1,\n",
    "        n_iter_no_change=5,\n",
    "        warm_start=False,\n",
    "        average=False,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            loss=loss,\n",
    "            penalty=penalty,\n",
    "            alpha=alpha,\n",
    "            l1_ratio=l1_ratio,\n",
    "            fit_intercept=fit_intercept,\n",
    "            max_iter=max_iter,\n",
    "            tol=tol,\n",
    "            shuffle=shuffle,\n",
    "            verbose=verbose,\n",
    "            epsilon=epsilon,\n",
    "            random_state=random_state,\n",
    "            learning_rate=learning_rate,\n",
    "            eta0=eta0,\n",
    "            power_t=power_t,\n",
    "            early_stopping=early_stopping,\n",
    "            validation_fraction=validation_fraction,\n",
    "            n_iter_no_change=n_iter_no_change,\n",
    "            warm_start=warm_start,\n",
    "            average=average,\n",
    "        )\n",
    "\n",
    "    def fit(self, X, y, **kwargs):\n",
    "        return super().fit(X, y)\n",
    "    \n",
    "\n",
    "scve = SafeCrossValidationExecutor(\n",
    "    sklearn_model_instance=XGBoostModel(**mahdi_params), #SklearnWrapper(alpha=1),\n",
    "    evaluation_metric=lambda x, y : pearsonr(x, y).statistic,\n",
    "    kfold_object=TimeSeriesSplit(5),\n",
    "    train_csv_path=train_csv_path,\n",
    "    test_csv_path=test_csv_path,\n",
    "    target_col_name='label',\n",
    "    training_features=[\"X863\", \"X856\", \"X344\", \"X598\", \"X862\", \"X385\", \"X852\", \"X603\", \"X860\", \"X674\",\n",
    "    \"X415\", \"X345\", \"X137\", \"X855\", \"X174\", \"X302\", \"X178\", \"X532\", \"X168\", \"X612\",\n",
    "    \"bid_qty\", \"ask_qty\", \"buy_qty\", \"sell_qty\", \"volume\"],\n",
    "    pipeline_transforms=[\n",
    "        # ReduceMemoryUsage.transform,\n",
    "        RemoveEventsFromTimePeriod(list_of_major_events).transform,\n",
    "        FillInfValues.transform,\n",
    "        # lambda x : RemoveOutliers.transform(x, lq=0.2, uq=0.8),\n",
    "        FillNullValues.transform,\n",
    "        StandardScaleNumerical.transform\n",
    "                        ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuwei-1/anaconda3/envs/ktools/lib/python3.12/site-packages/sklearn/model_selection/_split.py:1213: UserWarning: The groups parameter is ignored by TimeSeriesSplit\n",
      "  warnings.warn(\n",
      "0it [00:00, ?it/s]INFO:cross_validation_log:The CV results of the current fold is 0.10367283225059509\n",
      "1it [00:02,  2.91s/it]INFO:cross_validation_log:The CV results of the current fold is 0.0839976966381073\n",
      "2it [00:07,  3.80s/it]INFO:cross_validation_log:The CV results of the current fold is 0.045926980674266815\n",
      "3it [00:12,  4.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################################################################################################\n",
      "OOF prediction score :  0.06885551907582667\n",
      "Mean 3-cv results : 0.0778658390045166 +- 0.02397006005048752\n",
      "####################################################################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "(score_tuple,\n",
    " oof_predictions,\n",
    " model_list,\n",
    " test_predictions) = scve.run(train_data=train_df, test_data=test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sub = pd.read_csv(\"/Users/yuwei-1/Documents/projects/Kaggle-tools/data/DRW_crypto_price_prediction/sample_submission.csv\", index_col=0)\n",
    "sample_sub['prediction'] = sign_test_predictions#mag_test_predictions * (sign_test_predictions - 0.5)#test_predictions\n",
    "sample_sub.to_csv(\"/Users/yuwei-1/Documents/projects/Kaggle-tools/submissions/drw/submission_25.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.002162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.020968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.035647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.003229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.058872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538146</th>\n",
       "      <td>-0.031419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538147</th>\n",
       "      <td>-0.027702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538148</th>\n",
       "      <td>0.049828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538149</th>\n",
       "      <td>0.042671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538150</th>\n",
       "      <td>0.016614</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>538150 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        prediction\n",
       "ID                \n",
       "1        -0.002162\n",
       "2        -0.020968\n",
       "3         0.035647\n",
       "4        -0.003229\n",
       "5         0.058872\n",
       "...            ...\n",
       "538146   -0.031419\n",
       "538147   -0.027702\n",
       "538148    0.049828\n",
       "538149    0.042671\n",
       "538150    0.016614\n",
       "\n",
       "[538150 rows x 1 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ktools_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
